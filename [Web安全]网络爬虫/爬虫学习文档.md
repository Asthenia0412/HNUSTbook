# 一：网络爬虫的规则

## 1.Requests库的入门-使用Request库进行http操作

```cmd
pip install requests
```

```python
import requests
r = requests.get("http://baidu.com");
print(r);//可以输出结果
```

| 方法               | 说明                                        |
| ------------------ | ------------------------------------------- |
| requests.request() | 构造一个请求,是支撑以下方法的基础方法       |
| requests.get()     | 获取HTML网页主要方法，对应HTTP的GET()       |
| requests.head()    | 获取html头信息，相当于http的head            |
| requests.post()    | 向html网页提交post请求的方法，对应http的Put |
| requests.patch()   | 向html网页提交局部修改请求，对应http的patch |
| requests.delete()  | 向html网页提交删除请求,对应于http的delete   |
| requests.put()     | 向html页面提交Put请求,对于http的put         |

> HTTP定义了多种请求方法，来表示不同的操作语义。以下是常见的几种HTTP方法的区别及其具体例子：
> ### GET
> - **用途**：获取资源。应该只用于数据的读取，不应产生副作用。
> - **幂等性**：幂等。
> - **安全性**：安全，不会改变资源的状态。
> - **数据大小**：数据在URL中传输，大小有限制。
> - **例子**：查询某个用户的个人信息。
> ```
> GET /users/12345 HTTP/1.1
> Host: example.com
> ```
> ### POST
> - **用途**：创建新的资源或提交数据。
> - **幂等性**：非幂等。
> - **安全性**：不安全，可能会改变资源的状态。
> - **数据大小**：数据在请求体中传输，理论上不受大小限制。
> - **例子**：创建一个新用户。
> ```
> POST /users HTTP/1.1
> Host: example.com
> Content-Type: application/json
> {
>   "username": "newuser",
>   "email": "newuser@example.com"
> }
> ```
> ### PUT
> - **用途**：更新资源，或创建指定ID的资源（如果不存在）。
> - **幂等性**：幂等。
> - **安全性**：不安全，可能会改变资源的状态。
> - **数据大小**：数据在请求体中传输。
> - **例子**：更新用户12345的邮箱。
> ```
> PUT /users/12345 HTTP/1.1
> Host: example.com
> Content-Type: application/json
> {
>   "email": "updatedemail@example.com"
> }
> ```
> ### DELETE
> - **用途**：删除资源。
> - **幂等性**：幂等。
> - **安全性**：不安全，会删除资源。
> - **数据大小**：通常没有请求体。
> - **例子**：删除ID为12345的用户。
> ```
> DELETE /users/12345 HTTP/1.1
> Host: example.com
> ```
> ### PATCH
> - **用途**：部分更新资源。
> - **幂等性**：非幂等（通常实现为幂等，但标准不要求）。
> - **安全性**：不安全，可能会改变资源的状态。
> - **数据大小**：数据在请求体中传输。
> - **例子**：只更新用户12345的邮箱，不更改其他信息。
> ```
> PATCH /users/12345 HTTP/1.1
> Host: example.com
> Content-Type: application/json
> {
>   "op": "replace",
>   "path": "/email",
>   "value": "patchedemail@example.com"
> }
> ```
> 这里使用的是JSON Patch格式，`op`代表操作类型，`path`代表要修改的资源路径，`value`代表新的值。
> 每种方法都有其特定的用途和语义，在设计API时应该根据操作的性质选择合适的HTTP方法。

1. 发送get请求

   ```python
   import requests
   response = requests.get("http://www.baidu.com");
   print(response.text)
   ```

2. 发送post请求

   ```python
   import requests
   data{
       'key1':'value1',
       'key2':'value2'
   }
   response = requests.post("http://www.baidu.com",data)
   print(response.text);
   ```

3. 查看状态响应码/响应头/json格式解析

   ```python
   print(response.status_code);
   print(response.headers);
   json_data = response.json();
   print(json_data);
   ```

4. 设置请求头

   ```python
   import  requests
   headersTest = {
       'Authorization':'Bearer your_token',
       'Content-Type':'application/json',
   }
   response = requests.get("http://getman.cn/mock/xiaoyongcai",headers = headersTest)
   ```

5. 错误处理

   ```python
   import requests
   response = requests.get("http://getman.cn/mock/xiaoyongcai");
   print(response.raise_for_status())
   ##如果网络请求有问题 会抛出一个Error错误
   ```

6. 会话对象创建

   ```python
   with requests.Session() as session:
       session.headers.update("Authorization":"Your token")
       response = session.get("http://getman.cn/mock/xiaoyongcai")
       
   ```

   > 在Python中，`with` 语句连同 `as` 关键字一起使用，是一种上下文管理器（Context Manager）的语法，用于设置和清理代码块执行的环境。这种用法可以确保某些资源在使用后被适当地管理和释放，即使发生异常也是如此。这与Java中的 `try-with-resources` 语句有点相似。
   > 以下是关于 `with` 语句的几个要点：
   > ### 上下文管理器
   > - **定义**：任何实现了 `__enter__` 和 `__exit__` 方法的对象都可以作为一个上下文管理器。
   > - **`__enter__` 方法**：当进入 `with` 语句块时调用，返回一个值，通常赋值给 `as` 后面的变量。
   > - **`__exit__` 方法**：当离开 `with` 语句块时调用，无论是因为代码块正常执行结束还是因为异常。这个方法负责清理资源。
   > ### 示例
   > 在你提供的代码中，`requests.Session()` 是一个上下文管理器：
   > ```python
   > with requests.Session() as session:
   >     session.headers.update({"Authorization":"Your token"});
   >     response = session.get("http://getman.cn/mock/xiaoyongcai");
   > ```
   > 这段代码的执行流程如下：
   > 1. `requests.Session()` 创建一个新的 `Session` 对象。
   > 2. 调用 `Session` 对象的 `__enter__` 方法，这个方法返回 `Session` 对象本身，并被赋值给变量 `session`。
   > 3. 在 `with` 代码块内部，你可以使用 `session` 对象发送HTTP请求，并设置请求头等。
   > 4. 当 `with` 代码块执行完毕后，无论是因为正常结束还是因为异常，都会调用 `Session` 对象的 `__exit__` 方法。
   > `__exit__` 方法确保了如会话连接等资源被适当地关闭，这是自动发生的，你不需要显式调用关闭方法。
   > ### 对比Java
   > 在Java中，你可能会使用 `try` 块来确保资源在使用后被关闭，比如这样：
   > ```java
   > try (Resource res = new Resource()) {
   >     // 使用资源
   > } catch (Exception e) {
   >     // 处理异常
   > }
   > ```
   > 在这个例子中，`Resource` 类必须实现 `AutoCloseable` 或 `Closeable` 接口，这样 `res` 对象在 `try` 块结束时会被自动关闭。
   > 在Python中，`with` 语句提供了类似的机制，但语法更为简洁，并且是语言内置的特性，不需要额外的接口实现。

### A.Requests库的get方法

```python
import requests
paramsTest = {
    'params1':'value1',
    'params2':'value2'
}
response = requests.get("http://getman.cn/mock/",params = paramsTest)
```

> 在Python的`requests`库中，一旦你发送了一个GET请求并获取了响应对象，你可以访问该对象的多项属性和方法来处理返回的数据。以下是如何使用`.content`和`.encoding`属性，以及一些其他有用的属性和方法的例子：
> ```python
> import requests
> # 参数字典
> paramsTest = {
>     'params1': 'value1',
>     'params2': 'value2'
> }
> # 发送GET请求
> response = requests.get("http://getman.cn/mock/xiaoyongcai", params=paramsTest)
> # 输出响应的状态码
> print("状态码:", response.status_code)
> # 输出响应的编码
> print("编码:", response.encoding)
> # 输出响应的内容（字节形式）
> print("内容（字节）:", response.content)
> # 输出响应的内容（字符串形式）
> print("内容（字符串）:", response.text)
> # 如果需要，可以更改响应的编码方式
> # response.encoding = 'utf-8'  # 假设你知道响应内容的正确编码
> # 输出响应头
> print("响应头:", response.headers)
> # 输出原始响应套接字
> print("原始响应套接字:", response.raw)
> # 检查请求是否成功
> if response.ok:
>     print("请求成功")
> else:
>     print("请求失败")
> # 如果需要，可以抛出异常来处理请求错误
> # response.raise_for_status()
> # 如果响应是JSON格式的，可以直接解析为Python字典
> # json_data = response.json()
> # print("JSON数据:", json_data)
> ```
> 以下是这些属性和方法的简要说明：
> - `.status_code`: 响应的状态码（例如，200表示成功）。
> - `.encoding`: 响应的编码（例如，'utf-8'）。
> - `.content`: 响应内容的字节形式。
> - `.text`: 响应内容的字符串形式，使用`.encoding`指定的编码进行解码。
> - `.headers`: 响应头信息的字典。
> - `.raw`: 原始响应套接字，用于低级别的网络访问。
> - `.ok`: 一个布尔值，如果状态码在200-400之间，则为`True`。
> - `.raise_for_status()`: 如果响应的状态码指示错误，则抛出`HTTPError`异常。
> 在处理网络请求时，了解如何使用这些属性和方法是非常重要的，它们可以帮助你有效地处理和解析服务器返回的数据。

### B.爬取网页的通用代码框架

```python
import requests
def scrape_webpage(url):
    try:
        response = requests.get(url);
        if response.status_code == 200:
            print(response.text)
        else:
            print("Error状态码{response.status_code}")
    except requests.exceptions.RequestException as e:
        print('This is a error ocurred')

if __name__ == '__main__':
    url_to_scrape = "http://getman.cn/mock/"
    scrape_webpage(url_to_scrape)
'''
import requests: 引入requests库。
def scrape_webpage(url): 定义一个名为scrape_webpage的函数，它接收一个参数url，即要爬取的网页地址。
response = requests.get(url): 使用requests.get()方法发送一个GET请求到指定的URL。
if response.status_code == 200: 检查响应的状态码是否为200，这意味着请求成功。
print(response.text): 打印网页的文本内容。
except requests.exceptions.RequestException as e: 捕获请求过程中可能发生的异常，如网络连接问题、超时等，并打印异常信息。
if __name__ == '__main__': 确保以下代码只有在直接运行脚本时才会执行。
url_to_scrape = 'http://example.com': 设置要爬取的网页URL。
scrape_webpage(url_to_scrape): 调用scrape_webpage函数开始爬取网页。

'''
```



### C.HTTP协议与Requests库方法

### D.Requests库主要方法解析

### E.Requests库查询性能的分析

## 2.网络爬虫的盗亦有道

### A.引发的问题

### B.Robots协议

### C.Robots协议遵循的方式

## 3.Requests库的五个实际例子

### A.京东商品页面的爬取

### B.亚马逊商品页面的爬取

### C.百度360搜索关键字提交

### D.网络图片的爬取与储存

### E.IP地址归属地的自动查询

# 二：网络爬虫的提取

## 1.Beautiful Soup库入门

### A.Beautiful Soup库的安装

### B.Beatuful Soup库的基本元素

### C.基于bs4库的HTML遍历方法

### D.基于bs4库的HTML格式化和编码

### E.单元总结

## 2.信息组织与提取方法

### A.信息标记的三种形式

### B.三种信息标记形式的比较

### C.信息提取的一般方法

### D.基于bs4库的html内容查找方法

### E.单元结论

## 3.中国大学排名爬虫

### A.实例介绍

### B.实例编写

### C.实例优化

# 三：网络爬虫的实战

## 1.Re(正则表达式库)入门

### A.正则表达式的概念

### B.正则表达式的语法

### C.Re库的基本使用

### D.Re库的match对象

### E.Re库的贪婪匹配和最小匹配

## 2.实例：淘宝商品比价定向爬虫

### A.实例介绍

### B.实例编写

### C.单元小结

## 3.股票数据定向爬虫

### A.实例介绍

### B.实例编写

### C.实例优化

# 四：网络爬虫的框架

## 1.Scrapy爬虫框架

### A.框架介绍

### B.框架解析

### C.requests库和Scrapy爬虫比较

### D.Scrapy爬虫常用命令

### E.单元小结

## 2.Scrapy爬虫基本使用

### A.Scrapy爬虫实例

### B.yield关键字的使用

### C.Scrapy爬虫基本使用

### D.Scrapy代码实战

## 3.通过Scrapy爬取股票数据

### A.实例介绍

### B.实例编写

### C.实例优化