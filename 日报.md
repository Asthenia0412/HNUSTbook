[toc]



### 9.3

#### **A.事务ACID复习**:

1. 结合SQL代码示例：
以下是一个简单的SQL示例，用于展示如何在不同的隔离级别下操作数据库：
```sql
-- 假设有一个账户表 account，包含字段 id, name, balance
-- 开启事务
START TRANSACTION;
-- 查询账户余额
SELECT balance FROM account WHERE id = 1;
-- 更新账户余额
UPDATE account SET balance = balance - 100 WHERE id = 1;
-- 提交事务
COMMIT;
```
2. 分析脏读、不可重复读、幻读及隔离级别：
- **脏读**：在一个事务中，读取到了另一个未提交事务修改的数据。这种情况会导致数据不一致。
  隔离级别：**Read Uncommitted** 允许脏读发生。
- **不可重复读**：在一个事务中，多次读取同一数据时，结果不一致，因为其他事务在两次读取之间修改了该数据。
  隔离级别：**Read Committed** 可以避免脏读，但不可重复读仍然可能发生。
- **幻读**：在一个事务中，多次执行同一查询，结果集不一致，因为其他事务在两次查询之间插入了新的数据行。
  隔离级别：**Repeatable Read** 可以避免脏读和不可重复读，但幻读仍然可能发生。
- **Serializable**：这是最高的隔离级别，可以避免脏读、不可重复读和幻读。它通过锁定事务涉及的所有数据行来实现，但可能会导致较大的性能开销。
3. 文本内容（无Markdown标题）：
**Read Uncommitted**：允许脏读，事务可以看到其他事务未提交的数据，这是最低的隔离级别。
**Read Committed**：避免了脏读，但不可重复读和幻读仍然可能发生。这是大多数数据库系统的默认隔离级别。
**Repeatable Read**：避免了脏读和不可重复读，但幻读仍然可能发生。在这个隔离级别下，事务在执行过程中看到的数据是一致的。
**Serializable**：这是最高的隔离级别，可以避免脏读、不可重复读和幻读。它通过锁定事务涉及的所有数据行来实现，但可能会导致较大的性能开销。

#### **B.MVCC机制：**

在InnoDB中，每一个table中都有两个隐藏column。分别对应创建版本和删除版本号。且每个时间节点系统版本号不同

- B.1: insert/update/delete 操作对应版本号的处理方式如下：
  - **insert**: 写入当前系统版本号作为新行的“创建版本号”。
  - **update**: 写入当前系统版本号作为新行的“创建版本号”，并将当前系统版本号作为旧行的“删除版本号”。
  - **delete**: 写入当前系统版本号作为行的“删除版本号”。
- B.2: select 操作的查询规则如下：
  - 查询会返回那些创建版本号小于当前系统版本号的行，且这些行的删除版本号大于当前系统版本号。这样可以确保事务看到的是一致的数据快照，即事务开始时已经存在的数据，且没有被其他并发事务删除的数据。

#### **C.索引：**

##### C.1：索引本质

索引的本质是一种数据结构，它允许数据库高效地进行数据检索操作。在数据库管理系统中，索引的作用类似于书籍的目录，它提供了快速定位到特定数据行的能力，从而避免了全表扫描，提高了查询效率。
以下是索引的一些核心特点和本质：
1. **数据结构**：索引通常是基于特定的数据结构实现的，如B树（B-Tree）、B+树、哈希表、R树等。这些数据结构被设计用来优化查找、插入和删除操作。
2. **有序性**：索引通常按照一定的顺序存储数据，这样就可以利用二分查找等算法快速定位到特定值或值的范围。
3. **映射关系**：索引建立了数据库表中数据行与索引键值之间的映射关系。对于每一行数据，索引记录了其键值与数据行物理地址的对应关系。
4. **空间换时间**：索引通过占用额外的存储空间来减少查询操作的时间复杂度。索引本身也需要存储和维护，因此会占用额外的磁盘空间。
5. **辅助查询**：索引主要用于加速查询操作，尤其是WHERE子句、JOIN操作、ORDER BY和GROUP BY等。
6. **唯一性**：某些类型的索引（如主键索引）保证了索引键值的唯一性，这有助于维护数据的完整性。
7. **选择性**：索引的选择性是指索引键值区分数据行的能力。高选择性的索引可以更有效地过滤数据。
8. **维护成本**：虽然索引可以加速查询，但它们也需要在插入、删除和更新数据时进行维护，这可能会降低写操作的性能。
索引的这些本质属性共同决定了其在数据库系统中的重要作用，即在保证数据完整性和一致性的同时，提供快速的数据访问路径。****

##### C.2：聚簇索引

聚簇索引（Clustered Index）的本质不是表（table），而是一种特定的索引结构，它决定了数据在磁盘上的**物理存储顺序**。以下是聚簇索引的一些关键点：
1. **数据存储顺序**：聚簇索引决定了表中数据的物理存储顺序。在聚簇索引中，索引的叶节点直接包含了表的数据行，这意味着表的**数据行**实际上存储在**聚簇索引**的**叶节点**上。
2. **唯一性**：在大多数数据库系统中，每个表只能有**一个聚簇索引**，因为数据行只能有一种物理排序方式。
3. **主键与聚簇索引**：通常情况下，数据库表的**主键会自动成为**聚簇索引。这是因为主键具有唯一性，而聚簇索引能够很好地支持这种唯一性。
4. **非聚簇索引**：与聚簇索引相对的是**非聚簇索引**（也称为**二级索引**或**辅助索引**），它们不决定数据的物理存储顺序，而是包含**索引键值**和**指向数据行的指针**。
聚簇索引的本质特点如下：
- **索引结构**：聚簇索引通常是基于B+树结构实现的，其中叶节点包含了完整的行数据。
- **性能考虑**：由于数据行按照聚簇索引的顺序存储，因此对于基于聚簇索引键值的查询操作，可以直接访问到数据行，通常会有更好的性能。
- **插入和删除操作**：插入、删除和更新操作可能会更复杂，因为它们可能需要重新组织数据页以维持聚簇索引的顺序。
总的来说，聚簇索引是一种特殊的索引，它与表的数据行紧密相关，但并不是表本身。表是一个更广泛的概念，它包含了数据行、列、索引、约束等所有元素。聚簇索引只是表的一个组成部分，用于优化数据的检索性能。

##### C.3：主键索引选取规则： 

存在列满足unique&&not null?设定该列为主键索引列：创建隐藏列为主键索引列

##### C.4：聚簇索引劣势：

**A.页分裂**（在插入操作时，如果新行的聚簇索引键值在物理顺序中位于当前页的中间，可能引起页分裂，降低插入性能，并可能导致磁盘空间利用率下降。）

**B.页合并**（删除操作可能导致页的空闲空间增加，当相邻页的空闲空间过多时，可能需要进行页合并，这个过程同样会带来性能开销。）

**C.删除插入性能影响**（如果查询不使用聚簇索引的列，那么进行全表扫描的效率可能会比较低，因为数据块的物理顺序并不一定是最优的查询顺序。）

##### C.5：覆盖索引（Covering Index）

是一种数据库索引，它包含了查询中需要的所有列的数据，这意味着使用该索引进行查询时，不需要回表（即不需要再次访问原始表来获取数据）。

```sql
SELECT name, department FROM employees WHERE department = 'Sales';//SQL需求 
CREATE INDEX idx_department_name ON employees (department, name);//因大量使用该SQL 因此创建覆盖索引

```

##### C.6：回表（Backward Index Lookup 或 Full Table Scan）

回表是指在数据库查询操作中，当执行非聚簇索引查询并需要获取完整的行数据时，必须额外进行的步骤。下面详细分析回表的过程：

**聚簇索引与非聚簇索引**

在深入理解回表之前，我们需要了解聚簇索引和非聚簇索引的区别：
- **聚簇索引**：聚簇索引决定了数据在磁盘上的物理存储顺序。在聚簇索引中，索引的叶节点包含了完整的行数据。通常，表的主键会自动成为聚簇索引。
- **非聚簇索引**：非聚簇索引（也称为二级索引或辅助索引）不决定数据的物理存储顺序。它的叶节点包含了索引键值和指向数据行的指针（通常是行ID或磁盘地址）。

**回表过程**

当执行一个查询，如果使用了非聚簇索引，以下是回表的具体步骤：
1. **索引查找**：数据库首先在非聚簇索引中查找符合条件的索引键值。如果找到了，它会得到一个指向数据行的指针。
2. **获取行数据**：由于非聚簇索引的叶节点不包含完整的行数据，数据库需要使用得到的指针回到聚簇索引（或表的数据行）中，以获取完整的行数据。
3. **数据返回**：一旦完整的行数据被检索到，它就会被返回给用户。

**示例**

假设我们有一个 `employees` 表，它有一个聚簇索引在 `id` 列上，以及一个非聚簇索引在 `last_name` 列上。
```sql
CREATE TABLE employees (
    id INT AUTO_INCREMENT PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    department_id INT,
    salary DECIMAL(10, 2)
);
CREATE INDEX idx_lastname ON employees(last_name);
```
如果我们执行以下查询：
```sql
SELECT * FROM employees WHERE last_name = 'Smith';
```
以下是回表的步骤：
1. **索引查找**：数据库使用 `idx_lastname` 索引查找所有 `last_name` 为 'Smith' 的记录。假设找到了多条记录，每条记录都有一个指向对应数据行的指针。
2. **获取行数据**：数据库通过指针回到聚簇索引（即 `id` 列的索引），检索完整的员工记录。
3. **数据返回**：完整的员工记录被返回给用户。

**回表的性能影响**

回表会增加额外的磁盘I/O操作，因为它需要从非聚簇索引的叶节点跳转到聚簇索引的叶节点来获取完整的数据行。如果查询涉及大量的行，这可能会导致性能问题。
为了减少回表的开销，可以考虑以下优化策略：
- **索引覆盖**：创建包含查询所需所有列的非聚簇索引，这样就不需要回表了。
- **选择性索引**：确保索引具有高选择性，以减少需要回表的行数。
- **查询优化**：尽量避免在查询中使用 `SELECT *`，而是只选择需要的列，以减少回表的数据量。

##### C.7：索引结构：

索引的本质是一种高效的数据检索机制，它通过存储额外的数据结构来加快数据库中记录的查找速度。以下是索引的两部分及其本质的详细解释：

**索引键（Index Key）**

- **定义**：索引键是从表中的列（一个或多个）提取的值，用于在索引结构中进行排序和查找。
- **本质**：索引键的本质是数据的一个映射，它将数据库中的记录与索引中的位置相对应。通过这个映射，数据库能够快速定位到满足特定条件的数据行，而不必扫描整个表。
- **功能**：
  - 提供排序依据，使得索引中的数据有序。
  - 作为查找的依据，支持快速的搜索操作。

**数据指针（Pointer）**

- **定义**：数据指针是索引中指向原始表记录位置的信息。在聚簇索引中，数据指针可能是记录的物理地址；在非聚簇索引中，它通常是记录的主键或其他唯一标识符。
- **本质**：数据指针的本质是一个引用，它建立了索引与实际数据之间的链接。通过这个引用，数据库能够从索引快速跳转到原始数据行。
- **功能**：
  - 标识记录在存储介质上的位置。
  - 支持从索引到数据的快速访问。

**索引的本质**

- **数据结构**：索引是基于特定的数据结构（如B-Tree、B+Tree、Hash表等）实现的，这些结构被设计来优化数据的插入、删除和查找操作。
- **空间换时间**：索引通过占用额外的存储空间来减少查询操作的时间复杂度，从而提高数据库的查询性能。
- **映射关系**：索引建立了一种映射关系，将索引键值映射到数据行的物理位置，使得数据库能够根据键值快速定位到数据。
- **优化器辅助**：数据库查询优化器使用索引来决定最有效的查询执行计划，从而提高查询效率。
- **维护开销**：索引需要额外的维护，如插入、删除和更新操作时对索引结构的调整，这可能会降低写操作的性能。
综上所述，索引的本质是一种以空间换时间的数据结构，它通过建立索引键与数据指针之间的映射关系，提供了一种快速访问数据库记录的机制。

##### C.8：哈希索引与自适应哈希索引

**哈希索引**

```sql
CREATE TABLE users(
	user_id INT PRIMARY KEY,
    username VARCHAR(50)
)
```

现在我们创建一个哈希索引：

```sql
create hash index idx_username on users(username);
```

在这个例子中，数据库会使用哈希函数来计算每个 `username` 的哈希值，并将这些哈希值映射到相应的 `user_id`。当我们执行以下查询时：

```sql
select user_id from users where username="john_doe"
```

**自适应哈希索引：**InnoDB引擎提供的特性，创建基于频繁访问的索引页上的哈希索引，是自动的。

```sql
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE
);

```

如果数据库检测到对于 `customer_id` 的查询非常频繁，InnoDB存储引擎可能会自动为这个列创建自适应哈希索引。这个过程是自动的，并且是动态的，数据库会根据查询模式和数据访问模式来调整哈希索引。

例如，如果我们频繁执行以下查询：

```sql
SELECT * FROM orders WHERE customer_id = 12345;
```

InnoDB可能会为 `customer_id` 列创建自适应哈希索引，这样后续的查询就可以通过哈希索引快速找到对应的订单，而不需要扫描整个索引。

需要注意的是，自适应哈希索引的具体实现细节和触发条件是由数据库内部机制决定的，用户无法直接控制或创建自适应哈希索引。在某些数据库系统中，例如MySQL的InnoDB存储引擎，自适应哈希索引是自动管理的，用户通常只能通过配置参数来启用或禁用这个特性。

##### C.9冗余索引和重复索引

在数据库中，索引是提高查询性能的关键工具，但不当的管理可能会导致索引的冗余和重复，这不仅浪费存储空间，还可能降低数据库的性能。以下是冗余索引和重复索引的定义和分析：

**冗余索引（Redundant Index）**

**定义**：
冗余索引是指在一个索引已经能够满足查询需求的情况下，又创建了另一个功能上等效或部分重叠的索引。
**分析**：
- **功能重叠**：如果索引A包含列(a, b)，而索引B包含列(a)，那么索引B可能是冗余的，因为索引A已经包含了索引B的功能。
- **非最优查询计划**：冗余索引可能导致数据库查询优化器选择非最优的查询计划，因为优化器需要考虑更多的索引选项。
- **维护成本**：冗余索引需要额外的维护，例如在插入、删除或更新数据时，所有相关的索引都需要更新。
**示例**：
```sql
CREATE INDEX idx_ab ON table1(a, b);
CREATE INDEX idx_a ON table1(a); -- 可能是冗余的，因为idx_ab已经包含了a列
```
**重复索引（Duplicate Index）**

**定义**：
重复索引是指具有完全相同的列和列顺序的索引，即两个或多个索引在结构上完全相同。
**分析**：

- **资源浪费**：重复索引会浪费存储空间，因为它们存储了相同的数据。
- **性能下降**：在写操作（如INSERT、UPDATE、DELETE）时，每个重复索引都需要更新，这会增加写操作的开销，从而降低性能。
- **管理混乱**：重复索引可能导致数据库管理员在维护和优化数据库时产生混淆。
**示例**：
```sql
CREATE INDEX idx_a ON table1(a);
CREATE INDEX idx_a ON table1(a); -- 这是一个重复索引，因为它与上面的索引完全相同
```
**区别**

- **冗余索引**：通常是指索引的部分重叠，即一个索引包含了另一个索引的全部或部分列。
- **重复索引**：是指索引的完全重叠，即两个索引在结构和列顺序上完全相同。

**如何检测和解决冗余和重复索引**

- **检测**：数据库管理系统通常提供了工具或命令来分析索引，例如MySQL的`information_schema`或`EXPLAIN`命令。
- **解决**：一旦检测到冗余或重复索引，可以通过删除不必要的索引来优化数据库。在删除之前
- ，应该仔细分析索引的使用情况，以确保不会影响现有的查询性能。
```sql
-- 示例：删除重复索引
DROP INDEX idx_a ON table1;
```
在处理冗余和重复索引时，应该谨慎进行，确保不会对数据库的性能和功能产生负面影响。

#### D.慢查询

##### D.1查询开销的衡量

在MySQL中，衡量查询开销通常涉及以下几个步骤和指标：

1. **使用 `EXPLAIN` 或 `EXPLAIN ANALYZE`**

- **`EXPLAIN`**：这个命令可以用来查看MySQL如何执行一个查询语句。它会返回关于表的读取顺序、使用的索引、每个表的行数估计以及其他一些执行细节。
  ```sql
  EXPLAIN SELECT * FROM my_table WHERE id = 1;
  ```
- **`EXPLAIN ANALYZE`**（MySQL 8.0+）：这个命令不仅提供查询计划，还会实际执行查询并返回实际的执行时间和其他性能指标。
  
  ```sql
  EXPLAIN ANALYZE SELECT * FROM my_table WHERE id = 1;
  ```
2. **分析 `EXPLAIN` 输出**

以下是一些关键的指标和它们的意义：
- **`id`**：查询中每个SELECT语句的标识符。
- **`select_type`**：SELECT的类型，如SIMPLE（简单查询）、PRIMARY（外层查询）、UNION（UNION中的第二个或随后的SELECT）等。
- **`table`**：查询所涉及的表。
- **`type`**：连接类型，如ALL（全表扫描）、index（索引扫描）等。这个指标可以告诉你查询是否高效。
- **`possible_keys`**：可能用于查询的索引。
- **`key`**：实际使用的索引。
- **`key_len`**：使用的索引的长度。
- **`ref`**：显示索引的哪一列被使用了，如果可能的话，是一个常数。
- **`rows`**：MySQL认为必须检查的行数，这是一个估计值，而不是实际值。
- **`filtered`**：按表条件过滤的行百分比。
- **`Extra`**：包含MySQL解析查询的额外信息，如Using index（使用了覆盖索引）、Using where（使用了WHERE子句来过滤结果）等。
3. **使用 `SHOW PROFILE`（MySQL 5.7及以下版本）**

- 这个命令可以用来分析查询的性能，包括CPU和内存的使用情况。
  ```sql
  SET profiling = 1;
  SELECT * FROM my_table WHERE id = 1;
  SHOW PROFILES;
  ```
4. **监控性能变量**

- MySQL提供了一系列性能变量，可以通过`SHOW STATUS`命令来查看，这些变量可以帮助衡量查询的开销。
  ```sql
  SHOW STATUS LIKE 'Innodb_rows_read';
  ```

5. **实际执行时间**

- 使用`NOW()`函数在查询前后记录时间，可以计算出查询的实际执行时间。
  ```sql
  SET @start_time = NOW();
  SELECT * FROM my_table WHERE id = 1;
  SET @end_time = NOW();
  SELECT TIMEDIFF(@end_time, @start_time) AS query_time;
  ```
  通过上述方法，你可以全面地衡量MySQL查询的开销，并据此进行优化。记住，优化查询时，不仅要关注单个查询的性能，还要考虑整体系统负载和并发情况。

##### D.2查询切分与分解关联查询

**查询切分（Query Splitting）**

查询切分是将一个大查询分解成多个小查询的过程，每个小查询处理查询的一部分数据。这种方法通常用于处理大数据集，可以提高查询的性能。

1. **水平切分（Sharding）**

水平切分是将**数据行**按照一定规则分散到不同的表中。每个表只包含数据的一部分，这样查询时只需要访问包含所需数据的表。

2. **垂直切分（Skewing）**

垂直切分是将**数据列**分散到不同的表中。每个表包含数据的一个子集，这样查询时只需要访问包含所需列的表。

**分解关联查询（Decomposing Joins）**

分解关联查询是将一个复杂的关联查询分解成多个简单的查询，每个查询处理查询的一部分数据。这种方法可以减少查询的数据量，从而提高查询的性能。
1. **连接条件分解**

将一个复杂的连接条件分解成多个简单的连接条件，每个条件处理查询的一部分数据。

2. **表分解**

将一个大表分解成多个小表，每个小表包含数据的一个子集。查询时，只访问包含所需数据的表。

**示例**

假设有一个大表`orders`，包含大量的订单数据，我们想要查询特定日期范围内的订单信息。

**原始查询**

```sql
SELECT * FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-01-31';
```
**查询切分**

我们可以将这个查询分解成两个小查询，每个查询处理日期范围的一半数据。
```sql
SELECT * FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-01-15';
SELECT * FROM orders WHERE order_date BETWEEN '2023-01-16' AND '2023-01-31';
```
**分解关联查询**

如果`orders`表非常大，我们可以将订单数据分解成多个小表，每个表包含特定日期范围内的订单信息。
```sql
CREATE TABLE orders_jan_01_to_15 AS SELECT * FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-01-15';
CREATE TABLE orders_jan_16_to_31 AS SELECT * FROM orders WHERE order_date BETWEEN '2023-01-16' AND '2023-01-31';
SELECT * FROM orders_jan_01_to_15 WHERE order_id = 1;
SELECT * FROM orders_jan_16_to_31 WHERE order_id = 1;
```
通过查询切分和分解关联查询，我们可以提高查询的性能，尤其是在处理大数据集时。然而，这种方法也会增加数据库的管理和维护成本，因此在实际应用中需要权衡利弊。

#### E.分区表

##### E.1分区表demo

MySQL的分区表是用于管理大型数据集的一种技术，它可以提高查询性能、优化存储空间和简化维护工作。以下是关于分区表的具体实例分析：
 **实例**
假设我们有一个名为`sales`的表，它包含大量的销售记录，其中`sale_date`列是销售发生的日期。随着时间推移，表中的数据量会不断增加，这可能会导致查询性能下降。

```sql
CREATE TABLE sales (
    sale_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    quantity INT,
    sale_date DATE
);
```
 **分析**
 1. 未分区表的问题
- **性能下降**：随着表中数据量的增加，查询性能可能会下降，特别是那些涉及`sale_date`列的范围查询。
- **存储空间**：未分区的表会占用更多的存储空间，因为所有数据都存储在同一个文件中。
- **维护难度**：对于大型表，备份、恢复和数据迁移变得更加复杂和耗时。
 2. 分区表的优势
- **提高性能**：分区可以将表分成多个较小的部分，每个分区可以独立存储和查询，从而提高性能。
- **优化存储**：分区可以更有效地利用存储空间，因为每个分区可以存储在不同的文件中。
- **简化维护**：分区可以简化备份、恢复和数据迁移的工作，因为可以单独处理每个分区。
 分区策略
 1. 范围分区（Range Partitioning）
- **定义**：根据某个列的范围将表分成多个分区。
- **示例**：我们可以根据`sale_date`列的范围将表分成每月一个分区。
```sql
CREATE TABLE sales (
    sale_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    quantity INT,
    sale_date DATE
)
PARTITION BY RANGE (sale_date)
(
    PARTITION p0 VALUES LESS THAN ('2023-01-01'),
    PARTITION p1 VALUES LESS THAN ('2023-02-01'),
    PARTITION p2 VALUES LESS THAN ('2023-03-01'),
    -- 更多分区...
);
```
 2. 列表分区（List Partitioning）
- **定义**：根据某个列的值将表分成多个分区。
- **示例**：我们可以根据`product_id`列的值将表分成多个分区。
```sql
CREATE TABLE sales (
    sale_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    quantity INT,
    sale_date DATE
)
PARTITION BY LIST (product_id)
(
    PARTITION p0 VALUES IN (1, 2, 3),
    PARTITION p1 VALUES IN (4, 5, 6),
    PARTITION p2 VALUES IN (7, 8, 9),
    -- 更多分区...
);
```
 3. 哈希分区（Hash Partitioning）
- **定义**：根据某个列的哈希值将表分成多个分区。
- **示例**：我们可以根据`sale_id`列的哈希值将表分成多个分区。
```sql
CREATE TABLE sales (
    sale_id INT AUTO_INCREMENT PRIMARY KEY,
    product_id INT,
    quantity INT,
    sale_date DATE
)
PARTITION BY HASH (sale_id)
PARTITIONS 4;
```
 **结论**
通过使用分区表，我们可以有效地管理大型数据集，提高查询性能，优化存储空间和简化维护工作。在实际应用中，应根据具体的数据特性和查询需求选择合适的分区策略。

##### E.2分区表的维护

一旦表被分区，你将需要采取一些特殊的措施来处理这些分区，因为它们可能分布在不同的物理位置上。以下是一些处理分区表的常见操作：
 添加新分区
1. **使用 `ALTER TABLE` 添加新分区**：
   
   ```sql
   ALTER TABLE sales ADD PARTITION (PARTITION p1 VALUES LESS THAN ('2023-04-01'));
   ```
2. **使用 `CREATE TABLE AS` 添加新分区**：
   
   ```sql
   CREATE TABLE sales_p1 AS SELECT * FROM sales WHERE sale_date > '2023-03-31';
   ```
    **删除旧分区**
1. **使用 `ALTER TABLE` 删除旧分区**：
   
   ```sql
   ALTER TABLE sales DROP PARTITION p0;
   ```
2. **使用 `CREATE TABLE AS` 删除旧分区**：
   ```sql
   CREATE TABLE sales_p0 AS SELECT * FROM sales WHERE sale_date <= '2023-03-31';
   ```
    **重命名分区**
```sql
ALTER TABLE sales RENAME PARTITION p0 TO p0_new;
```
 **移动分区**：
在某些情况下，你可能需要将一个分区移动到不同的物理位置，例如，从一个硬盘移动到另一个硬盘。这通常通过将数据从原始位置移动到新位置，然后更新分区定义来实现。
 **合并分区**：
在某些情况下，你可能需要将多个分区合并成一个分区。这通常涉及到将多个分区的数据合并，并更新分区定义。
 注意事项

- **备份和恢复**：在执行任何分区操作之前，确保你有适当的备份，以防操作失败。
- **性能考虑**：某些分区操作可能需要大量的时间和资源，因此在执行之前，请考虑这些操作对系统性能的影响。
- **一致性**：在处理分区时，确保数据的一致性。例如，在添加新分区之前，确保没有旧数据丢失。
在处理分区表时，请仔细考虑操作的影响，并确保你有适当的计划和步骤来执行这些操作。

#### F.后端分页

##### F.1.基于Mybatis-plus实现的分页机制

MyBatis-Plus 是一个增强型的 MyBatis 框架，它提供了许多方便的 API 和功能，包括分页支持。MyBatis-Plus 的分页功能依赖于 MyBatis 的插件机制，因此需要在 MyBatis 的配置文件中添加相应的插件配置。
 1. MyBatis 配置文件
首先，我们需要在 MyBatis 的配置文件（通常位于 `src/main/resources` 目录下的 `mybatis-config.xml`）中配置分页插件。
```xml
<configuration>
    <plugins>
        <plugin interceptor="com.baomidou.mybatisplus.extension.plugins.MybatisPlusInterceptor">
            <property name="pagination" value="true"/>
        </plugin>
    </plugins>
</configuration>
```
 2. 业务场景
假设我们有一个 `User` 实体类，它有一个 `id` 字段作为主键，以及一个 `name` 字段。我们的后端需要提供分页查询接口，以便前端可以根据页码和每页显示的数量来获取用户列表。
 3. 创建分页查询接口
在 MyBatis-Plus 中，你可以使用 `Page` 对象来创建分页查询。以下是一个简单的分页查询接口示例：
```java
import com.baomidou.mybatisplus.core.metadata.IPage;
import com.baomidou.mybatisplus.extension.plugins.pagination.Page;
import com.baomidou.mybatisplus.extension.service.IService;
import com.example.model.User;
public interface UserService extends IService<User> {
    IPage<User> page(Page<User> page, UserQuery query);
}
```
 4. 创建分页查询实现
在 `UserService` 的实现类中，你需要实现 `page` 方法。这个方法将接收一个 `Page` 对象和一个 `UserQuery` 对象，其中 `UserQuery` 包含了查询条件。
```java
import com.baomidou.mybatisplus.core.metadata.IPage;
import com.baomidou.mybatisplus.extension.plugins.pagination.Page;
import com.example.model.User;
import com.example.query.UserQuery;
import com.example.service.UserService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
@Service
public class UserServiceImpl implements UserService {
    @Autowired
    private UserMapper userMapper;
    @Override
    public IPage<User> page(Page<User> page, UserQuery query) {
        return userMapper.selectPage(page, new LambdaQueryWrapper<User>()
                .eq(query.getId() != null, User::getId, query.getId())
                .like(query.getName() != null, User::getName, query.getName()));
    }
}
```
在这个实现中，我们使用了 MyBatis-Plus 的 `selectPage` 方法来执行分页查询。我们传递了一个 `Page` 对象和一个 `LambdaQueryWrapper` 对象，后者包含了查询条件。
 5. 前端调用分页查询接口
在前端，你可以使用 JavaScript 库（如 jQuery 或 Axios）来发送请求，并根据页码和每页显示的数量来获取用户列表。
```javascript
function fetchUserList(pageNumber, pageSize) {
    var query = {
        id: null,
        name: null
    };
    $.ajax({
        url: '/user/page',
        type: 'POST',
        data: JSON.stringify({ page: pageNumber, size: pageSize, query: query }),
        contentType: 'application/json',
        success: function(response) {
            // 处理返回的用户列表数据
        }
    });
}
```
在这个例子中，我们假设后端 API 的路径是 `/user/page`，它接受一个 JSON 格式的请求体，其中包含页码、每页显示的数量和查询条件。
通过这种方式，你可以使用 MyBatis和 MyBatis-Plus 来实现一个分页功能的后端服务。下面是完整的示例，包括前端调用分页查询接口的代码。

 **后端代码**
在 MyBatis-Plus 中，你可以使用 `Page` 对象来创建分页查询。
 **控制器代码**
在 Spring Boot 应用程序中，你需要创建一个控制器来处理前端发送的请求。

```java
import com.baomidou.mybatisplus.core.metadata.IPage;
import com.baomidou.mybatisplus.extension.plugins.pagination.Page;
import com.example.model.User;
import com.example.query.UserQuery;
import com.example.service.UserService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
@RestController
@RequestMapping("/user")
public class UserController {
    @Autowired
    private UserService userService;
    @PostMapping("/page")
    public IPage<User> page(@RequestBody UserQuery query) {
        Page<User> page = new Page<>(query.getPage(), query.getSize());
        return userService.page(page, query);
    }
}
```

### 9.4

#### A.MongoDB复习

##### A.1Why we choose MongoDB

**1.灵活的数据模型:**MongoDB使用一种名为BSON（Binary JSON）的文档存储格式，它允许存储复杂的数据类型，无需预先定义表结构，使得数据模型非常灵活。

**2.高性能:**MongoDB支持高性能的数据持久化，它通过内部机制，如内存映射、索引和副本集，来优化读写操作。

**3.高可用性:**MongoDB的副本集（Replica Sets）功能可以提供数据冗余和自动故障转移，确保了数据的高可用性。

**4.水平可扩展:**MongoDB支持分片（Sharding），这使得数据库可以轻松地通过添加更多的服务器来水平扩展，以支持大数据量的存储和高吞吐量的操作。

**5.丰富的查询语言:**MongoDB提供了丰富的查询接口，支持动态查询、数据聚合、文本搜索和地理空间查询等。

**6.部署的易用性**MongoDB设计简单，部署和维护相对容易，它不需要复杂的配置，支持自动修复和简单的备份操作。

**7.支持多语言客户端**：MongoDB有官方的驱动程序支持多种编程语言，如Java、Python、PHP、C#、C++、Node.js等，方便开发者使用。

**8.副本集的容错性**副本集可以容忍多个节点的故障，并且能够自动恢复，提高了系统的容错能力。

**9.无模式(Schema-less)**MongoDB的无模式特性意味着同一个集合中的文档不需要有相同的字段，这对于处理不断变化的数据结构非常有利。

**10.网格文件系统**MongoDB通过GridFS可以存储和检索超过16MB的文件，适合处理大文件存储。

##### A.2MongoDB的文件架构

- **数据文件（.ns和.data）**:
  - `.ns` 文件：存储命名空间信息，即集合（collection）和索引（index）的元数据。
  - `.data` 文件：存储集合中的实际数据。
- **日志文件（.log）**:
  - 日志文件记录了MongoDB的操作和系统事件，对于调试和审计非常有用。
- **配置文件（mongod.conf）**:
  - 配置文件包含了MongoDB实例的配置选项，如端口号、数据目录路径、日志路径等。
- **诊断文件（.diaglog）**:
  - 如果开启了诊断日志，MongoDB会生成.diaglog文件，用于记录数据库的操作细节。
- **索引文件（.0、.1、.ns等）**:
  - 索引文件用于存储集合的索引数据，它们通常以数字命名。
- **journal文件（.journal）**:
  - 如果启用了journaling（日志记录），MongoDB会使用journal文件来确保数据操作的持久性。在系统崩溃时，journal文件可以用来恢复数据。
- **锁文件（.lock）**:
  - 锁文件用于防止同一数据目录被多个MongoDB实例同时使用。
- **元数据文件（.metadata）**:
  - 存储有关 WiredTiger 存储引擎的元数据。
- **WiredTiger存储引擎文件（WiredTiger.wt等）**:
  - 对于使用WiredTiger存储引擎的MongoDB实例，这些文件用于存储数据和索引。

##### A.3MongoDB的调试工作

**调试MongoDB：**

调试MongoDB通常涉及以下几个步骤：
- **检查日志文件**:
  
  - MongoDB的日志文件通常位于`/var/log/mongodb/mongod.log`（在Linux系统上）。查看日志文件可以帮助诊断问题。
- **使用MongoDB shell**:
  
  - 启动MongoDB shell (`mongo`) 并执行各种命令来检查数据库状态，例如：
    ```javascript
    db.stats() // 查看数据库统计信息
    db.serverStatus() // 查看服务器状态
    ```
- **使用诊断命令**:
  - 使用`db.currentOp()`来查看当前正在进行的操作。
  - 使用`db.killOp()`来终止特定的操作。
- **配置诊断参数**:
  - 在配置文件中设置`diagnosticLogging`和`verbose`参数来增加日志的详细程度。
- **使用第三方工具**:
  - 使用如MongoDB Compass等工具来可视化数据库操作和性能分析。

**MongoDB CRUD操作：**

以下是MongoDB的CRUD（创建、读取、更新、删除）操作的示例：
- **创建（Create）**:
  ```javascript
  db.collection.insertOne({ name: "Alice", age: 25 }) // 插入单个文档
  db.collection.insertMany([{ name: "Bob", age: 30 }, { name: "Charlie", age: 35 }]) // 插入多个文档
  ```
- **读取（Read）**:
  ```javascript
  db.collection.find({}) // 查询所有文档
  db.collection.find({ age: { $gt: 30 } }) // 查询年龄大于30的文档
  db.collection.findOne({ name: "Alice" }) // 查询第一个名为Alice的文档
  ```
- **更新（Update）**:
  ```javascript
  db.collection.updateOne({ name: "Alice" }, { $set: { age: 26 } }) // 更新第一个名为Alice的文档的年龄
  db.collection.updateMany({ age: { $lt: 30 } }, { $inc: { age: 1 } }) // 将所有年龄小于30的文档年龄加1
  ```
- **删除（Delete）**:
  ```javascript
  db.collection.deleteOne({ name: "Alice" }) // 删除第一个名为Alice的文档
  db.collection.deleteMany({ age: { $gte: 35 } }) // 删除所有年龄大于等于35的文档
  ```
  在进行这些操作时，可以使用各种查询操作符来精确控制数据的操作。需要注意的是，CRUD操作可能需要适当的权限才能执行。

##### A.4MongoDB的索引相关


 索引的类型：

1. **单字段索引（Single Field）**：在文档的单个字段上创建索引。
2. **复合索引（Compound Index）**：在文档的多个字段上创建索引。
3. **多键索引（Multikey Index）**：用于索引数组类型的字段，可以对数组中的每个元素创建索引。
4. **文本索引（Text Index）**：用于文本搜索。
5. **哈希索引（Hashed Index）**：对字段值的哈希进行索引，主要用于分片。
 索引命令：
1. **创建索引**：
   ```shell
   db.collection.createIndex({ <field1>: <type>, <field2>: <type>, ... })
   ```
   例如，为`name`字段创建升序索引：
   ```shell
   db.users.createIndex({ "name": 1 })
   ```
2. **创建复合索引**：
   ```shell
   db.collection.createIndex({ <field1>: <type>, <field2>: <type>, ... })
   ```
   例如，为`name`和`age`字段创建复合索引：
   ```shell
   db.users.createIndex({ "name": 1, "age": -1 })
   ```
3. **列出集合的所有索引**：
   ```shell
   db.collection.getIndexes()
   ```
4. **删除索引**：
   根据索引名删除：
   ```shell
   db.collection.dropIndex(<index-name>)
   ```
   删除所有索引（除了`_id`索引）：
   ```shell
   db.collection.dropIndexes()
   ```
5. **解释查询计划**：
   使用`explain`来获取查询的执行计划，这有助于理解是否使用了索引：
   ```shell
   db.collection.find({ <query> }).explain("executionStats")
   ```
    索引的注意事项：
- **性能影响**：索引可以提高查询速度，但也会降低插入、更新和删除操作的速度，因为索引本身也需要维护。
- **存储空间**：索引需要额外的存储空间。
- **索引选择性**：选择那些能够有效区分文档的字段作为索引，以提高查询效率。
- **索引大小**：复合索引的条目数量受限于索引键的总大小，所有索引键的大小总和不能超过1024字节。
使用索引时，应根据实际的应用场景和查询模式来创建合适的索引，以达到最优的性能。

> 在MongoDB中，当你创建一个索引时，MongoDB会自动为该索引生成一个名称。默认情况下，这个名称是由索引的字段名和排序方向组成的。对于你提供的例子 `db.users.createIndex({ "name": 1, "age": -1 })`，生成的索引名大致会是这样的格式：
> ```
> name_1_age_-1
> ```
> 要找到具体的索引名，你可以使用 `getIndexes()` 方法列出集合中的所有索引，并查看每个索引的 `name` 字段。
> 以下是列出所有索引并找到特定索引名的步骤：
>
> ```shell
> db.users.getIndexes()
> ```
> 这会返回一个数组，其中包含所有索引的详细信息，包括每个索引的名称。你可以从中找到与你创建的复合索引对应的名称。
> 例如，输出可能会是这样的：
> ```json
> [
>   {
>     // 第一个索引对象
>     "v" : 2, // 索引的版本号。v:2表示这是MongoDB 2.2及以后版本的索引格式。
>     "key" : {
>       "_id" : 1 // 索引的键。这里表示_id字段上的索引，且排序方向为升序（1表示升序，-1表示降序）。
>     },
>     "name" : "_id_", // 索引的名称。对于_id字段，MongoDB默认创建一个名为_id_的索引。
>     "ns" : "yourDatabaseName.users" // 索引所在的命名空间。格式通常是数据库名.集合名。
>   },
>   {
>     // 第二个索引对象
>     "v" : 2, // 同上，索引版本号。
>     "unique" : false, // 布尔值，表示索引是否是唯一的。这里为false，意味着索引中的值可以重复。
>     "key" : {
>       "name" : 1, // 索引的键。这里表示name字段上的索引，且排序方向为升序。
>       "age" : -1 // 索引的键。这里表示age字段上的索引，且排序方向为降序。
>     },
>     "name" : "name_1_age_-1", // 索引的名称。MongoDB根据索引的字段名和排序方向自动生成。
>     "ns" : "yourDatabaseName.users" // 同上，索引所在的命名空间。
>   }
> ]
> 
> ```
> 在上面的输出中，`"name" : "name_1_age_-1"` 就是你需要用来删除索引的索引名。
> 现在，你可以使用这个索引名来删除索引：
> ```shell
> db.users.dropIndex("name_1_age_-1")
> ```
> 这将删除名为 "name_1_age_-1" 的索引。如果你不确定确切的索引名，一定要先通过 `getIndexes()` 方法检查，以免删除错误的索引。

##### A.5基于电商业务整合MongoDB命令

> 要将shell命令写入一个文件并执行它，你可以按照以下步骤操作：
> ### 步骤 1: 创建一个脚本文件
> 使用文本编辑器（如 `nano`, `vim`, `gedit` 等）创建一个新的文件。例如，我们可以创建一个名为 `script.sh` 的文件。
> ```shell
> nano script.sh
> ```
> ### 步骤 2: 写入shell命令
> 在打开的编辑器中，输入你的shell命令。例如：
> ```shell
> #!/bin/bash
> # 这是注释，说明这个脚本将使用bash来执行
> # MongoDB 示例命令
> mongo <<EOF #<<EOF 是一个Here文档的标记，它告诉shell接下来的文本块直到另一个 EOF 标记为止都应该被当作标准输入传递给 mongo 命令。
> use yourDatabase;
> db.collection.insertOne({ field: "value" });
> db.collection.find();
> EOF#Here文档结束: 这标记了Here文档的结束。所有在 mongo <<EOF 和这个 EOF 标记之间的文本都将作为标准输入传递给 mongo 命令。
> ```
> 确保在文件的第一行包含了 `#!/bin/bash`（或者适合你脚本的其他解释器路径），这被称为shebang，它告诉系统应该使用哪个解释器来执行这个脚本。
> ### 步骤 3: 保存并关闭文件
> 在 `nano` 中，你可以通过按 `Ctrl + X`，然后按 `Y` 来保存文件，最后按 `Enter` 确认文件名并退出编辑器。
> ### 步骤 4: 使脚本可执行
> 你需要给脚本文件设置执行权限：
> ```shell
> chmod +x script.sh
> #chmod:change mode的意思
> #+x +是添加 x是execute permission也就是添加执行权限
> #script.sh 是一个shell脚本文件
> ```
> ### 步骤 5: 执行脚本
> 现在，你可以通过以下命令来执行你的脚本：
> ```shell
> ./script.sh
> ```
> 确保你位于包含 `script.sh` 文件的目录中，或者提供脚本的完整路径。
> ### 注意事项：
> - 如果你的脚本需要以root权限运行，你可能需要使用 `sudo` 来执行它：`sudo ./script.sh`
> - 如果你的脚本中包含对其他用户不安全的命令，或者你需要以其他用户身份运行，确保你知道你在做什么。
> - 如果你在Windows系统上编写脚本，你可能需要创建一个批处理文件（`.bat`）或PowerShell脚本文件（`.ps1`），并且使用不同的命令和语法。
> 以上就是将shell命令写入文件并执行的基本步骤。

在电商业务中，MongoDB可以用来存储和查询各种类型的数据，例如用户信息、商品信息、订单信息等。以下是一些与电商业务相关的MongoDB命令和知识点的讲解：

**1. 创建集合（Collection）**

首先，你可能需要为不同的数据类型创建集合：
```shell
db.createCollection("users")       # 创建用户集合
db.createCollection("products")    # 创建商品集合
db.createCollection("orders")      # 创建订单集合
```
**2.插入文档（Insert Documents）**

插入数据到集合中：
```shell
# 插入用户
db.users.insertOne({
  username: "user123",
  email: "user123@example.com",
  password: "hashed_password",
  createdAt: new Date()
})
# 插入商品
db.products.insertOne({
  name: "Product Name",
  category: "Category",
  price: 99.99,
  stock: 100,
  description: "Product Description"
})
# 插入订单
db.orders.insertOne({
  userId: ObjectId("507f191e810c19729de860ea"), # 假设userId是用户的ObjectId
  productId: ObjectId("507f1f77bcf86cd799439011"),
  quantity: 2,
  status: "pending",
  createdAt: new Date()
})
```
3. **创建索引（Create Indexes）**

为了提高查询性能，可以在常用查询的字段上创建索引：
```shell
# 在用户集合的username字段上创建唯一索引
db.users.createIndex({ username: 1 }, { unique: true })
# 在商品集合的name字段上创建文本索引，以便进行全文搜索
db.products.createIndex({ name: "text" })
# 在订单集合的userId和status字段上创建复合索引
db.orders.createIndex({ userId: 1, status: 1 })
```
4. **查询数据（Query Data）**

执行各种查询操作：
```shell
# 查询特定用户的所有订单
db.orders.find({ userId: ObjectId("507f191e810c19729de860ea") })
# 查询类别为"Electronics"的所有商品
db.products.find({ category: "Electronics" })
# 查询价格低于50元的所有商品
db.products.find({ price: { $lt: 50 } })
# 使用文本索引进行全文搜索
db.products.find({ $text: { $search: "Product Name" } })
```
5. **更新数据（Update Data）**

更新集合中的文档：
```shell
# 更新商品库存
db.products.updateOne(
  { _id: ObjectId("507f1f77bcf86cd799439011") },
  { $inc: { stock: -2 } } # 假设卖出了2个商品，库存减少2
)
# 更新订单状态
db.orders.updateOne(
  { _id: ObjectId("507f1f77bcf86cd799439012") },
  { $set: { status: "shipped" } }
)
```
6. **删除数据（Delete Data）**

从集合中删除文档：
```shell
# 删除特定订单
db.orders.deleteOne({ _id: ObjectId("507f1f77bcf86cd799439012") })
# 删除所有已完成的订单
db.orders.deleteMany({ status: "completed" })
```
**注意事项：**

- 使用 `ObjectId` 来引用其他集合中的文档。
- 对于敏感信息（如密码），应当存储其哈希值，而不是明文。
- 索引可以显著提高查询性能，但会占用额外的存储空间，并且会稍微减慢写操作的速度。
- 使用 `$text` 索引进行全文搜索时，确保字段已经创建了文本索引。
这些命令和知识点是MongoDB在电商业务中常用的基础操作。在实际应用中，你可能还需要处理更复杂的查询、聚合操作、事务处理等。

##### A.6聚合管道操作符

MongoDB的聚合操作是一种用于处理数据并返回计算结果的方法，类似于SQL中的GROUP BY语句。聚合操作可以对集合中的数据进行分组、转换、计算等操作，最终输出一个结果集。
在MongoDB中，聚合操作主要通过聚合管道（Aggregation Pipeline）来实现。聚合管道是一系列的数据处理阶段，每个阶段对输入的文档序列执行特定的操作，并将结果输出到下一阶段。下面是一些常用的聚合管道操作符和它们的功能：
 聚合管道操作符：
1. **$match**：
   - 用于过滤文档，只输出符合条件的文档。
   - 类似于SQL中的WHERE子句。
2. **$group**：
   - 用于将集合中的文档分组，可以对分组后的数据进行聚合。
   - 类似于SQL中的GROUP BY。
3. **$project**：
   - 用于重塑每个文档的结构，选择、添加或删除字段。
   - 可以用来创建计算字段。
4. **$sort**：
   - 用于对输入的文档进行排序。
5. **$limit**：
   - 用于限制聚合管道返回的文档数。
6. **$skip**：
   - 用于在聚合管道中跳过指定数量的文档。
7. **$unwind**：
   - 用于将数组类型的字段拆分成多个文档。
8. **$out**：
   - 将聚合管道的结果输出到一个新的集合中。
    重塑文档示例：
     假设我们有一个名为 `orders` 的集合，其中包含以下文档：
```json
{
  "_id": ObjectId("507f191e810c19729de860ea"),
  "customer": "Alice",
  "orderDate": ISODate("2023-01-01T00:00:00Z"),
  "items": [
    { "name": "T-shirt", "price": 20, "quantity": 2 },
    { "name": "Jeans", "price": 40, "quantity": 1 }
  ]
}
```
以下是一个聚合管道的示例，它将重塑文档以计算每个订单的总价：
```javascript
db.orders.aggregate([
  {
    $project: {
      customer: 1,
      orderDate: 1,
      total: { $sum: "$items.price" }
    }
  }
]);
```
在这个例子中，`$project` 阶段用于添加一个新的字段 `total`，它是通过计算数组 `items` 中每个项目的 `price` 字段的总和得到的。
 聚合管道操作步骤：

1. **$match**：首先，你可以使用 `$match` 来过滤出特定条件的文档。
2. **$group**：然后，使用 `$group` 来对文档进行分组，并计算每个分组的总和、平均值等。
3. **$project**：接下来，使用 `$project` 来重塑文档，选择需要的字段，并可以添加计算字段。
4. **$sort**：使用 `$sort` 来对结果进行排序。
5. **$limit** 和 **$skip**：最后，可以使用 `$limit` 和 `$skip` 来限制结果的数量或跳过一些文档。
聚合管道是非常强大的，可以执行复杂的操作来处理和分析数据。在实际应用中，这些阶段可以组合使用，以完成复杂的数据处理任务。

> 以下是一个基于电商场景的MongoDB聚合管道操作示例，该示例尽可能使用了各种聚合管道操作符。假设我们有一个名为`orders`的集合，该集合包含了电商平台的订单数据。
> ```javascript
> db.orders.aggregate([
>     {
>         $match: {
>             status: "completed"  // 筛选出已完成状态的订单
>         }
>     },
>     {
>         $group: {
>             _id: "$customerId",  // 按客户ID分组
>             totalAmount: { $sum: "$amount" },  // 计算每个客户的总消费金额
>             averageAmount: { $avg: "$amount" },  // 计算每个客户的平均消费金额
>             orderCount: { $sum: 1 }  // 统计每个客户的订单数量
>         }
>     },
>     {
>         $sort: {
>             totalAmount: -1  // 按总消费金额降序排序
>         }
>     },
>     {
>         $limit: 10  // 取消费金额最高的前10个客户
>     },
>     {
>         $lookup: {
>             from: "customers",  // 从customers集合中查找客户信息
>             localField: "_id",
>             foreignField: "_id",
>             as: "customerInfo"
>         }
>     },
>     {
>         $unwind: "$customerInfo"  // 展开customerInfo数组
>     },
>     {
>         $project: { //控制输出文档的结构
>             _id: 0,  // 不显示_id字段
>             customerId: "$_id",
>             customerName: "$customerInfo.name",
>             totalAmount: 1,
>             averageAmount: 1,
>             orderCount: 1
>         }
>     },
>     {
>         $out: "top_customers"  // 将结果输出到新的集合top_customers
>     },
>     {
>         $facet: { //在同个聚合管道阶段中同时进行多个分组操作。输出多个分组的结果
>             stats: [
>                 { $count: "total" }  // 统计总客户数
>             ],
>             categories: [
>                 {
>                     $unwind: "$customerInfo.categories"  // 展开客户类别
>                 },
>                 {
>                     $group: {
>                         _id: "$customerInfo.categories",
>                         count: { $sum: 1 }  // 统计每个类别的客户数量
>                     }
>                 }
>             ]
>         }
>     },
>     {
>         $bucket: {//将输入文档根据指定的表达式和边界划分为桶 对每个桶内文档引用聚合 
>             groupBy: "$averageAmount",  // 按平均消费金额分组
>             boundaries: [0, 100, 200, 300, 400, 500, 600],  // 分组边界
>             default: "Other",  // 默认分组
>             output: {
>                 count: { $sum: 1 },
>                 averageAmount: { $avg: "$averageAmount" }
>             }
>         }
>     }
> ]);
> ```
> 这个聚合管道操作包含了以下步骤：
> 1. `$match`：筛选出已完成状态的订单。
> 2. `$group`：按客户ID分组，并计算总消费金额、平均消费金额和订单数量。
> 3. `$sort`：按总消费金额降序排序。
> 4. `$limit`：取消费金额最高的前10个客户。
> 5. `$lookup`：从customers集合中查找客户信息。
> 6. `$unwind`：展开customerInfo数组。
> 7. `$project`：选择要显示的字段。
> 8. `$out`：将结果输出到新的集合。
> 9. `$facet`：对数据进行分组统计，包括总客户数和每个类别的客户数量。
> 10. `$bucket`：按平均消费金额分组，并计算每个分组的客户数量和平均消费金额。

### 9.6

1.英语复习89新20

2.算法：DP-找出乘积最大的子数组的积

JUC（Java并发工具）:

- 操作系统管理进程，而进程可以包含多个线程（轻量级进程）。
- 传统GUI程序使用主事件循环，其缺点是如果有耗时操作会阻塞整个界面。解决方法是使用事件分发线程（Event Dispatch Thread, EDT）。
- 进程之间内存空间是独立的，而同一进程中的多个线程共享内存空间。
- 线程安全：当多个线程访问某个类时，该类始终能表现出正确的行为。
- 有状态：如果在堆区开辟了共享实例，例如 `private long count = 0;`，则该变量为全局变量。
- 有状态可能导致线程不安全：策略A是使用 `AtomicLong`，策略B是对涉及 `count` 的方法使用 `synchronized`。
- 竞态条件：程序的执行结果依赖于线程的执行时序或者事件的顺序，可能导致不一致的结果。
- 延迟初始化：如 `Instance s = null;`，在方法中延迟创建实例。若 `s` 没有被 `volatile` 修饰，可能会在线程间导致不一致的结果。
- 内置锁：通过 `synchronized` 关键字实现。
- 可重入：一个同步方法可以调用另一个同步方法，即使是嵌套调用，而不会发生死锁。
- `synchronized` 的特性：实现操作的原子性和保证内存可见性（所有读写操作都必须在同一个锁上同步）。
- 重排序：在没有同步化的情况下，编译器、处理器、运行时都可能会调整操作的执行顺序。
- 非原子64位操作：早期处理器无法原子地处理64位操作，如 `long` 和 `double`。解决方法是使用 `volatile` 关键字确保操作的原子性。
- 发布与逃逸：如果一个对象在方法外被访问，则称其逃逸出该方法。
- 匿名内部类发布：可能会导致外部类被逃逸，因此使用工厂模式来创建对象。
- 线程封闭：通过避免不同线程间的数据共享来避免同步问题。例如，Swing的线程分发机制。
- `ThreadLocal`：为每个线程提供一个单独的实例，例如数据库连接 `ThreadLocal<Connection>`。
- `final` 修饰的变量：`final` 变量本身是线程安全的，但其所引用的对象的状态不一定线程安全。

ElasticSearch:

- ElasticSearch 是基于 Lucene 的分布式文档型搜索引擎。

- 在早期版本中，ElasticSearch 的逻辑结构是索引包含类型，类型包含文档。但从 7.0 版本开始，类型已被废弃，索引直接包含文档。

- TF-IDF 算法：词频-逆文档频率，用于评估一个词在文档中的重要性。

- 逻辑设计：文档被类型分组，类型类似于 SQL 中的表格，包含行（文档）。一个或多个类型存在于索引中，类似于多个表格存在于数据库中。

- 物理设计：索引由多个分片组成，主分片和副本分片可以分布在不同的节点上。默认情况下，每个索引有5个主分片，每个主分片有一个副本分片。

- 文档用 JSON 表示，ElasticSearch 处理的最小单元是分片。

- ElasticSearch 索引物理上被分割成多个分片，每个分片是一个独立的 Lucene 索引。

- 倒排索引：通过词条快速找到包含它们的文档。

- 随着机器节点的增加，ElasticSearch 集群可以水平扩展，实现负载均衡。

- 文档在索引中根据其 ID 进行散列分配到特定的分片上，不是随机分布，确保了分片的均匀使用。

- 索引例子：以下是一个正确的 curl 命令，用于在 ElasticSearch 中创建一个文档

- **数据类型**：ElasticSearch 支持多种数据类型，包括字符串、数字、日期、布尔值、地理空间数据等。正确地定义字段类型对于搜索和分析至关重要。

- **分析器**：在索引和搜索过程中，分析器用于处理文本字段。它包括字符过滤器、分词器和令牌过滤器，用于将文本转换为词条。

- **缓存**：ElasticSearch 使用多种缓存来提高性能，包括字段数据缓存、查询缓存和请求缓存。

- **监控与日志**：ElasticSearch 提供了内置的监控工具，可以监控集群的健康状况、性能指标和日志信息。

- **安全**：ElasticSearch 提供了安全功能，包括用户认证、角色权限控制、加密通信等。

- **插件**：ElasticSearch 允许通过插件来扩展其功能，例如，Kibana 插件用于数据可视化，X-Pack 插件提供安全、监控、告警等功能。

- **集群伸缩**：ElasticSearch 集群可以通过增加节点来伸缩，以处理更多的数据和更高的查询负载。

- **故障转移**：当主分片所在的节点发生故障时，副本分片可以提升为主分片，确保集群的持续可用性。

- **索引模板**：索引模板可以用于自动为新创建的索引设置映射和设置，简化索引管理。

- **快照和恢复**：ElasticSearch 支持创建索引的快照，并可以将快照恢复到新索引中，用于备份和灾难恢复。

  - ```sh
    curl -X PUT "http://localhost:9200/get-together/group/1?pretty" -H 'Content-Type: application/json' -d'
    {
      "name": "ElasticSearch Denver",
      "organizer": "Lee"
    }'
    ```

    



# 10.2日报

1.力扣：合并两个有序链表-以前学的是L1指针 L2指针 cur指针，选小的给cur用。这次自己想了一个纯暴力法，时间复杂度高，不过自己能独立想出思路也算不错

2.六级单词新20复115

3.计网相关：

A.RPC本用做C/S架构通讯,HTTP比RPC晚,RPC比较定制化,但后续Web浏览器流行,需要有统一的规则,于是HTTP替代了RPC,RPC退出了外网通讯舞台,但是由于比HTTP的头信息更少,信息冗余少,因此用于微服务组件通讯.http2.0也是RPC的底层之一,一般是基于TCP

B.WebSocket是在http建立成功后，在消息头用connection:upgrade 并且配置upgrade为websocket的实现的,注意要带一个base64编码的密钥,这个用于C/S校验.WebSocket全双工.替代了原本的长轮训or不断轮循(百度网盘验证码登录的原理就是这个,不断发http请求对客户端负载大)

4.OS相关:

1.图灵机：纸带+读写头+(储存/控制/运算)

2.32和64位得分CPU和总线来看,64位AMD和因特尔基本都是48位寻址,64位没必要.软件64位不能在32位机器上面跑,因为指令集不兼容,但是32位软件加兼容代码可以在64位机器跑.CPU的32和64看的是地址总线,传输总线,数据总线的宽度。

3.寄存器：通用寄存器,指令寄存器(不存地址存当前命令),程序计数器(存下一个地址)->Cache,注意这是SRAM,L1分数据和指令,L2不分,L1L2内核私有,L3共享.后面就是内存和硬盘

4.想让代码跑的快,二维数组得连续,因为cache读的是cacheline,一般64kb,这是从数据上做优化,从指令上优化就是尽可能排序,让分支预测器优化,给数排序后再读入Cache里面.切记：就是让代码容易被缓存命中，命中了就跑得快了。

5.缓存一致性：写直达问题不大,直接写memory,坏处就是慢.写回倒是快了,就是脏数据容易导致两个核的缓存不一致。怎么办？总线嗅探，实时让核心去问旁边的核是不是一致的，不一致就改。坏处是太消耗性能了。基于这个想法

6.MESI协议,M修改,E独享,S共享,I无效.先独享,有人读了就共享,我改了另外一个就无效,无效了就继续重新读

7.因为cacheLine读的是64kb（默认64kb,）,万一AB都在一个cacheline里面，特别是在结构体经常发生,那AB就容易出现伪共享：说白了就是缓存了跟没缓存没什么区别，属于是看似移动到缓存，但是最后又得重新读。所以你得做内存对齐，别让cacheline一次性给你把两个core里面放的数据都读进去了,策略是在cpp代码里面写宏定义就行

8.调度优先级,deadline,realtime,fair.越急值越小 fair就是公平了

9.**在 CFS 算法调度的时候，会优先选择 vruntime 少的任务**，类似一堆杯子里面奶茶少的杯子就加点奶茶，加奶茶就是调用你，调用你vruntime就增加。我就一定先用vruntime少的任务。

10.SCHED_DEADLINE：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度； SCHED_FIFO：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以「插队」； 

SCHED_RR：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；

 而 Fair 调度类是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略： 

SCHED_NORMAL：普通任务使用的调度策略；

 SCHED_BATCH：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级

# 10.3日报

1.LC:复习了BFS,用bfs做了计算二叉树深度,很久没写dfs了,果然手生了

2.六级单词新20复习100

3.工程知识：

A.就[多类型场频搭建统一发奖服务]的场景,重构if-else的冗余代码为工厂模式,即先设计统一的发奖接口,再针对优惠卷,月卡等不同具体商品的发奖设计发奖Service的实现类,然后编写工厂类,工厂类提供多个方法来new具体的发奖服务,接着拿到new出来的发奖服务再去调用service里面的方法。然后写了单元测试

B.就[替换Redis双集群升级]-原本服务A和服务B的调用redis工具类不同,现在需要整合到一个集群中统一调用。策略是设计服务A的Adapter类,服务B的Adapter类.这两个类都实现了adapter接口.adapter接口的包含了get和set方法.而服务的Adapter类就是Override上述方法,然后通过new一个A服务,在get方法中调用A服务的getX()方法。接着就是拿JDK动态代理去新建代理对象，测试类通过jdkproxy拿到代理对象，接着就对对象进行操作。这体现了抽象工厂模式。

4.操作系统:

A.宏内核，包含多个模块，整个内核像一个完整的程序；+微内核，有一个最小版本的内核，一些模块和服务则由用户态管理；+混合内核，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序；Linux 的内核设计是采用了宏内核，Window 的内核设计则是采用了混合内核。

B.SMP对称多处理，代表着每个 CPU 的地位是相等的，对资源的使用权限也是相同的，多个 CPU 共享同一个内存，每个 CPU 都可以访问完整的内存和硬件资源。

C.ELF它是 Linux 操作系统中可执行文件的存储格式

D.0.1+0.2!=0.3原因是。计算机处理小数的策略是乘2取整法，但是0.1是无限循环的,所以只能截取部分精度。

E.为什么负数要用补码？-3+1直接算就是-4,用了补码避免将负数转化为减法才能得到-2,说白了就是得统一计算方式，减少资源浪费。

F.虚拟内存是一种内存管理技术，使程序可以使用比物理内存更大的地址空间。页表是操作系统用来映射虚拟地址到物理地址的数据结构，它记录了每个虚拟页对应的物理页框。多级页表是一种优化的页表结构，通过分层管理减少内存占用，提高访问效率，适用于大规模虚拟地址空间。段页是一种内存管理方式，结合了段式管理和分页管理的优点。它将程序分为多个段，每个段可以进一步划分为固定大小的页，从而实现灵活的内存分配和保护。通过这种方式，可以提高内存的利用率，同时简化地址转换过程。

G.操作系统的进程形式，状态，通信方式，调度算法。操作系统的中断，系统调用....and so on...

# 10.5日报

1.LC继续做了一道easy的BFS,二叉树翻转

2.六级新20复习109个

3.Mybatis源码-看到XML解析部分+一点点执行器

4.设计模式-原型模式：顺序不同的卷子 单例模式-懒汉(安全+不安全)饿汉+内部类+双重锁+枚举(枚举是最推荐的)+建造者模式(房屋装修选配场景)

5.看了点http,复习了状态码,补充了强制缓存和协商缓存的知识..

今天看不太进去东西,吃完晚饭倒头就睡,睡了两三个小时脑子昏昏沉沉.

下午也是时间多用作娱乐了

感觉呆寝室里学习久了很无趣，也没什么动力出去玩。学校太偏僻了，感觉但凡去个略微繁华的地方都得浪费很多时间通勤，算了，还是呆寝室吧，太懒了。

# 10.9日报

1.六级新学20复习108

2.LC完成BST树插入节点：依旧是递归,我错误的原因在于想的过于复杂,只用考虑 A.判断root.val与val的关系从而令root.left or right被递归结果赋值 B.最后的叶子结点处插入,不应该单独用if判断,而是直接放在root为Null的If中.我的思维惯性是root为null就返回null,而插入节点恰好就得在没有子节点之处插入.还是思维太僵化了,很不灵活,很悲哀.

3.看了一部分计组

4.比较详细的看了MySQL的锁部分：

A.全局锁(flush tables with read Lock) 表锁(表锁-元数据-意向锁-auto-inc锁) 行级锁(GapLock RecordLock 以及缝合上述二者的Next-KeyLock)

B.加锁的对象是索引

C.S锁共享 X锁独占...

D.一些具体的代码实操,lock in share mode与for update对应S与X

E.MySQL加行级锁的详细分析,唯一索引等值与范围查询,非唯一索引的等值与范围查询,没有加锁的查询.这一块比较特别的是,所谓的非唯一索引就是二级索引了,二级索引加锁得同时加二级索引和对应的主键索引.这一块说实话没太学明白,明日有空继续研读,还有一块就是gapLock的偏移量一般是5,经验主义得到的.当然,如果gap不在最左或者最右,那就给区间加上锁,反之如上.

F.生产事故-update在生产机切记得用索引,否则慢SQL

5.复习适配器模式-学习场景-多个Mq消息体去提取指定字段值-这一块复杂的逻辑在于,假若我有3个pojo类,我希望将A类的a字段与B类的c字段建立关联,会使用link,涉及一些反射操作. 适配器模式的本质就是提供一个更广泛的接口,让一个接口能兼容多种差异

6.复习桥接模式-多渠道支付与多支付模式-最笨的写法就是传两个参数进来,一个channel一个mode,然后channel里面套mode的if,例如在微信的if里面写若干个mode来表示多支付模式.这么干的坏处就是,支付宝也得写3个mode,明天我加个云闪付是不是也得写3个,那支付平台这么多,这么写不得累死么,而且后人维护起来挺麻烦.桥接模式就是做抽离工作,你的mode和channel得分开,既然那么多渠道共享相同的若干mode,我为何不单独写好mode,然后你们新增平台后,我调用mode即可.这样就实现了解耦合.



# 10.10日报

1.六级复习109新增20

2.LC: 给一串字符串,问最长构造的回文串是多少.策略是hashset,contains存在就意味着有两个相同，则长度+2.最后把偶数个的字符都测完后,set中剩下的全都是字符串中只有一次出现的字符,那我随机取一个放在回文中间即可，就是+1. 上述操作的三目运算放在最后return时候就行了

3.MySQL

A.补了一个一直没意识到的知识漏洞,聚合函数如果和字段一起用,得将一起用的字段group by.

B.一些MySQL中细节的储存结构-表-分段-块-页-行,以及建表操作导致的底层储存原理,譬如not null可以节省一个byte空间.

4.熟悉了一些Python语法,刷了点题

5.今天是真的摆完了,明天得振作些了,天天满课是真疲惫啊

# 10.11日报

1.六级复习102新增20

2.算法：完成-最大数-思路：int数组转为String数组,自定义排序规则,通过compareTo比较AB和BA两种拼接方式的值,使得两两相近的局部拼接结果和最大,使用while解决前导零问题,最后substring裁剪除去前导零索引后的正式值

3.MySQL学习：

A.黑马书P1-P101,梳理了知识体系,将数据库理论与MySQL实操进行结合,对先前项目中的零散知识做了归纳.

4.Python学习：

A.对PyGame库进行了API的认知,完成了简单的框内移动精灵的游戏demo

5.Java学习：

A.设计模式1：学习组合模式-营销差异化场景-决策树引擎搭建场景的代码训练，通过对传统if-else代码的重构，实现松耦合。

B.设计模式2：学习装饰器模式-SSO单点登录的功能拓展场景，增加了拦截用户访问方法的范围场景，重构强耦合代码

# 10.12日报

1.六级复习104，新增20

2.算法：计算投递简历，AB两个HR检测到相同简历顺序的期望。实际是将整个int数组进行去重，然后返回去重后的长度。证明部分是围绕：为什么去重后的数组长度等于期望

3.MySQL：

A.黑马MySQL教程P102-P201,一天的时间都用来阅读和复现代码了,收获很大,知识相对之前的零散状态更加体系化了,不得不说,黑马出的书籍是很浅显易懂的.我读国外经典的中译本的效率,是远远不如阅读这种商业化培训机构在市场中卷出来的资料的效率的.

# 10.13日报

1.六级复习84，新增20

2.算法：滑动窗口easy-给定一个二进制字符串,以及一个数字k,要求找出字符串,满足条件为,0出现次数<=k ||1出现次数<=k.思路是把字符串转char数组,用cnt数组,0索引表示0的数量,1索引表示1的数量,遍历char数组,拿cnt[char[i]&1]++.然后遇到cnt[char[0]]>k&&cnt[char[1]]>k就移动左边界left,然后让cnt减法(左边界移动了自然滑动窗口里面的0或者1就减少了)具体实现:cnt[char[left++]]--;单次for拿到的子数组数量+=到ans中,具体表达是 i-left+1.最后return ans就行了

3.MySQL

A.黑马书P201-P306,这本书基本读完了,启发比较大的是:数据库编程/控制流程/相关一些MySQL支持的API/函数/存储过程/游标/事件/水平和垂直分表/分区技术/Innodb三个特点：事务/外键/行级锁。总的来说，这本书写的非常接近实践。同时唤醒了我半年多前的关于MySQL的具体实操的记忆。太久没写过sql了，许多知识点的遗忘是客观的，这些缺漏被这本书提供的知识补齐了。

4.复习了MongoDB相关八股,翻看自己的技术存档,发觉上一次复习MongoDB还是9月4日,如今已经一个多月过去了,时光的流逝真是令人慨叹啊。

5.展望与规划：

1.十一月之前把Spring Microservices in action读完,体系化一下微服务的知识。

2.读Redis in action

3.多复盘场景题

# 10.14日报

1.六级新增20个，复习108

2.LC：给定一个数组，与k值。若k>0，每个元素替换为其后k个元素相加的和，若k<0， 每个元素替换为前k个元素相加的和，若k=0,初始化为0。且数组为环形，需要考虑到边界问题，策略是对于k<0情况，索引设计为(i-j+n)%n，对于k>0情况索引设计为(i+j)%n，使用%n来确保索引始终在n的范围内

3.Redis

A.归纳整理了Redis的三种缓存模型，输出了博客，录制了视频。这一块记忆会比较深刻

4.JavaSE

A.把学校发的SE书简单过了1/3，补齐了一些平时用的较少的API，对于细微处的遗忘拾起记忆。

5.整体这一天过得比较松弛，可能是因为一上午没有课的缘故。

# 10.16日报

1.六级新增20个，复习102个

2.LC做了一道中等难度，给定数组与标准k,要求连续子数组相乘乘积小于k,策略是维护一个滑动窗口,左边界初始化为0，右边界也初始化为0，我们for循环先动右边界，左边界的移动是被迫的。伴随右边界移动，滑动窗口框起来的数字越来越多，就有可能相乘结果大于等于k，一旦遇到这种情况，左边界就得开始动了，我们这里的动分两步，一方面是总乘积里面得除去一个左边界上的元素，另外一方面是左边界的索引需要自增一位，且这个行为得是while，不能是if,为什么？如果你运气够好，左边界移动一位你的总乘积就回到比k小状态了，那是最好的。但是倘若运气不够好呢？倘若这个数组设计的右边界的新值很大呢?因此我就得在该for循环里面不断while，直到乘积<k。然后每次for走完一遍，都将right-left+1加到ans里面。这是一个通用知识，在很多问：子数组个数+滑动窗口的题目里面都是这样处理的。本质是计算右边界为right，左边界为left，二者围成的滑动窗口的子数组组合可能性。

3.JSP/Servlet阅读-林信良书P1-P73，从下午读到晚上，虽然已经是十五年前的书了，但是对于jsp这种古老技术的描述的确很细致，从整个项目结构的文件功用，到宏观的架构设计，循序渐进。我认为最可取的一点是，介绍是渐进式的，没有一上来就灌输一大堆定义，让大脑摄取的知识存在逻辑上的推导连贯性。就目前来看，这本书写的不错。

4.黑马的NoSQL阅读，上午看了一节课，大致把MongoDB的开端复习了一下，然后收获比较大的是BASE理论，最终一致性,CAP之类的纯理论知识，先前在背面经时候遇到，但是没有做系统性整理，这里给出了表格和划分。有时间可以多读读，最近要读的书太多了，有点应接不暇了。

5.Vue.js实战，回寝室后做了7个简单lab,大致是重温了组件传值,然后进阶语法里面把四个常用全局属性的demo敲了，不得不说,Vue2的有些东西被Vue3舍弃不是没有理由的。

6.总结：

我的整个大一对技术的学习是急于求成的，成果很显然，养成了干活的能力。但是缺陷在于：我对技术的发展沿革没有细致的了解，就像我知晓怎么用SpringBoot做crud,但是我对ssm做crud是相对生疏的，对用Servlet+jsp做网站是更加生疏的。我先前的所作所为仅仅是满足了目前市场最广泛应用的技术需求，但是这显得太浮于表面了。

我愈发觉得，技术发展的沿革就是在复杂的技术上面修建新的建筑，让这个复杂的技术更加易用，但利弊均衡，易用性的代价是牺牲了灵活性，丧失了对更底层设计的细致控制。

沉下心来老老实实把那些不再直接被使用，但是依旧在新兴框架之中发挥着作用的陈旧技术学好，理应是我大二上学期的主旋律。

# 10.17日报

1.力扣一道easy。题意是给定一个数n，判断n是否完全由给定的数乘得，譬如给定2,3,6.那我输入6就满足，输入17就不满足。我们转化思路：让n依次除2,3,6就行了。但是考虑到一次可能不到位，那我对于2,3,6都得分别做一个while,条件是n%2==0，对于3和6同理。然后想起来得考虑特殊情况，如果n比1要小，直接return一个false就行。然后如果一个数n能挺过三个while，最终等于1，就说明这个数完全就是可以用2或3或6任意取数做乘法组成的。

2.六级新增20个，复习106个。

3.JSP书从P74读到P203，大致把Servlet相关的Listener,Filter看完了，看了一部分jsp的语法。感触是：古老的东西用起来确实挺不方便的，但是能想出来这样设计的人，挺牛的。

4.Vue做Lab，全局属性，实例属性，全局配置一共13个lab做掉了，组件进阶没太看懂，createElement目前没在业务场景里面遇到过，先不管了，以后遇见再说。然后走马观花的看了transition的用法，感觉用处不大，因为这种渐变效果大概率是用其他组件库来做，我觉得Vue的定位更像是一个工程框架，至于渐变这种视觉层的东西，用第三方封装的库肯定比自己写要好，还是以工程建模为主线。然后敲了五个关于路由的lab,这路由以前也用过，但是感觉认知还是太浅了，也发觉忘掉了许多东西，明日有闲暇再多看看吧。

5.总结：

效率还行，整体来说学习的痛感不强烈。jsp书读了大半天还是有些枯燥，明天读NoSQL+JSP换换口味。这理论知识啊，就得交叉轮换复习，不然早晚都忘掉。

# 10.19日报

1.力扣一道easy,题意是：在一个无环无向图中，给定二维数组edges，找出哪一个节点是中心节点（该图中心节点仅有一颗，与其他节点相连）。实际考察的是度的概念。倘若有n个节点，则希望我找到度为n-1的节点。策略是将整个edges遍历。edges的0索引和1索引的元素分别添加到map中，节点名是key,出现的次数是value。接着就是遍历整个keyset()。寻找map.get(key)==n-1，倘若找到了，直接返回key。倘若没找到，返回-1

2.六级复习102，新增20个

3.早上把黑马的那本Vue.js看完了，对Vue2的工程化加深了印象，捡起了遗忘的许多知识，也补充了第一回学时遗漏的细节。有时间了得把Vue3从头看看，我记着几个月前仿佛还是看了Vue3的，但是也只是对ref和reactive这种印象比较深刻了，肯定有很多细节掌握的不清楚的。

4.下午和晚上在看黑马的SpringCloud书。不得不说，我先前对微服务的理解还是太浅薄了，对于SpringCloud中的若干组件并没有一个体系化的认知，对于其细分功能没有深层次的研究。所幸问题暴露的较早，还有时间挽回。

5.手搓了一个简单的微服务Demo，传了仓库，针对开发过程中的问题做了归纳。SpringCloud的版本和SpringBoot版本对不上的情况，大概率会导致服务启动失败。

6.重构了HNUSTBook这个仓库的文件结构，先前设计的还是太乱了。现在重新按照前端，后端，Web3，爬虫进行了归纳整理。知识体系还是要保持清晰明朗的状态，这样复习起来才有头绪。

7.今明两天的任务是继续归纳SpringCloud组件的原理细节，继续写文档和做lab。

# 10.20日报

1.力扣一道easy:给定图（节点数量,边的集合,src,dest）请求判断，是否有一条边正好连接src和dest。策略：1.针对每一个节点建立邻接表,将与其相邻的所有节点添加到表中,该表以List形式表达 2.定义visited数组,针对每一个节点,默认其没有被访问 3.深度优先搜索所有的邻接表,每一次新的DFS都令src为邻接表的下一个元素,递归出口为：src与dest重合。

2.六级新增20个，复习88个。

3.SpringCloud NetFlix相关组件的使用策略复习

4.分析了一个基于SpringCloud架构的博客项目代码，拆解了功能，总结了文档。

5.看了部分Dubbo知识,了解其大致生态位与基本用法

6.复习Nacos与Sentinel，复习了SpringCloudAlibaba与SpringCloudNetFlix的差异。

7.复习了SpringMVC的基本机制，学习了SpringMVC的部分源码

8.明日读计文柯的Spring技术内幕，把SpringMVC的源码阅读部分，第一遍先建立认知。再复习一下SpringIOC和SpringAOP看过的源码，这两个知识点时间线太久了，整个的继承与实现的复杂关系遗忘了很多，需要再巩固一下。

# 10.21日报

1.力扣一道Easy。给定有向图(节点个数n,有向边二维数组,从0走到n-1总共需要走的轮数k)，判断从0号节点能走到n-1号节点的走法种类。从图出发会想到DFS，从种类出发会想到用dp。本次用dp,考量状态转移方程：由于给定了二维数组的每一个一维数组，且一维数组的0索引元素是起点，1索引元素是终点。于是发现每一条有向边都代表了一次转移。我们需要初始化`dp[0][0]=1`原因很简单，走了0轮且还在0号元素,说明就是从0号走到0号，只有一种走法，于是组合数为1，后面所有的dp都是从这个1出发的`dp[i+1][dest]=dp[i+1][dest]+dp[i][src]`这里的第一个维度是当前已经走的轮数,第二个维度是当前走到的节点.我们将这个状态转移方程应用0~k的循环,最后return`dp[k][n-1]`可以得到结果

2.六级新增20个，复习88个。

3.SpringMVC的源码阅读，先前只是捋清楚了DispatcherServlet&&HandlerMapping&&HandlerExecutionChains&&Adapters的流程，没有下沉到web容器的IOC容器的启动上。今天补充了web.xml方式启动(Servlet2.0)与代码启动(Servlet3.0)两种初始化策略,补充了初始化流程，复习了Servlet,Filter,Listener。但是还是没有看懂SpringMVC的源码，这玩意的继承结构太复杂了。暂且先如此吧，姑且放置段时间，等代码量高些了再尝试读读。

4.NoSQL方面看了Neo4J的概念，以及Java客户端操作其工作的方式。这种图形数据库感觉很适合用来储存网站的用户好友关联，准备自己项目的时候可以考虑加进去。然后看了点MongoDB的主从集群，我发现和微服务沾点边的中间件都支持集群模式。Redis,MongoDB,ElasticSearch比比皆是。

5.很客观的说，今天没有很大的进步。只能说是在既有知识储备的基础上，对其原理性做了一点点深入，没有拓宽技术视野，而且整体学习的比较表面，这是读不懂Spring源码导致的。计网好久没看了，许多记忆模糊了很多，明日复习计网吧。

# 10.22日报

1.力扣一道easy：实现Fn=Fn-1+Fn-2+Fn-3。惯性思维是直接DP，今天学习记忆化数组，把每一次算出来的Fn存在数组里面，DFS先判断当前Fn是否已经存在数组里了，如果有就不重新计算，好处是减少冗余计算。另一个策略是滚动数组，三个变量分别代表Fn-1,Fn-2,Fn-3，不断更新这三个量，适合递推的场景

2.六级新增20个，复习80个

3.计网看了Http1.1的优化方向，大致三个：A.尽量不发http请求(尽量缓存) B.如果要发请求,则三个策略(1.合并多个请求到一条http中，2.延迟发包，3.减少重定向(重定向产生更多的http请求)) C.response做压缩,引出gzip无损压缩,以及有损压缩形成的webp.

4.Spring源码读了SpringCore的资源加载和元数据过滤的部分，首先是学习Resource,将url,file,classpath等指向的文件用resource统一起来。其次是ResourceLoader,将上述统一起来的Resource加载到Java程序中，再者是ResourcePatternResovler,考虑到ResourceLoader可能会遇到通配符,例如"http:\\example.com\java\**"这种**通配情况，所以得用这种模解析器。最后就是DocumentLoader，用来支持XML的文档加载。完成Bean定义的加载。之后就是元数据问题，这一块目前只看了MetadataReader，生成的MetaData是一个 "字段=布尔值"的结构，针对的是具体的JavaBean,涉及的内容包括是不是abstract的,是不是接口,是不是final,是不是static的，注意：上述描述的都是类的特性，如果要考虑方法，则只考虑被注解修饰的方式，只要是被注解修饰了的方法都会被添加到metadata中,其本质是ASM框架,性能比反射更高。联想到AOP的实现原理中，有CgLib和JDK动态代理，为什么CgLib性能好？就是因为其基于ASM直接操纵字节码，而JDK动态代理需要反射，需要通过虚拟机。接着就是看了AnnotationMetadata，它是基于有MetadataReader读出来，再对其中的“注解
“做特定的判断，比如是否是@Service这种特异性判断,以及去取注解的属性。

# 10.23日报

1.力扣一道中等：给定形如[1,5,2,3,4,5,100,200]数组,要求找出最长连续子序列.因子序列不考虑顺序,因此我先Arrays.sort.拿到[1,2,3,4,5,5,100,200].接着准备贪心,先用max表示全局最优,用tmp_max表示局部最优.依次遍历新数组,考虑三种情况：A.nums[i-1]和nums[i]相等,相等则continue B.nums[1]-nums[i-1]=1.这正是我想要的连续,所以tmp_max++,C.nums[i]-nums[i-1]>1.这就是典型的不连续,例如5和100的相邻关系.接着每次for.max = Math.max(max,tmp_max)贪心即可,最后return max.

2.六级新增20,复习122.

3.JUC复习：

A.Thread三种创建：继承thread,实现callable,实现runnable.

B.Thread有target,类型为runnable.构造传参实现runnable类是可行的,以及复习join,yield,start,run这些....

C.就绪和运行的state都是runnable.区别在于前者没有轮到时间片.

D.@FunctionInterface修饰的接口,只可以有一个抽象方法.

E.量太多无法一一列举,诸如此类吧.

4.Spring源码阅读：

4A. `TypeFilter` 类与先前学习的 `Resource` 类可以有机结合。假设我希望读取一个存在通配符的文件夹，识别其中是否有 `@Controller`、`@Service` 等 Spring 注解。可以创建 `PathMatchingResourcePatternResolver`，由于要判断注解，注解部分修饰的方法，因此需要获取元数据。首先构造 `metadataReaderFactory`，获取 `metadataReader`，读取到元数据，将文件夹的元数据全部读入 `Resource[]` 中。这时 `TypeFilter` 发力了，我们可以用 `TypeFilter annotationTypeFilter = new AnnotationTypeFilter(MyAnnotation.class)`，这样只会读取有 `@MyAnnotation` 修饰的类。例如 `@Service` 之类的，会因为没有被 `TypeFilter` 指定。这其实是在为 Spring 的 `@ComponentScan` 做铺垫，`@ComponentScan` 无非是将读取的类扩大到 `@Component`、`@Controller`、`@Service`、`@Repository` 等。如果没有实现读取类的功能，又如何将类加载到 IoC 容器中形成 Java Bean 呢？万丈高楼平地起。不过，`TypeFilter` 接口除了对注解进行特异化读取外，还可以使用 `AssignableTypeFilter` 考量指定类型的相关类，并支持 AspectJ 表达式和正则表达式。

4B. 我们实际上并不希望每一个 Bean 都被强制性加载，因此提供了 `Condition` 接口——Spring 中的 `@Conditional` 来决定该 Bean 是否被加载。现在考量其构造：我们可以使用类似 `TypeFilter` 的 `match` 方法，参数一：`ConditionContext`，参数二：`AnnotatedTypeMetadata`。我们发现，元数据确实很重要，出现频率很高！在 `match` 的具体实现中，我们可以使用 `getClass().getClassLoader().loadClass(className);` 进行判断。如果加载成功，则返回 `true`；若失败，则抛出一个错误。好了，现在我们可以结合 `Resource`、`Resolver` 和 `Reader` 来对类进行识别了。对于 `com.test.spring.ConditionDemo` 是可以识别的，但 `com.test.spring.ConditionDemo1` 就因为 `className` 不匹配而识别失效了。

4C.SpringValidator的逻辑：在 POJO 类中，我们常常使用 `@NotNull`、`@Size`、`@Min`、`@Max` 等注解来约束数据的有效性。然而，深入探究这种校验机制的实现，会发现它主要由两个部分构成：`supports` 和 `validate`。`supports` 方法用于判断给定的类是否可以被当前的 `Validator` 验证，而 `validate` 方法则包含具体的校验逻辑。需要注意的是，`Validator` 与 Spring 的关联并不非常紧密。实际上，`Validator` 在应用中最常见的使用场景是控制器层（Controller）中，用于处理和校验前端传入的数据。通过这种方式，开发者能够将复杂的校验逻辑封装起来，只需在 POJO 属性上添加简单的注解，便可以触发相关的校验逻辑。此外，Spring Validator 的实现也可以与其他校验框架（如 Hibernate Validator）结合使用，后者是 Spring 默认的实现。Spring 的 `@Valid` 注解常用于方法参数中，以触发验证过程，确保输入数据的有效性。

# 10.24日报

1.LC一道medium，给定形如[1,4,6,7,3,2,4,6,8],找出现次数大于Math.floor(nums.length/3)的数构成的List,策略是A.设计map,key存数,value存数出现的次数 2.设计list 3.遍历map.Entry,判断题设条件,符合要求则放入list 4.返回List.似乎还能用摩尔投票法做,但是下回再看这个方法吧,也算是没看题解就ac了,有题目太套路的嫌疑...

2.六级新增20,复习100

3.读SpringBoot in action.有一说一,翻译的真不错.SpringSecruity部分在微服务中感觉用处不大了,已经用sso单点登录替代了。但是配置类得实现SecruityConfigAdapter接口这个思路值得记忆,适配器模式在这的用处面试可以举例.

4.把SpringDataJPA,MybatisPlus又复习了一次,这两套ORM框架的注解差异还挺大,得注意分辨

5.看了点js,filter符合条件才能走,map相当于把每一个数组元素用函数体内函数映射构成新数组,reduce比较复杂,四个参量,累加器,当前元素,当前元素索引,初始数组.

6.数据库课设打算认真用学过的技术做,暂定前段Vue2+Element,后端上SpringCloud吧，今天先做一点点，把前端整体UI和表格整了，写了九个vue组件，真得累似了吧

7.看了点Converter.较为有用之处是：SpringMVC中，将http请求参数转化为控制器方法参数的DataBinding.SpringData中数据库实体和应用程序的Domain模型的转换。做Bean认证中验证逻辑前后的转换。写微服务时，假设不用Feign而用RestTemplate,序列化和反序列化http响应体的转换。以及整个Spring内部的ConversionService,实现普遍的类型转换。Converter比较特别的是，高度和泛型相关。

# 10.25日报

1.LC一道medium二分，题意是在一个单调递增数组中，找出最大的（索引和指向值相同）的索引。策略是判断nums[n-mid]>=mid则left=mid+1.否则right = mid-1.

2.六级新增20个，复习100个

3.读高并发书，复习了异步同步阻塞非阻塞等概念，重新梳理了进程和线程的结构，Java线程三种创建方式，要牢记FutureTask是集成了Runnable和Future.因此可以当target用.看了部分线程池原理，复习了线程状态，JNI，以及JVM线程的runnable等价于操作系统线程的就绪/运行(取决于是否拿到时间片).线程池submit接收可回调的runnable和不可回调的runnable，execute() 只能接受 Runnable。
可以将 Callable<T> 包装为 FutureTask，然后通过 execute() 提交。常用策略是：A.Callable<T> xxx B.FutureTask<T> futureTask = new FutureTask<>(task); C.futureTask等价Runnable做Target用

4.读Spring源码，就验证绑定和类型转换这一块而言。ConversionService里面包含三个主要组分：A.Parser:将字符串转为对象 B.Converter:对象之间的互相转化 C.Printer:将对象转化为字符串。上述三者要注意，Spring是支持Locale的，因此可以做国标转换，例如人民币换美元。接着看了点BeanDefinition,以及相关的Hold和Registry

5.看了点cpp的万能头和一些stl

6.明天重点还是读juc的书，我的理论基础太浅薄了。

# 10.26日报

1.LC一道medium二分，给定数组形如[3,2,1,4,5,2],要求找出其中重复元素，且数组中值属于[1,n-1],数组长度为n。策略：提取数据构建新的逻辑数组。l初始化为1,r初始化为n-1,不断计算mid.mid实质是一个（索引+1）.统计在[l,r]中小于等于mid的元素个数cnt.倘若cnt<=mid.则让l=mid+1,反之则r=mid-1

2.六级新增20，复习120

3.今天只看了JUC

A.线程调度/优先级/生命周期/Jstack

B.线程名/sleep/interupt和stop区别/join/yield/daemon

C.JUC线程池架构/Executors四种快捷池/submit与execute/线程池调度流程/ThreadFactory/任务阻塞队列/调度器钩子方法/线程池四种拒绝策略/线程池优雅关闭(先shutdown,再awaittermination,再shutdownnow,最后补一个awaitTermination)/Executors线程池潜在问题，分两组。A.FixThreadPool和newSingleExecutor共性于无界队列问题,cache和sceduled共性于线程数量无上限导致的OOM

D.线程池IO型2n线程,CPU密集n线程/混合型通常是微服务的http请求/cpu密集让核与线程对应是避免上下文切换导致的时间浪费

E.ThreadLocal的演进，KEY都是thread,但是value部分从value演变为了`<ThreadLocal,value>`好处在于同个线程可以有多个独享变量了/ThreadLocal两个主要场景:A.线程隔离,数据库的connection B.跨函数调用:传递请求中的UserID,Session,HttpRequest(微服务)

# 10.27日报

1.LC一道medium.给定二维数组,找出其中第k小的数字,先二维展开一维,然后排序,返回第k-1索引对应的数就行

2.六级：新增20，复习120

3.JUC：线程安全/自增运算的线程不安全分析/临界区资源和临界区代码段/synchronized的锁芯问题,class对象和实例对象的区别/静态同步方法(class对象为锁芯)/生产者-消费者模型

4.redis的cluster集群：分片slots/Moved和ASK(收缩)两种重定向/节点通过gossip算法逐渐实现数据一致性/HashSlots 16384/故障转移：从主观下线到客观下线

5.redis热点key:集群扩容/key分散到不同服务器/jvm二级缓存

6.TCP第三次握手没回复：没有收到ack,服务端会做超时重传,此过程容易发生SYN FLOOD攻击：,向服务器大量发起SYN报文。当服务器回复SYN+ACK报文后，不会收到ACK回应报文，导致服务器上**建立大量的半连接队列**,半连接队列满了，这就无法处理正常的TCP请求

7.Redis的RDB日志卡顿,RDB分三种：A.直接save(会卡) B.bgsave(fork一个子进程去save,不卡) C.savemn(m秒内n修改触发bgsave),RDB是直接存数据,AOF是追加已执行的命令,三种策略:A.Always(同步写回磁盘) B.everySec(先写到aof缓存,每秒同步磁盘一次) C.no(先写aof缓存,再操作系统决定合适写到磁盘),AOF重写：把AOF日志中的无效和重复命令去掉

8.@NotEmpty会接受空字符串,@NotBlank不接收空字符串,收前端的数据时记得用@NotBlank.当然,二者都不支持Null.

- @NotNull: 不能为null，但可以为空（如空字符串或空集合）
- @NotEmpty: 不能为null,可以为空的字符串，**但长度必须大于0**
- @NotBlank：不能为 null，不能为空字符串

# 10.28日报

1.LC一道medium。给定目标值target与一个数组。从数组中找出差值与target最小的三个数之和。朴素想法是三重for嵌套,但时间复杂度是O(N^3)，考虑先给数组排序,对有序数组进行二分,时间复杂度为 `O(N^2)+O(nLogN)`，实际为`O(N^2)`。具体策略是：遍历数组，确定基础点i,i+1为二分起点,数组最后一个元素为二分右端点。不断二分逼近。

2.六级新增20复习122

3.JUC

A.生产者-消费者模式-线程安全于不安全的实现模式

B.对象结构与内置锁：Java对象结构/Mark Word的结构信息/JOL查看对象布局/大小端：X86为小端，但HTTP等协议通讯用大端。小端是操作系统低地址存放字符串的高地址/无锁,偏向锁,轻量级锁,重量级锁

C.偏向锁的原理/膨胀/撤销

D.轻量级锁的原理(普通自旋锁/自适应自旋锁)/分类/膨胀

E.重量级锁/开销分析

F.线程通讯：管道/等待-通知/共享内存

G.低效率轮询与wait和notify(注意：Object对象和Class对象均可以使用上述二者)/WaitSet/EntrySet

4.装了RedisInsight,可以在windows操作虚拟机里的redis服务

5.跑前端项目,积累一些pnpm经验,后端环境配了一半,服务能启动了,但是启动停机,明天继续排查.

6.分析复杂项目的pojo层次，Entity->Enums->DTO->Vo,其中VO分req和resp,对于有分页需求的：ApiResult(返回给前端的resp)>PageBaseResp(分页的resp)>xxxxResp(具体业务的resp)对于无分页需求的：ApiResult>xxxxResp

7.练习mybatisplus,还是写的不熟练,练习时长有限,明日回寝室继续练

8.十一月要开始准备六级听力了,还尚未写过一套六级卷子,期中的Java和数据库也得开始准备了，起码书得开始看了。还得准备排球考试，真是沟槽的窒息感啊

# 10.29日报

1.Lc一道medium,给定数组与目标值与元素个数,要求返回一个List,包含目标值附近的k个数.策略是构建滑动窗口,根据给定距离算法,移动left和right直到窗口闭区间内元素刚好等于k,则返回这个集合.

2.六级新增20个，复习122个

3.调了一下午项目,没什么进展,促使我要切记把API接口文档写好后再建表,写Entity,写DTO,写VO(Request和Resposne).SpringBoot尽量用2.x版本,3.x和mybatisplus不兼容的情况太普遍了,试了三四个3.x的版本,报各式各样的错.下午时间空耗到debug上面去了,很悲哀的时间浪费.

4.把ruoyi代码生成器看了看，看来相当方便，先建表，然后代码生成器导入表，下载zip，zip中有三个文件，一个sql,这个sql是给ruoyi的菜单栏用的，一个vue的api+views，直接覆盖丢进去就行。最后一个java中分src和mapper,前者合并到ruoyi-system的src中，后者丢resource里面就行。还有点小bug，前端可以输出表格，分页数据也可以看到，但是elementUI不显示已添加的数据行，有空得再看看。ruoyi这玩意还是强，虽然僵化，但是干活速度还是快。暑假看了部分ruoyi的源码，感慨是封装的太完备了，一层套一层。

5.晚上时间把Atomic类看了看,大致分为AtomicInteger,AtomicLong,AtomicReference一类,AtomicIntegerArray,AtomicLongArray,AtomicReferenceArray一类,还有些分类忘记了。看了看基本的API。然后就是Atomic类的底层Unsafe类，相当于是用java做类似C的行为，管理内存。所以不安全。值得注意的是，unsafe的theUnsafe是static final的，因此不能new实例拿到。比较的策略是A.用反射getDeclaredField B.JvmUtil去取。再者就是Unsafe层面的CASAPI是得o,偏移量,预期值,新值。但是对unsafe做封装的atomic类就直接用预期值,新值做入参了。这点也反映出对地址偏移量的考虑是比较不安全的，得加一层封装。

6.把前天看的大端小端复习了点，然后synchronized的锁膨胀部分，从轻量级锁到重量级锁，也就是锁的mark word的线程Id和自旋信息被替换成了monitor,以后新的线程过来发现你这个重量级锁，就不会做CAS自旋了，直接走monitor的阻塞和等待队列，等着拿线程就行。

7.工程性质的探索本身就意味着对时间的浪费，我还是常常为这种没有稳定收益的时间亏损而内疚。等量时间用于学习原理性知识，回报率是较高的。不过工程经验的确只能靠不断debug获得，哎，很难权衡吧。

# 10.30日报

1.LC一道medium，给定一个形如[1,6,4,3,2,5,9]的数组,问调整几个元素后,数组将变升序。策略是使用单调栈，分别从左右开始迭代。在发现当前元素比栈顶元素要小（单调左栈）和当前元素比栈顶元素要大（单调右栈），更新左边界和右边界到当前值的索引。不断迭代，最后返回right-left+1即可。实际是用单调栈来找滑动窗口，最后返回窗口的大小

2.六级新增20单词,复习122单词

3.把JavaSe的IO流部分复习了，手写了若干demo，上一回接触这个知识点还是去年的十月份，一年过去忘了许多，考试前得记起来了。

4.上oj把22年的c期末考试机试做了，大概只有两三道A不出来。原因是string.h这个库的函数不太熟,strcat,strcpy,strcmp,strlen,strchr....写了若干demo作为复习参照。

5.JUC部分：

A.JUC的Atomic原子操作包

B.基础原子类AtomicInteger

C.数组原子类AtomicIntegerArray

D.AtomicInteger的安全性分析，实质是对CAS的封装+不断的自旋->引入降低自旋开支的LongAddr(空间换时间,类似哈希)

E.引用类型原子类->解决以前只能只有基本类型原子类的问题

F.属性更新原子类：譬如AtomicIntegerFieldUpdater,因为引用类型原子类例如<User>不能保证user类内部的属性操作是原子的，那就得用Updater来代劳

G.ABA问题-用一个栈的结构来阐明

H.使用AtomicStampedReference和AtomicMarkableRefernce来解决，前者用计数器，后者用布尔值标记

I.CAS弊端分析与LongAddr原理分析

J.CPU物理缓存结构/并发编程的原子性,可见性,有序性问题/总线锁与缓存锁/MSI协议/MESI协议与RFO请求

# 10.31日报

1.LC一道medium,给定形如[4,5,3,0,0,2,4]的数组,要求输出一个数组,新数组与输入数组长度相等,每一个位置对应,且对应位置的值是整个数组中第二次出现输入数组相应位置的值的索引与相应位置索引值的差。譬如上述数组的输出结果为[1,5,4,2,1,1,0].用2号索引举例,其值为3,遍历数组直到6号索引才出现了比3大的数,因此2号索引在新数组的值就是6-2=4.策略是维护一个单调栈，从左遍历到右，单调栈从栈底到栈顶索引依次升高。在遍历到每个节点用栈顶元素指向的值与当前值比较，如果当前值大，则记录当前索引与栈顶元素这个索引，做差+1，将其存入新数组的对应索引指向的值，最终返回新数组。

2.六级新增20，复习100

3.MybatisPlus的IServce<>的crud方法/Wrapper的妙用/分页..充分训练crud能力/看了部分Mapper层的方法..select/delete/insert/update.

4.配复杂项目的环境,总结经验：先改application.yml中的中间件ip/再看项目的schema.sql(有的开源项目只给建表语句不给建库语句,那你得去application的url里面找数据库的名字,自己建库,在库里面去跑schema.sql)/IDEA直接pull项目加载更快/要对涉及的每一个中间件的账号和密码仔细考虑,启动失败的原因常见于本地中间件的conf配置与application的属性不一致,其次就是关注Maven中依赖的版本是否与本地跑的中间件服务的版本兼容.

5.JUC部分：

A.编译器重排序/指令重排序/As-if-Serial规则/硬件层面的内存屏障

B.JMM/JMM与JVM区别/JMM八个操作/JMM解决有序性问题/Volatile语义中的内存屏障/这一块得在脑子里面把 主存-工作内存-高速缓存-线程 操作的图勾勒出来，本质就是：主存到工作内存read,工作内存到高速缓存load,高速缓存到线程就是use,这个过程也会assign赋值.接着从工作内存往主存是先store后write.read和load/store和write成对出现.后面谈的JMM层面的内存屏障也是基于store和Load来说的。

C.学习的脉络是从物理层到逻辑层。也就是先学习：内存->三级缓存->寄存器中的#lock汇编指令造成的缓存锁相较于总线锁的优越性.再考虑到希望跨平台兼容不同的机器码,设计出JMM来屏蔽底层.接着就JMM这个逻辑层设计出Read,Load,Store,Read与对应的逻辑内存架构...

# 11.1日报

1.LC一道单调栈的medium.给一个形如[6,5,4,0,9,12,34,74]的数组,要求找出一个元组（i,j）,要求j-i最大。要求是i<j&&nums[i]<=nums[j]。考量i是小元素且i指向的元素是小元素.那从较小的左侧开始找,对包含在单调递减区间的元素,都取其索引压入单调栈中.（stack.isEmpty()||nums[stack.peek()]>nums[i]）。接着从右往左利用stack找i,也就是固定点j,然后不断匹配合适的i.用贪心找j-i的最大值即可。

2.六级新增20，复习100左右

3.看数据库的考试课ppt,这一块主要是关系代数运算没有接触过,刷了一些往年真题,感觉这门课问题不大,每天看一点点吧。

4.JUC复习：

A.JMM/六个操作/内存屏障/volatile防止重排序+保持可见性/volatile不保护原子性的理由/Happens-before的六个规则
B.显示锁Lock/可重入锁ReentrantLock/显示锁模板/用Condition来在显示锁中复刻Object.wait()和notify()的等待-通知机制/LockSupport的pack与unpack与Object.wait()与Thread.Sleep()的差异比较/显示锁分类

5.今天总体来说比较放松，首先是睡到十点钟，背完单词刷完算法就一点了。看一些数据库考试内容，写一些题，玩会手机一下午就过去了。晚上就看了点JUC的新内容，复习了昨天知识，然后写了两篇博客。这种松弛的生活还是太容易腐蚀人的意志了，明天真得耐下性子继续看书+复现代码+总结博客吧。高并发这一块的细微知识点太丰富了,在延伸广度的同时,得逐层分析依赖关系,学习底层的设计原理。

# 11.2日报

1.LC一道单调栈的medium,判断数组中是否有满足132规则的子数组。策略是维护单调递减栈，从右到左处理。画图做起来比较快

2.英语新增20，复习106

3.看了几套往年的Java考试题

4.SpringCloud相关复习

A.${xxx}的配置需要在centos中export xxx=xxx

B.bootstrap比application早加载,一般存springcloud环境变量

C.Redis6379,Zookeepr2181,rabbitmq3306,eureka7777,configserver7788,zuul7799.nginx80

D.eureka分server,instance,client

E.脚手架模块设计->单个服务的设计->api/client/server

F.微服务治理三板斧->缓存/限流/降级

G.ConfigServer相关的本地配置/dev/xxxmaker-redis.config文件夹放在config-server的resource中

H.RESTful-表示层状态转换-一些基础API

I.把Client的Feign服务+熔断器类流程复习一遍，输出博客一篇

J,Ribbon的负载均衡算法->线性轮询/随机/响应时间权重/最少链接/重试策略

5.看了点JUC

A.把悲观锁乐观锁:syn轻量和基于aqs的ReentrantLock是乐观。syn重量锁悲观/可重入锁/公平与非公平:ReentrantLock默认非公平,传参true公平,tryLock非公平/syn不可中断,Lock可中断/ReentrantLock是独占锁,但是ReentrantReadWriteLock是共享锁

B.JUC看不太进去,明天再看吧。

6.头有点晕,不清楚为什么,感觉最近比较松弛来着.

# 11.3日报

1.LC一道单调栈,题意是找符合要求的最长子数组长度(涉及到数组元素大小)。策略是先用前缀和求出截止到i天工作饱和和工作不饱和的程度（>8小时为1,else为-1）。然后针对这个前缀和数组进行从左到右的单调递减栈的压入元素。然后从右到左用nums[i]与栈中元素做比较，从而找到合适的i和j,从而算出最长的子数组长度

2.单词新增20，复习86个

3.JUC部分：

A.悲观锁的问题/通过CAS实现乐观锁/不可重入的自旋锁/可重入的自旋锁(有state来计数了)/CAS可能导致的总线风暴（拓展SMP架构的多内核-1总线-1内存图）/CLH自旋锁（AQS的基础）

B.公平锁与非公平锁(ReentrantLock默认非公平,传参为true则公平),Semaphore传参true为公平

C.可中断锁与不可中断锁：指的是等待取锁的排队过程是否可以中断，syn不可以，lock中有一个lock.lockInterruptibly.

D.共享锁与独占锁：ReentrantReadWriteLock共享/ReentrantLock` 和 `synchronized独占。注意：读写锁可以取出读锁和写锁，写锁可以降级为读锁，但是读锁不可以升级为写锁，因为可能有死锁风险

E.AQS的状态标志位，队列节点类，JUC显示锁基于AQS,ReentrantLock和AQS的关系

F.模板模式-ReentrantLock的公平和非公平实质是依托于AQS的sync的两种模式...这一块没有看的很细，明天继续

4.SpringCloud部分

A.Hystrix的三种状态：**闭合状态（Closed）**：**打开状态（Open）**：**半开状态（Half-Open）**：以及相关的一些配置参数的深入学习

B.探究Feign的实现原理，基于RestfulAPI+SpringCloudweb依赖环境，就动态代理和静态代理的区别输出了一篇博客。

# 11.4日报

1.LC一道Medium,给定数组,输出新数组,新数组每一个元素都是旧数组该索引元素右侧的第一个比其大的数。且旧的数组是循环数组。策略是取模+遍历2*n

2.六级新增20,复习96

3.Feign原理部分:

A. XXXService->XXXServiceProxy->InvocationHandler->MethodHandler. 在Java代理模式中，`XXXService` 是定义业务方法的接口，`XXXServiceProxy` 是该接口的代理实现。`InvocationHandler` 是一个接口，用于处理代理实例上的方法调用，而 `MethodHandler` 是一个具体的概念，通常在 `InvocationHandler` 的实现中，它负责处理特定的方法调用。在 `InvocationHandler` 中，通常会维护一个 `dispatch` 映射（Map<Method, MethodHandler>），其中键是 `Method` 对象，值是 `MethodHandler` 对象。在 `InvocationHandler` 的 `invoke` 方法中，具体的实现是通过 `dispatch.get(method).invoke(args)` 来完成的，而不是 `dispatch.get(method).invoke()`。

B. Hystrix集成到Feign中，可以通过定义一个实现了Feign客户端接口的fallback类来实现。在 `InvocationHandler` 的 `invoke` 方法中，如果调用原始方法失败，Hystrix 会捕获异常并执行 fallback 方法。这通常通过在 `@FeignClient` 注解中指定 `fallback` 属性来实现，而不是直接在 `InvocationHandler` 的 `invoke` 方法中处理。

C. Ribbon的负载均衡集成到Feign中，Feign 客户端默认已经集成了 Ribbon。在发送 HTTP 请求之前，Feign 使用 Ribbon 来选择服务实例，并将请求发送到该实例。在 `MethodHandler` 中，Feign 会使用 Ribbon 的客户端配置来构建请求的完整 URL，而不是直接在 `MethodHandler` 中拼接 URL 和发送 HTTP 包。通过配置 Ribbon，可以自定义负载均衡规则，例如使用不同的 `IRule` 实现。

4.AQS原理分析

A. （AQS）概述（AQS）是Java并发包中的一个抽象类，它提供了一个基于FIFO队列的框架，用于实现阻塞锁和相关的同步器（如信号量、事件等）。AQS内部维护了一个状态变量（state）和一个FIFO等待队列，用于管理同步状态和线程的阻塞与唤醒。

B. AQS核心组件

1. 状态变量（state）：一个整数值，表示同步状态，用于表示锁的持有情况或信号量的数量。
2. 等待队列：一个FIFO的双向链表，节点类型为Node，用于存储等待获取同步状态的线程。
3. Node节点：包含线程引用、等待状态、前驱节点和后继节点等信息。

C. AQS核心方法

1. acquire(int arg)：独占方式获取同步状态，如果当前线程成功获取同步状态，则返回；否则，进入等待队列等待。
2. release(int arg)：独占方式释放同步状态，释放后，唤醒等待队列中的下一个节点。
3. acquireShared(int arg)：共享方式获取同步状态，如果当前线程成功获取同步状态，则返回；否则，进入等待队列等待。
4. releaseShared(int arg)：共享方式释放同步状态，释放后，唤醒等待队列中的下一个节点。

D. AQS实现原理

1. 独占锁获取与释放：
   - 获取锁：尝试修改状态变量，如果成功，则表示获取锁；否则，将当前线程封装为Node节点，加入等待队列，并通过自旋或阻塞的方式等待。
   - 释放锁：修改状态变量，唤醒等待队列中的头节点。
2. 共享锁获取与释放：
   - 获取锁：尝试修改状态变量，如果成功，则表示获取锁；否则，将当前线程封装为Node节点，加入等待队列，并通过自旋或阻塞的方式等待。
   - 释放锁：修改状态变量，唤醒等待队列中的所有符合条件的节点。
3. 等待队列的管理：
   - 当线程获取同步状态失败时，会被封装为Node节点，并加入到等待队列的尾部。
   - 当线程释放同步状态时，会唤醒等待队列中的头节点。
   - 等待队列中的节点会通过自旋或阻塞的方式等待，直到前驱节点为头节点，才有机会尝试获取同步状态。
4. 条件队列：
   - AQS还支持条件队列，用于实现类似Object的wait/notify机制。
   - 条件队列是一个单向链表，节点类型为ConditionNode，用于存储等待特定条件的线程。

# 11.5日报

1.力扣一道easy,给定苹果包数组,篮子数组.苹果包数组的每一个元素表示这个包中有多少个苹果,篮子数组每一个元素表示这个篮子可以放多少个苹果.现在希望把苹果从包中取出,全部放入篮子中,且使用最少的篮子完成此事。策略:A.取出所有苹果 B.对篮子按照容量升序排列 C.依次从最后的篮子开始放苹果,当当前苹果余额小于篮子容积时break.在return处 sum>0?ans+1:ans;即可

2.六级新增20个，复习96个

3.工程知识：

A.改用FinalShell连虚拟机,上传文件和下载文件比FXP方便许多,且更有linux的体验感了

B.复习了常用的linux命令

C.复习了微服务项目的docker-compose部署-先配env,再处理service.

D.复习了RabbitMQ的队列-绑定-交换器模型.复习了创建和管理上述三者的命令,以及相关业务场景-譬如微服务中的订单处理,客户之间的邮件发送:但凡是异步信息传递,我们优先考虑RabbitMQ

4.数据库课设:

A.针对SpringBoot版本和相关依赖进行了技术选型,通过互联网查询确保所有依赖版本都是稳定的.[耗时较久]

B.环境变量记录-记录工程细节

C.完成了五个实体表和一个关系表的设计

D.完成了2个模块,12个接口的设计

E.完成6个entity和1个result的设计

F.有时间再把MongoDB和Redis接入的相关功能和接口进行设计,后端文档处理完后再写代码,接着就是重新设计前端界面..还可以加入发布帖子和点赞/登录排行榜功能...充分利用中间件.可以考虑使用RabbitMQ完成站内信箱,使用ElasticSearch实现分词搜索-这些不急着做,先把主体功能写完.毕竟新功能又得设计新表了

# 11.6日报

1.力扣一道medium,给定多个背包及其当前装载情况和可放置的额外石头，计算能放满背包的最大数量。策略是拿背包数组和已放数组做减法,构造差数组。接着排序差数组,从零开始给差数组里面填充石.直到用完额外石头为止.最后遍历差数组,记录差为0的背包个数.该记录就是能放满背包的最大数量

2.六级新增20，复习100

3.JUC：

A.Condition单向队列和AQS双向队列转换：C多个,A一个：为Condition与ReentrantLock提供底层支持

B.安全同步容器类：Collection.synchronizedxxx(),vector,hashtable,stack：锁的粒度太粗：只适合低并发量的多线程,不适合高并发场景

C.高并发容器类：ConcurrentHashMap,CopyOnWriteArrayList与扩容机制：以迭代过程是否可以crud容器为例验证synchronized粒度过粗导致无法实现，转而使用高并发容器类

D.BlockingQueue:Array/LinkedList/Priority/Delay/Synchronous.其中Priority对消费者阻塞,Delay是通过计算时间差来允许消费者获取队列中元素

E.ConcurrentHashMap的JDK6和JDK8差异性/从Segment(继承R锁)走向CAS全体,syn局部/hashmap的扩容与树化,切记阈值64节点长度,8链表
F.饿汉模式/双重检查锁

4.工程：

A.完成Admin模块的六个接口的：Mapper/Service/ServiceImpl/Controller代码实现。

B.补充Entity对应的DTO,实现Entity->DTO一一对应,针对日期字段做了修饰处理

C.使用Eolinker测试了六个接口,可以正常工作->积累经验：写测试json一定要注意：从SQL表出发。只看Entity可能会忽略部分字段的特性，譬如enum...

D.完善项目文档

5.前端：

A.使用IDEA构建Vite项目get

B.了解了OpenTiny组件库,个人认为比elementUI更美观？也许这次项目可以试试这个UI库

# 11.7日报

1.力扣一道medium.买雪糕数组,策略是先给数组进行排序,再从便宜的买起,从而买到数量最大的雪糕

2.六级新增20，复习96个

3.工程经验复盘

A.Linux的权限并非是子目录继承父目录的..譬如chmod 777 /opt了,但是/opt/zookeeper这个目录是没有rwx权限的.你如果想在该目录上传内容,得单独chmod 777 /opt/zookeeper .当然,上述的操作得是sudo状态的

B.tar用来解压和压缩：-z是使用gzip算法工作。处理.tar.gz格式或者.tgz的压缩包;-x表示解压-extract ;-v表示verbose详细输出解压的内容;-f表示要解压的文件名,注意-f得最后用

C.`cp zoo-sample.cfg zoo.cfg`复制一份前者,新文件命名为后者.

D.vim相关操作：vim 文件名->打开;按i进入插入模式,按ESC退出模式;ESC后输出:wq保存退出vim

E.sh脚本通常通过./zkServer.sh start来启动.此处的./表示当前目录.start是一个参数,恰好脚本用这个参数来决定启动

4.ZooKeeper的应用场景/部署/结构

5.离散数学-复习

# 11.8日报

1.LC一道Easy,K次取反后最大化数组和.因为策略是优先对负数的元素翻转,因此我先将整个数组升序排列.从0索引开始遍历,当当前元素值为负数且k>0,则翻转该元素且令k自减。接着是处理边界情况,也就是k的数量比负数更多,也就是将所有的负数翻转后k还有冗余.这些冗余的k次数会导致我们的正数变成负数,如果希望损失最小,则得再将数组排序,让翻转发生在nums[0]这个最小元素上.当然：并非所有的k都会导致nums[0]翻转,倘若余下的k是偶数,无事发生,因为偶数次数会抵消.若是技术次,则让nums[0]为0,然后计算数组元素和返回即可

2.单词新增20,复习126个

3.阅读Netty书籍,明确了同步/异步/阻塞/非阻塞的关系,清晰化了用户态和内核态的区分.认知了read和write系统调用,捋清楚了传统的IO(同步阻塞),没有商业价值的同步非阻塞(不断新建线程轮询-过于消耗内存而不被使用),IO多路复用(Netty使用的模型-先建立select,接着使用一个线程通过select控制多个文件描述符(譬如socket))/以及异步非阻塞的关系.

4.回忆了9.27日记忆的Netty相关API操作,EventLoopGroup/Bootstrap/Handler相关概念/跟着教程手搓了CS架构的通讯代码.

5.高数刷题：看了部分函数和极限的内容

6.IO流和File的代码实现不太熟悉,太久没有写过相关的业务了。今日训练了：新建多级文件夹+文件+使用i/o的FileStream,接着用i/o的Reader/Writer进行编码,最后套入Buffer中去读/写.

7.最近的日子开始紧迫起来了,需要应付期中两门和期末若干门数理课+思政课的考试。压力还是比较大的。有时候看到朋友圈里同学们较为雀跃的日常，难免心生羡慕。但是就业环境太严峻了，假使现在沉溺于脱产而虚妄的惬意中，我应该会成为那80%失业率的贡献者之一吧。思绪这些没有太多价值，焦虑并不能解决实际的问题，还是得凝心聚力，继续沉淀。

# 11.9日报

1.Lc一道medium.给定形如[5,5,5,6]的数组与值k。我可以做k次删除元素的操作，希望k次之后能够让数组中留存的元素种类最少。观察到[元素]与[出现次数]的关系。建立hashmap。将[元素]作为key，将[出现次数]作为value。使用for循环填充map。因为看到"最少"的描述，因此考虑贪心，贪心得排序，map没法排序。我们取map.values()作为list进行排序即可。接着就是迭代list，只要k>0且k-tmp>0就继续迭代，同时list的size自减一次，毕竟你删去了一种元素。倘若k-tmp<0直接返回size即可。

2.六级新增20，复习96个

3.离散数学复习

4.NIO的Buffer/Selector/Channel相关的大量API复习，以及串通学习了几个代码demo

5.今日感悟：

A.今日刷了几份大厂面经，其中juc的出现频率很高，命中了我先前花费十天梳理的juc知识体系。但是我的深度仍然不够，面对面试题无法高效组织语言回答。因此juc书必须得反复看，反复给自己讲，这样才能做到逻辑清晰，脱口而出。

B.Netty的API和NIO的API需要深挖，这一块的模版代码往常是被我忽略的。但是考量到面试的挖掘细节程度，需要提高注意。

C.学历>实习>项目>技术。观察多篇面经，基本上来都是先问实习，问完实习再问项目，纯八股场景少。在设计话术时候需要通过项目和实习把面试官往我的八股舒适区引。这一块的项目还没开始准备，暂定一个Netty相关的轮子，一个Springcloud的应用吧。大一做的苍穹外卖应该进入历史的垃圾堆了。

D.今日很摆，哎。

# 11.10日报

1.力扣一道easy,给定数组，求和最大且元素最少的子序列，满足子序列和大于余下元素和。排序+贪心即可 

2.六级新增20，复习76 

3.看了sentinel和docker的原理

 4.把数据库课设第一个版本的接口写完了，测完了。今天工作量是6个接口。 

5.把前端的架子搭起来了，vue2+ts。有时间继续完善

# 11.11日报

1.力扣一道medium.今天是第300道题。给定整数数组arr,你可以删除若干个数，最后要恰好令余下的数之和>=原数组各元素之和的一半。由于涉及到元素和出现次数，下意识考虑map。key存数字的类,value存这个数出现了几次。然后对把values都取出来放到list里面,对list排序。降序排序完成后用两个变量分别记录已经删除的种类和已经删去的值。用已经删去的值做判断，最后返回一个种类数量即可

2.六级新增20，复习97个

3.数据库考试复习-看了一半的题库

4.写数据库课设：

A.后端加一个新接口：登录接口

B.前端从零开始写，把主页面+注册+登录的功能写完了

C.路由守卫这一块是第一回实操，出现许多问题，排查很久bug。我们做前端鉴权的思路可以是：登录成功后拿到json,存入localstorage.接着再往里面设置一个登录成功的key-value对。接着就是从json中取出代表该用户权限的字段(数据库里面设定好的-在注册时候确定的)-注意这一块是没有引号的。然后根据这个字段分类做router.push。

D.vue3的话，前端发送请求得在vite配置跨域。毕竟现在前后端分离，前端的端口和后端端口不一样。通过配置同源策略实现localhost:5173/Admin/getList转化为localhost:8080/Admin/getList的场景。

5.今天几乎没有摄入任何理论性的知识,大部分的时间都在刷题+写项目。这种节奏下，时间过得比较快。但是要警惕这种快节奏催生的浮躁心理。原理性的知识是必须得静下心来认真学习的。

# 11.12日报

1.力扣easy：打折购买糖果最小开销。题设给定：买三颗糖只用付两颗的钱。且免费的那颗糖只能比付费的两颗糖便宜。策略是先给数组排序，接着考虑i%3位置的糖。我们只让i%3位置的糖免费，也就意味着需要为i+1和i+2颗糖付费。这里的i+1和i+2我们通过if(i%3!=0)中的total+=nums[n-i]表达.之所以让i从左到右,但是算糖从右到左：是因为考虑到当总的糖<3时候,nums[0]的付费无法被计入.那只能用nums[n-i]表达了。

2.六级新增20，复习82

3.工程知识：

A.梳理了定时任务：从Timer到ScheduleExecutorService,再到DelayQueue.在单体项目中的定时任务,实现了从单线程到多线程的演变.内在机制也具备多样性,譬如Timer和ScheduleExecutorService依赖线程,而DelayQueue的底层是PriorityQueue和小根堆

B.动手实操了XXL-JOB：1.每一个boot项目都是一个执行器,boot中用@XxlJob修饰的方法是任务 2.执行周期在调度中心用corn配,在方法中写业务即可。3.给不同的用户分配执行不同"执行器"的权限。实现调度和执行的解耦。4.分组任务在注解中使用,group属性区分,在调度台分别执行。5.失败重试得在注解里配置failRetryCount=3..这样的次数。6.广播机制：就电商业务而言，平台需要发布全站促销活动，我们希望每个执行器都执行，则要在注解属性中配置executorRouteStrategy="broadcast" 7.分片机制：传入index和total，处理ABCD四个地区的发货业务。这里xxl-job干的活就是自动把任务分配给四个不同的执行器。具体实现：控制台给多个任务配一样的jobHandler.但是参数不一样。这部分如果改为多机的话,就是让多个服务使用相同的AppName.

C.美化了前端项目

4.知识获取：

复习了些se的方法，应付考试所需要的。

# 11.13日报

1.力扣easy：给定乱序数组,定义数对（i,j）。就2n个元素可以分为n个数对。现在要求从每个数对中取出的最小值相加结果最大。观察题设：发现"最"：考虑贪心。策略：先对数组升序排列。观察得到0 2 4 6等数对中位置较小的索引指向的值较小。步长为二遍历2n，对每一个nums[i]累加即得到结果

2.六级新增20个，复习92个

3.工程训练：

A.重构了课设项目的前端UI，从左右布局更新为了Top导航栏+内容栏平铺满，更符合我的审美习惯。

B.实现了类Apple的整体UI风格，新增Main页。

C.新增Admin功能页：实现五个功能模块的数据获取+前端请求发送。此过程中发现Delete接口和Update接口在后端设计时存在稳定，在前后端联调环节修复问题。5个功能模块正常工作

D.前端项目积累工程性排查知识x2：关于router-link和view的一一对应+vue-router的循环导向问题

4.Netty学习：

A.Netty逻辑架构应分为网络通讯，事件调度，服务编排三层。分别对应Sor非SBootStrap(BSP)+Channel/EventLoopGroup与EventLoop/ChannelPipeLine与ChannelHanlder

B.Netty本质是对NIO的封装,解决了TCP重连/TCP粘包拆包/手动编解码的痛点

C.ES,RocketMQ,Dubbo对Netty有比较深刻的使用

D.梳理了从while到ThreadPerConnection再到Reactor的模型演变路线,技术的演变是有逻辑可循的,是一个不断优化和解决问题的过程

E.单/多/主从多：本质是ELG的EL数量不同+Boss和Worker分工。针对主从：是主负责建立,而通道注册到从.Ps,

F.ChannelPipeline的双向链表结构，大概为[I1,O1,I2,O2,I3,O3],入站是(I1->I2->I3),出站是(O3->O2->O1)

G.一对一：Channel->EventLoop->ChannelPipeLine

5.总结：

白天学习效率太低了，看一会书就想玩手机，很悲哀。

晚上连着干了三小时项目，都忘记休息了。做工程时间纵然是飞逝，知识性储备收益却是细微的。

# 11.14日报

1.力扣一道medium：给定偶数元素数组，先尽可能让构成和较小的若干数对，然后从这些数对里面找最大的数对和。先排序再二分，这题太套路化了。

2.六级新增20，复习86个

3.工程：

A.看基于Netty的Http服务器实现GET响应源码。梳理Netty工作流程，了解自定义Handler的细节，输出复盘视频一个。

B.手写RPC看了三个topic的代码：

B.1：topic0:最朴素的思路是，使用传统的BIO，调用者凭借Socket+ObjectI/OputStream传递一个字段（譬如id）。接着提供者用这个id在自己的Service中取到返回值，将这个返回值通过Socket+ObjectI/OputStream传递给调用者。这样做问题挺大：首先是传递的字段数量只有一个，其次是成功和失败没有响应。

B.2：topic1针对前者设计了RPCRequest和RPCResponse两个pojo.传递方法名,类名,参数类型与参数表，以及返回结果+状态码。我们觉着将IO操作和业务放一起，耦合性太高了，因此将IO独立出来，作为IOClient,以后Client调用IOClient即可。后来发觉构造Request也有复杂的模版流程，这些代码和业务逻辑放一起也太耦合了，于是也独立出来，因为无论什么请求都要构造Request，我们采用JDK动态代理来处理该部分。OK了，现在我们在业务层调用Client,Client里面调用Proxy,Proxy中调用IOClient.我们的Code变得优雅起来了。

B.3：topic2：这回不去动调用者部分，目前已经足够优雅了。我们考量一下服务提供者。先前我们只考虑调用的方法来自一个Service，但是业务场景是复杂的，我们需要让更多的Service被支持。map是个不错的想法，我们在Spring源码中发觉了它的妙用。我可以使用反射来获取Service的名字作为key，用Service的Class对象作为value。又考量到先前while+新建线程的Server有点性能低下，将其迭代为线程池是更好的选择。但是线程池和Runnable逻辑放起来似乎还是太耦合了，那我们将Runnable分离为一个类，在线程池版的Server调用，业务逻辑更清晰了。

C.复习登录：cookies->session->jwt->oauth2/相关的web前端存储：localstorage,sessionstorage,cookies相关机制。

4.感觉明天得开始复习六级听力和刷题了。又是临近期中，真是屋漏偏逢连夜雨啊。

# 11.15日报



1.力扣一道medium：给定数组，每个元素都是一个人的重量。一艘船可以坐两个人，每艘船有限重。问最少多少艘船可以运走所有的人。思路是：不论如何，都可以运走重的人。能否运走轻的人就看轻重二人相加是否还在limit内。如果是的话轻的指针就右移动。每次重的人代表的右指针都会左移。因为他无论如何都能上船。

2.单词新增20，复习116

3.知识性学习：

A.复习了JavaSE的考试内容

B.学习了RPC框架的手搓流程，这一块还得继续深化

4.项目工作：

A.设计好了数据库课设的留言板。思路是：使用grid卡片布局生成若干个卡片。每个卡片呈现的内容需要实时向后端请求json。后端发过来的是一个json数组。其中有两个比较关键的参数，一个是contentPreview,一个是content.这个Preview是用来展示在卡片上的短文字。策略是在后端的Service层通过对content的截取来实现。目的是让界面看起来美观。而content是完整的preview.content只有当用户点击卡片才可以触发。这个类似于小红书网页版的卡片。不点开只能看一点内容，点开可以在不改变网页router的前提，弹出一个卡片，在卡片上做更多呈现。牛客的网页版也是这种思路，我觉得还是比较美观的

B.然后就是比较传统的后端设计，这个无非就是先写一个BlogRequest和BlogResponse.然后就是Entity,Service,ServiceImpl.Controller.写完这些就拿apifox测后端接口，后端测完了没问题就和前端项目联调。联调主要是看前端的axios是不是配置错了。今天因为新开了一个Discuss类，之前配置的跨域没有包含这个类目，导致debug许久没出来。以后要注意先配vite.config.js了。

C.这个数据库课设整体上感觉已经不错了，UI采取黑白灰+卡片浮动布局。我还是觉着鼠标滑到卡片上，卡片有滑动效果挺商务的。下周抽时间把user的前端界面画一下，基本就完工了。这一块工作量很小，因为业务逻辑和Admin端差不多。倒是今天做的留言板的业务逻辑比较复杂，需要花较多的时间。

# 11.16日报

1.力扣一道Medium：给定数组nums,要求对数组任意排序后存在最多的伟大数:伟大数定义：排序后数组perms perms[i]>nums[i] 且i在[0,nums.length-1]之间。由于没有指定要求按升序或者降序排列，排列顺序可以自定。我们先升序排列。接着用tmp遍历perms,设定变量i。一旦出现tmp>nums[i]。就让i++,最后返回i的值就是伟大数的最大值。思路是田忌赛马。拿最劣等的马做nums[i]。只有tmp>nums[i]时候才让i自增，意思是才让tmp和nums[i]绑定在一起，从而捆绑一个较大的元素，这样不断向右迭代，大的tmp全部被小的nums[i]消耗掉了。我们再将绑定的tmp和nums[i]放在对齐的位置即可（因为顺序是我们自定义的，我们想怎么排就怎么排）。

2.六级新增20，复习102

3.离散复习

4.JavaSE复习

5.C语言复习

6.复习Redis：看了String的数据结构，跟着敲了一遍SDS的C代码，学习了SDS相较于传统string.h的优越性：1.查长度因为有len所以O(1) 2.不用\0判定结束而是用len，从而让char[] buf可以存二进制值了（传统用\0判定会导致0x00的二进制数据存在buf中时因为0而异常），复习相关的strcpy,realloc,malloc,struct的相关定义,熟悉语法，输出博客一篇
7.六级做了一套听力，一套长篇阅读，错麻了，还是得练题，明天得开始训练了。
8.今天真的太摆了，完全没有什么产出，明天必须状态拉满，再这样下去要废了。

# 11.17日报

1.力扣：easy一道：给定两个数组，分别代表椅子和人的位置，要求移动最少得次数让人坐上椅子。策略是先对两个数组升序排列。接着从i=0开始计算移动距离，累计和即可。这是比较典型的两排序贪心。

2.六级新增20，复习103

3.前端：

A.box-sizing默认是content-box,该状态width=content,会产生子元素边框超过父元素情况。策略是修改为border-box，该情况下width=content+padding+border，让整个子元素都在父元素框内

B.导航栏常见的设计是 display:flex; 让其子元素默认横向排布, justify-content: space-between;均匀空白 切记justify控制主轴 align-content: center; align控制交叉轴。且默认flex-direction的主轴是水平轴

C.box-shadow的参量分别是x轴偏移,y轴偏移,模糊半径,阴影颜色。切记xy是斜向右下方正方向的。

D.transition用于控制元素变化的持续时间，实现一个"过程的呈现"。而具体的变化结果是伪类的css决定的。譬如.scale-box{ transition: transform 0.3 ease} .scale-box:hover{transform: scale(1.5)}前者在transition后指定transform,从而监听到其hover时候的transform变化，添加一个持续时间。ease是一个先加速后减速的时间函数，产生平滑效果。其他的时间函数例如liner,steps等也有不同功能。

E.transfrom常见有：translate(x,y)平移水平x,竖直y；rotate(angle)该元素旋转若干角度；scale(x,y)元素在x和y方向缩放倍率；skew(x,y)在x,y方向组件缩放倾斜角度；matrix(a,b,c,d,e,f)两两一组作为因子，分别是缩放，倾斜与平移。transform-orgin可以确定基准点。这里的倾斜是有点3d效果在里面的

F.align-items控制单行对齐，align-content控制多行对齐。

4.后端：

A.redis的string,list,set,sortedset,bitmap,hyperlog,geo,hash,streams,pubsub的电商业务场景下运用。输出博客一篇。

5.备考：

A.复习JavaSE的IO流和多线程

6.项目：

A.完成数据库课设，整理前后端文档，整理开发基本思路，输出视频一个。

# 11.18日报

1.LC一道medium，双序列匹配，双指针+排序+贪心解决。

2.六级新增20，复习103

3.复习Java考试

4.备考是最难受的，难以言喻的难受吧。

# 11.19日报

1.LC一道Medium：给定两个字符串，均为英文字母。要求实现方法：判定x串的每一个元素均可以在y串中找到一个字典序比其更大或者等于的字符。如果判定成功则return true。一个简单例子："abc"和"def"，这就是典型的true。但是"abc"和"aaa"就不行，因为第一个串的里的bc没在y串中找到比其字典序更大的元素。解决策略是：x字符-'a'做索引，然后value是该元素出现的次数。接着遍历y串元素，对于每一个y串元素，都从当前元素开始遍历，遍历完整个hash(也就是26个字符的数组)。一旦找到匹配的y串元素，则break，并且记录为flag为true。在外层判定flag，一旦发现有false情况，直接break。x串就没必要看后面的元素是否在y串能找到>=自己的字符情况了。

2.六级新增20，复习117。

3.前端学习：

A.filter是image的滤镜,常用的效果是filter: brightness(80%).让Banner的背景图暗淡,衬托文字。

B.flex-wrapper:wrap。让display:flex的子元素贯彻：如果一行排不下，就排成两行。但是如果是默认的nowrap,则会排成一行,且超出部分切割。wrap-reverse是换行，但是被挤出去的那一行放在原始行上方。

C.flex子元素的距离通过父元素的gap设置，譬如gap:20px。

D.页面背景遮罩设计：position:fixed;固定在页面，不随滚轮滑动。通过对top,left,right,bottom均设置为0：遮罩四个边缘固定在页面的四个角。background-color:rgba(0,0,0,0.5)：半透明的黑色背景是遮盖的精髓。最后是z-index:999，确保弹窗和遮盖在页面展示内容之上。剩下就是display:flex以及justify-c和align-c这种常规操作了。

E.弹出框体设计：border-radius: 10px;边界圆角设计;width:80%.宽度要小于界面元素,这样才能凸显出弹窗是漂浮在原始界面之上的。max-width为800px，防止大屏幕弹窗过大。overflow-y:auto。这里因为要展示从后端拉取的员工信息，员工信息可能有几百条，一个屏幕是展示不完的，避免溢出，让用户滚动滑轮从上到下逐条看。

F.表格设计：border-collapse：collapse：两个表格的重复边界只展示一条，单线边框更美观。

G.th和td用padding8px，让文本不至于紧贴边框，美观。

H.button要注意两点：a.border:none去除传统按钮的默认边框 b.cursor:pointer 鼠标悬停转为手符号,有交互感.

4.备考：
C语言看API,string和io的部分例子手敲若干遍加深印象。

5.项目：

把数据库课设的欢迎页前端UI美化了一下，先前的卡片太丑了，加点对比色和阴影。

6.Java终于考完了，明天开始看数据库了，还有6天时间。

# 11.20日报

1.LC一道medium，田忌赛马题，使用TreeMap的API完成。比如higherKey和firstKey

2.单词新增20个，复习112个

3.复习数据库

4.沟槽的换季感冒，效率非常低。

# 11.21日报

1.LC一道medium：给定二进制数组，问将其全部翻转为内容为1的最少次数。翻转通过异或实现，三个三个一组从左到右进行，在nums[i]==0时候进行三个连续翻转。这里的连续是因为要求尽可能使用最小的次数。记录次数返回即可

2.六级新增20，复习116

3.数据库复习刷题

4.Completable：

A.好处：相较于Future可以实现异步的编排,异常的捕获,简而言之就是可以链式调用和处理了,而且可以组合多个异步任务的结果

B.`CompletableFuture` 常用的 API 包括：`supplyAsync()` / `runAsync()`（异步执行任务），`thenApply()` / `thenAccept()` / `thenRun()`（任务完成后的回调），`exceptionally()` / `handle()`（异常处理），`allOf()` / `anyOf()`（组合多个任务），以及 `get()` / `join()`（获取结果）。

5.设计模式：

A.复习责任链模式：后端对用户登录的校验场景。责任链模式另外一个应用是Netty的pipeline中的Handler链条，整理逻辑图，输出博客与视频

B.复习策略模式和桥接模式：针对线上/线下的wx/zfb/银行卡场景做代码的解耦。编写逻辑是：先写策略接口，再写策略实现类。先写桥接处理器接口，再写桥接处理器实现类，桥接处理器需要注入策略接口，然后调用策略接口的内容。最后在OrderService中凭借Map完成前端请求和具体Processor的配对，然后根据前端请求参数完成具体支付平台的配对(switch判断wx等平台)最后将这个判断得到的策略实现类注入到桥接处理器中，调用桥接处理器，调用其中被注入的策略类，从而调用这个策略类中实现的具体逻辑。整理逻辑图，输出博客一篇，输出视频一个。

# 11.22日报

1.LC一道medium：给定二进制数组，需求是翻转为全1，问最少翻转次数。且翻转操作会导致nums[i]到nums[nums.length-1]的所有元素一起翻转一次。因此需要考虑：针对0元素，我希望翻转该元素后一共翻转了奇数次，针对1元素，希望翻转它后一共翻转了偶数次。因此在翻转前，我们需要判定num==(operation%2)。只有为真才进行翻转。

2.六级新增20，复习112

3.复习数据库，刷题

4.写设计模式的项目：设计模式相关：输出博客+视频+代码传Github仓库

A.观察者模式：基于到店业务的观察者监听店铺状态,多系统联动场景[商铺系统/用户系统/统计系统/订单系统->监听店铺状态,及时做出业务变更]：核心是Observer接口和Subject接口。将Observer的实现类注册到Subject。然后将Subject注册到某个业务Service中，让Controller调用该业务Service，从而让具体的Observer监听前端请求。

B.命令模式：整合Request和Response到一个Command中，通过实现Command接口，建立多个命令类。命令类整合到CommandService中调用。在Controller中通过不同的调用CommandService具体方法的顺序，次数与组合。即可模拟小红书的发布笔记/点赞/评论笔记功能。该设计模式使得功能之间的耦合程度更低，将来需要添加收藏/店铺等功能，只需要实现Command类，然后于execute()中完善业务逻辑，设计新的Request和Response，在CommandService中创建新的方法对应即可。

# 11.23日报

1.LC一道medium：合并后数组的最大元素。策略是从右往左合并。用sum表示右侧元素,nums[i]为左侧元素。如果sum>=nums[i]就合，否则就用nums[i]代替sum。

 2.六级新增20，复习150

 3.复习数据库考试

 4.设计模式：

 模板模式

 A.接收订单请求：无论是普通订单还是秒杀订单，首先都需要接收来自客户端的订单请求。 

B.验证用户信息：模板方法流程中的第一步是验证用户信息，这是所有订单处理的共同步骤。 

C.计算订单价格：接下来，根据订单信息计算订单的总价格，这也是所有订单处理流程中的通用步骤。 

D.创建订单：在系统中创建订单记录，这一步骤同样适用于所有类型的订单。

 E.特殊处理：这是模板方法中的一个抽象方法processorSpecificOrder，具体实现由子类提供。对于普通订单，可能涉及发货处理；对于秒杀订单，可能需要进行库存检查等特殊流程。 通过模板模式，这些通用步骤被封装在抽象父类OrderProcessor中，而特殊步骤则由具体的子类实现，从而避免了代码冗余，并提高了代码的可维护性和可扩展性。 

状态模式

 A.创建订单： 用户通过发送POST请求到orders/create来创建一个新订单。系统将订单状态初始化为"待支付"，并返回订单创建成功的信息。 

B.支付订单： 用户通过发送POST请求到orders/pay/{orderId}来支付一个订单。系统将订单状态从"待支付"转换为"已支付"，并返回订单支付成功的信息。

 C.发货订单： 在订单支付成功后，系统可以通过发送POST请求到orders/ship/{orderId}来处理发货操作。订单状态从"已支付"转换为"已发货"，并返回订单发货成功的信息。

 D.完成订单： 最后，系统通过发送POST请求到orders/complete/{orderId}来完成订单。订单状态变为"已完成"，并返回订单完成的信息。 

E.设计模式的作用： 整个业务流程采用了状态模式，它允许订单状态在保持对象结构不变的情况下，根据不同的状态改变行为。 状态模式通过将每个状态封装成独立的类，并在上下文（OrderContext）中管理这些状态，使得状态转换逻辑更加清晰和灵活。 当订单状态发生变化时，只需要改变上下文中的状态对象，而不需要修改上下文本身，从而实现了开闭原则，增强了代码的可维护性和可扩展性。

# 11.24日报

1.LC一道medium。喂饱仓鼠的排列：策略是先判定HHH中间，仅有H，HH头或者HH尾。然后给每一个仓鼠都放一个食物，接着判定H.H情况，一旦出现跳转两格并且在总食物数减1即可

2.六级新增20，复习122

3.复习数据库

4.设计模式-访问者模式，写代码，测试用例，输出博客，输出视频。

# 11.25日报

1.LC一道medium：贪心+交换次序

2.六级新增20，复习130

3.复习数据库

4.复习到有点身体不适了,很厌恶考试

# 11.26日报

1.LC一道easy：是统计将二进制数组m转化为k，且只能将1转化为0，需要转化多少位。策略是先将m和n做&，如果m&n==n，则用Java的Integer.bitCount()API统计m^k的个数，这个个数就是需要转化为1的0的个数了

2.单词新增20，复习130

3.知识储备新增：

A.设计模式的行为型暂时撒花了，常用的八个设计模式做好了代码实现+博客输出+视频输出

B.今天开始复习结构型设计模式，从享元模式开始，我比较喜欢从陌生处向熟稔处行进。

C.重构了设计模式测试项目的文件结构，在Controller Service Pojo的分包里面分别按行为，创建，结构建了新分包，这样不同隶属的设计模式代码的寻找速度更高。

4.写了3小节的离散数学作业，这一块12.29得提交，否则没有平时成绩，高数A上的笔记也要开始补了。

5.终于考完数据库了，无考一身轻。接下来就是每天渐进式的复习高数，离散，大物，概率轮，计组这几门课了。还得记着把马哲选择题和大题看看，思修得交实践报告，得去上课了。

6.近期的规划是把设计模式的基础打扎实，发觉放慢脚步的学习方式效果不错。我对设计模式的初次学习是通过阅读博客实现的，但是显然效果不佳，因为自己没有做相应的编码练习。这次通过编码+代码测试+文本输出+视频输出的策略，发觉掌握的愈发牢靠。很多的知识巩固是发生于输出时的，这是我应该谨记的。

# 11.27日报

1.LC一道medium：经典的环形数组问题。需要将一个长度为n的数组，展开下标到2n。通过下标取元素时用取模来规避数组越界问题。本题的创新点在于：需要辨别相邻两块瓷砖的颜色不同的连续k集合数量。这里我们需要在2n的(n到2n-1)区段才能进行集合数量的自增。原因是如果在0到n-1集合自增，会遗漏第n-1块瓷砖。因为这块瓷砖得跟第n块(真实数组中的第0块)进行比较。

2.六级新增20，复习102

3.复习概率论到古典概型

4.离散数学作业P1.4 P1.5 P2.1 P2.2

5.复习外观模式，输出博客，总结流程图，输出视频

6.看开源项目，学习新思路：密码加盐的具体处理流程

7.复习Mybatis-plus的CRUD操作，避免手生导致的肌肉性遗忘

8.听六级听力一套

9.发觉时间很不够用，需要推进的事情太多了，但是我的效率太低了。

# 11.28日报

1.LC一道medium:比较经典的记忆化+DFS板子题。题设是给定两个字符串，要求找到最长公共的子序列。注意两点：A.把字符串转化成字符数组考虑 B.子序列是有序的。DFS的两个参数分别为第一个字符数组的索引和第二个字符数组的索引，分别都从最后一个位置开始向左走。记忆化走过的路，如果走过该点直接返回。如果数组1的第i个元素和数组2的第j个元素值相等，说明是公共的，则i和j都往左边dfs+1。如若不然则贪心判别dfs(i-1,j)和dfs(i,j-1)。最后判定一下i和j小于0的边界case，遇到就return 0;

2.六级新增20，复习102

3.看概率论，推动一点点进度

4.离散写作业

5.设计模式：装饰器模式+价格在会员和优惠卷场景下的打折。这一块涉及到同一个接口的不同实现类注入，出现了循环依赖问题。折腾了一下午+晚上。最后的策略是用一个Config类注入@Bean，然后给DefaultPriceCalculator之外的装饰器都用@Lazy修饰，避免循环依赖问题。觉着装饰器模式的逻辑还是比较复杂的，卡了我许久。

6.看RocketMq的八股+实操经验

7.过一篇六级听力。

8.今天很累，由于代码Debug原因，几乎没有做休息。中午写算法卡了思绪许久，写完代码后考量到时间不够充裕，直接开始写设计模式的代码，但是又因为循环依赖问题的debug，空耗时间到三点四十，急冲冲的去一教上课。下课后想着bug还没解决，想先把问题揪出来，折腾到七点多终于解决，遂吃一份晚餐，接着把数据库的课设和实验的报告都整理+打印，随后整理和输出了装饰器模式的细节，整理思绪。余下时光慢慢看技术博客，做一些日积月累性质的学习。

# 11.29日报

1.LC一道medium：树形DP：打家劫舍的场景发生在二叉树上。我们考虑状态有两种：选择or不选择。倘若选择该节点，则状态转移方程为：rob=当前节点的值+该节点左子节点不选择的值+该节点右子节点不选择的值。倘若不选择该节点，则状态转移方程为：notRob=左子节点选or不选的最大值+右子节点选or不选的最大值。每层返回一个int[]{rob,notRob}即可。值得关注的是：我们考虑问题是从二叉树的最下层开始逐层往上考虑的，但是代码逻辑是从二叉树的最上层逐层往下执行的。

2.六级新增20，复习110

3.完成思政课程报告，注意十六周周一得交，两次缺到了，最后两节课必须得去了，不然真怕挂科了。

4.完成C语言实验报告，注意这个十八周之前得交

5.写了2个part的离散作业

6.看概率论，没太看进去，心神不宁

7.设计模式：适配器模式：业务需求：将前端发送的double数据修改为Integer类型。策略：抽象出适配器接口+实现适配器类。Service调适配器类,Controller调Service，输出博客，输出视频，项目归档Github仓库。

8.Docker拉取不到镜像的主要诱因有两个：其一是dns是8.8.8.8，这个需要在linux的resolv.conf文件中修改为114.114.114.114。其二是代理挂了，这个因素很波动，因为昨天配置在daemon.com里面的代理可能今天就不工作了，只能在需要的时候自己去临时搜今日可用的代理。另一个解决策略是在docker pull的时候直接注明来源的仓库，可以临时绕开镜像挂的问题：譬如用docker.unsee.tech/apache/rocketmq而非/apache/rocketmq

9.看Kafka，RabbitMq,RocketMq相关八股+业务场景，归纳异同，这一块内容较多。

10.六级听力一份。

11.感觉三十天备考六级+高数离散概率计组大物马哲思政的任务约莫有点重了。但是每日均一些时间备考吧，不然技术到位但拿不到学位证就幽默了。

# 11.30日报

1.LC一道easy。简单博弈论+分类讨论。给定一个数组，其中元素分两种，一种是个位数，另一种是十位数。有A和B两个玩家，二者只能选择取个位数或者十位数的累加和。且A是先手+永远选择对自己最优的策略。请问就输入的数组而言，A能否赢？既然A每次都选最优策略，也就是但凡个位数和十位数各自相加的和不等，天平朝一方倾斜，A都能拿到较大的结果，导致B只能选较小的结果，从而A赢。因此我们只需要考虑个位数和与十位数和相等情况return false即可，因为其他情况A都赢定了。这道题A下来蛮快的，约莫是两三分钟？

2.六级新增20，复习103.

3.离散数学作业part2

4.概率论：二维离散rv刷题

5.高数：极限的几种情况，洛必达，等价无穷小等一些刷题技巧。上次学这一块还是大一下开头准备转专业考试的时候。记忆的确遗忘许多了。

6.新增知识Kafka相关八股：架构/顺序写/磁盘结构/Partition负载均衡/Kafka劣势/and so on.

7.搭架子调依赖：把之前用于测试中间件的微服务架子的Eureka换成了Nacos,用GateWay替代了zuul。依赖这一块因为版本差异导致异常，花费了许久时间调试。

8.复习RabbitMQ知识：看文档，复习API。本地部署了一个RabbitMQ。遥想上回部署它还是九月份。

9.就电商下单业务场景写了MQ的Config+Client和Consumer的业务代码，太久时间不写业务代码手就会很生疏，这种纯唯手熟尔的活真得有事没事干一干，不然工程能力就慢慢退化了。

10.设计模式新增组合模式：场景是后端树化表达课程结构。实际上是在Pojo部分抽象出一个接口，然后提供两种实现类：普通节点和叶子结点。普通节点维护List,叶子节点不维护。然后于接口定义add,remove,getList,getName等方法。整合代码+输出博客+输出视频+push到github的repo。

11.六级一份听力。

12.明日主要任务是继续复习RabbitMQ,这一块的知识不够体系化,需要熟悉。

# 12.1日报

1.LC一道medium：给定一个数组，找到最小的，相同元素包裹的长度。策略是哈希+模拟。针对每一个元素，都将其添加到map中，然后如果之前已经存在map，则将二者索引作差，且保证左右闭区间，从而取到距离，接着对距离不断贪心即可。

2.六级新增20，复习103

3.设计模式：单例模式的七种情况，输出博客+视频+代码和笔记同步到repo

4.学习OAuth2.0：这个概念先前看过，但是没有做过实操。常见业务场景：前端登录需要向后端微服务的UAA鉴权服务器发起请求，账号密码正确则返回前端一个token。接着前端拿着这个token访问后端具体的资源服务器，资源服务器拿着前端传过来的token询问鉴权服务器这个token的真实性，如果为真则放行。上述描述的是同一平台场景。但是更多场景是：类似网易云音乐使用QQ登录：点击第三方登录拉取第三方的登录框，这里的请求是发给QQ的鉴权服务器，接着QQ鉴权服务器返回一个token给网易云客户端，然后网易云客户端拿着这个token去找QQ的资源服务器获取部分用户信息，资源服务器询问鉴权服务器这个token的真实性，如果为真，则开放部分数据给网易云音乐使用。

5.RabbitMQ继续深化：四种交换机，topic和direct都是针对routingkey而言的，fanout是给这个交换机绑定的队列都发。Headers比较特殊，需要维护一个hashmap，发送者和Config中的Queue的HashMap不匹配则无法推送消息到指定Queue(通过.whereAll().match())实现。其他部分的掌握较好，不需要额外记忆。

6.GateWay启动失败：多数场景是WebFlux与Spring-web存在冲突，在yml中配置web-application-type: reactive即可

7.Seata学习：分布式事务是我的薄弱点：这一块主要涉及到2PC,3PC.AX.AT,SAGA。2PC的问题：TC如果挂了，子服务上锁资源无限等待。3PC问题：网络分区导致数据不一致。AX问题：只适合单节点，不支持分布式，事务都是原子性的，不支持回滚，从而无法一致性。AT：补偿事务复杂性高，容易导致不一致，其次是只能提供最终一致性，无法提供强一致性。SAGA：一个较优方案，但是最终一致性和补偿事务复杂性问题依然存在。

# 12.2日报

1.LC一道medium：给定二维数组，返回一个最小数量，描述的是：让二维数组中的一维数组彼此之间不重叠。这道题很类似面试150题中的打气球问题。策略是：先就每一个一维数组的end索引做升序排列，先设定一个合规数量为ans,接着设定一个right，初始化为0号一维数组的右边界。接着迭代所有的一维数组，只要满足当前节点的左边界值大于right，则说明不存在重叠区间，则ans自增，最后n-ans即可。

2.六级新增20，复习103

3.离散数学复习

4.概率论刷题

5.技术学习：

A.Seata-TCC-银行转账场景

B.MySQL单库-读写分离集群-分库分表集群-分片算法

C.大厂为何做垂直分表？将高频字段和低频字段分离

D.缓存：客户端->应用层->服务层&一致性

E.大表涉及到分库分表为何禁用自增主键？A.范围法扩展困难 B.自增主键分布式环境的顺序问题

F.布隆过滤器在亿级电商场景的应用：A.预防缓存穿透 B.先预热 C.已经集成在Redisson中 D.通过对key进行hash+多个比特位从0到1的情况判定 E.判定不存在一定为真，判断存在有可能为假(哈希冲突导致)

G.京东开发为何禁用IP直连？个人项目和企业项目的区分点。A.简单策略用DNS B.企业方案是自建注册中心，譬如Nacos来统一管理多台Mysql服务器

H.CAP真实场景应用：CP：银行用的多，用户时延不重要，数据一致安全重要；AP：互联网用的多，数据不一致问题不大，但是用户响应必须得快；CA：小公司用的多，因为项目规模小，不存在分区容错的问题。

I.负载均衡器-四层LVS-七层Nginx-负载均衡策略：从轮询/权重/IP哈希/url哈希/公平模式

# 12.3日报

1.LC一道medium：旋转链表。启示：对于链表类题目，第一反应不应该是重新构造一个符合题设的链表，而是将当前链表改造为符合题设的链表，这样开销更小。对于本题A.不用重新构建链表，开销大。在原有链表上改变指向关系,除去三个特殊节点外，其他引用指向不变，开销小。 且若k%n为0，则相当于没转，直接返回原head即可。B.关注三个特殊节点 B.1.新链表的头节点,是原链表的n-k号元素 B.2新链表的尾节点,是原链表的n-k-1号元素,且注意尾节点next为null避免成环 B.3新链表位于原链表头节点之前的元素：一定是n-1号元素。 C.n为链表长度 k为旋转次数

2.六级新增20，复习112

3.复习离散/概率论/计组

4.总结jwt在分布式环境下的网关请求鉴权+子服务申请鉴权场景，输出视频。

5.项目学习：

A.阿里规范禁用外键约束的原因

B.Canal+MQ实现异构数据同步(数据先存MySQL，再同步到ES和MongoDB)

C.基于MHA实现的MySQL高可用

D.基于Sentinel的Redis高可用方案

E.阿里Seata场景(TC协调,TM全局事务,RM子服务内事务干活)

F.京东金融保障接口幂等性的策略(redis构建幂等表+RestFul请求触发AOP)

G.京东金融乐观锁解决并发数据冲突:版本号+Spring-Retry

H.阿里规范：超过三表不可join。需要join的字段数据类型必须一致！多表关联时，被关联的字段必须有索引

I.存储过程：银行使用，互联网不推荐。银行项目比较老，存储过程已经大量耦合在特定数据库中，迁移风险高。互联网强调解耦合，技术发展快，迁移成本低。

J.JWT-分布式场景

K.无状态的JWT令牌如何实现续签（不改token-存redis/改token-双token）

L.公共表如何在分布式架构下访问(下沉共性表,上浮业务表,多表服务注册中心连接,注解RPC实现CRUD)

M.凭借DNS轮询+VIP漂移-组合多Nginx+KeepAlived+多Tomcat确保架构高可用

6.明天继续备考期末+项目学习

# 12.4日报

1.LC一道Medium：双指针+原地处理。给定一个链表，要求组合成奇偶链表。策略是改变指向关系而不重新构建新的链表。这道题和链表排序比较相似，都是双指针+原地操作。

2.六级新增20，复习119。要考六级了，没怎么刷题，有点焦虑了。

3.复习离散数学/计组

4.项目经验：

A.Redis Cluster集群模式(集群和哨兵二选一：前者分散存，后者集中存。前者Raft，后者Gossip,切记最多1024x16个槽)

B.MVCC解决幻读——(Innodb之所以能在RR阶段解决幻读,是因为针对快照读提供了MVCC,对于当前读提供了行锁+间隙锁.当然,MVCC比后者弱,不能完全解决幻读)

C.宜信的架构演化过程（单体->垂直拆分(每个垂直模块重复了基础服务如MySQL)->无注册中心的IP耦合RPC通讯->昂贵的ESB总线(总线压力大，要抗通讯和注册)服务阶段->微服务架构）

D.Docker构建+部署Nginx+3个App+MySQL应用集群

E.蓝绿/红黑/灰度发布(蓝绿和红黑都是全量,灰度是增量/蓝绿和红黑都是有两套业务环境,灰度是一套业务环境/红黑和灰度都是逐步切换,蓝绿是直接切换)

F.MySQL索引的选择性陷阱（少用尾匹配和低选择性字段+使用Canal+MQ把异构数据导出到ES等搜索引擎进行处理）

G.替代JDK序列化的方案：SpringBoot内置的Jackson/Dubbo内置的Hessian/Google的Protocal Buffers

H.MQ中间件通用消息投递过程/丢消息情况分析：这一块的逻辑是：强调ACK作用+Broker先存盘+通讯没ACK就重发+注意Consumer幂等性。

I.京东的CI/DI：持续发布集成的策略(代码push到GitLab,带上maven和dockerfile/配置管理员通过Jenkins使用mvn package打包并根据dockerfile构建镜像,镜像push到Harbor（类似Dockerhub但是公司内网中的）接着Jenkins定期发布到K8s中,k8s逐步通过：测试区,UAT区,生产区。测试区开放网关给测试，UAT区开放给产品经理,生产区进行灰度发布)

5.期末复习九门，还有三十天，压力有点爆炸了。

# 12.5日报

1.Lc一道medium：以x为分界点进行重排序链表。策略是设定两个dummy节点，然后双指针即可。

2.六级新增20，复习109

3.刷离散真题

4.项目学习：

A.为何表的主键用代理主键(自增编号)而非业务主键(..身份证)

B.生产环境的JVM与垃圾回收的配置策略(JDK1.8优先用G1-G1不用设置新生代大小/-x基本参数/-XX涉及到GC选型和stw时间和日志打印等高级参数,是虚拟机特异性的)

C.线上排查OOM问题思路(1.先jps看线程总数 2.jstat -gcutil 17038 1000 10 一秒一次输出十次查17038线程 3.重点看老年代相关的OC和OU以及FC全GC情况 4.jstack 17038可以用,但是输出太不友好,我们换Arthas 5.curl下载Arthas 并且jar -jar运行 6.选择线程,可以在命令行看到界面查询了 7.heapdump /tmp/dump-1.hprof 7.下载到本地,load到VisualVM 8.发现类只有1w个,但是实例有100w个,且大部分是HashMap,右键进去,查看Gc-Roots,看和哪些线程相关 9.发现都和阿里云的Oss相关,在代码中排查相关部分 10.发现原来是连接Oss的Client连接一直没有删除,导致时间堆积的对象过多,调整代码,排查完毕)

D.消息积压与死信队列：每日十点前台获取单据的速度远超信审系统处理单据的速度->引发消息积压->避免消息丢失->引入死信解决

E.主键不可用UUID：UUID随机性->新增row随机插入->B+树无法预测位置,需要重新IO,IO开销大->B+树比较UUID大小,寻找合适插入位置->UUID若在中间某个叶子节点插入,会导致页分裂问题,并且需要树的重新平衡->造成慢SQL和IO开销

F.KafKa为何快？（1.磁盘顺序读写 2.页缓存 3.零拷贝 4.批量处理）

G.Cassandra列式数据库的高性能原理(用磁盘的图来记->行式数据库是按数据行连续排列,而列存储是按相同的列连续排列/读快更新难/版本号机制更新+逻辑删除+空闲删除)

H.微博大V更新的消息推送：Push和Pull区别（微信用Push所以限定用户好友数量）

I.动静分离：京东抗单页10wQPS的方案（把静态资源丢cdn+将部分动态数据存Redis）

J.医渡云：医疗大数据场景为何拥抱MongoDB而非MySQL？（1.医生处方无结构 2.新增字段不需要重启服务 3.支持自动分片）

K.MySQL模糊查询：Ngram替换Like（5.7.6提供ngram简单场景）/(create fulltext index以及select * from xxx match(content) against('测试'))使用match against匹配；

M.复习JDBC流程，写crud保持手感

N.复习Mybatis和SpringBoot的工作流程

# 12.6日报

1.LC一道medium：给定两个用链表表示每位值的数，求和。策略是构建新链表。首先建立dummy节点以及当前位置的累加和t。接着赋值dummy的next给一个tmp节点,这里考虑t的值大于等于10，因此填入新节点的数值是t%10。考虑到进位：因此t在每轮遍历后都得t/=10。最后return dummy.next即可.

2.六级新增20，复习109

3.离散数学刷题+看书补齐知识

4.Web2知识：

A.Raft选举算法(Nacos和RedisSentinel的原理)

B.基于ZooKeeper的临时顺序节点实现分布式锁(粗糙版本,直接用ZooKeeper对象链接)

C.Curator集成ZooKeeper解决超卖问题：使用Apache提供的客户端，更优雅。

D.BASE最终一致性：BA实质是牺牲纯粹的C,促生基本可用的A（用户以为执行完毕了，但是后台会慢慢干活）也就是所谓基本可用。S是软状态：三个服务的订单状态在短暂时间内可以不同，E是最终一致性，这是S的结果。

E.更新缓存不可用update！而是先写库再删缓存(线程顺序不严格,可能线程1覆盖线程2的修改,导致数据库和redis的数据不一致)，因此得用旁路缓存，先写库，再删缓存。有请求来了再从库同步数据到缓存

F.既然有了Redis和ZooKeeper这种分布式锁，为何还需要Seata等分布式事务？前者只关注互斥，后者满足ACID和回滚补偿修复

G.秒杀业务：Nginx+Lua+预先准备好的Redis秒杀表+RocketMQ

H.Sentinel：替代Hystrix的分布式限流工具（Sentinel实现：拦截器+责任链模式slot+客户端只接入pom依赖/在UI面板配置具体方法的qps或者线程数约束）

I.SPI-服务提供者接口：为Sentinel新增Slot-通过接口来扩展已有框架的功能,在框架中定义自己的组件,不需要修改框架核心源码

J.复习SpringBoot的相关注解，以及启动原理

K.复习Mybatis的CRUD，很生疏了。

5.Web3：

A.不用Remix了,在本地搭好了Solidity环境和hardhat测试环境

B.学习了简单的合约写法,实现了一个简单投票合约

C.对区块链的认知新增：hash相关的算法被大量使用+非对称加密，这一块和token很像。

# 12.7日报

1.LC一道medium：找出循环链表的环首节点。考虑到每一个ListNode有val和next两个属性，是独一无二的。可以用Set存储已经走过的元素。第一次Set.add添加失败的节点就是环的首节点。

2.单词新增20，复习103

3.复习大物,高数

4.工程学习：

A.Sentinel：如何熔断保护避免分布式雪崩(慢调用/异常比例/半开状态-限流-降级-熔断)

B.Reids热门商品访问倾斜的解决：商品被分片到多个Redis，单台redis只能抗5w,现在有10w qps,则新建Redis集群,全量存储热点数据,走负载均衡访问,这样10w qps稀释到每一个商品是10w/n 只要有三台机器就能抗住/另外可以在前端的localstorage临时存储或者在前端应用进程内缓存)

C.从ELK到EFK-KEFK-日志更新沿革:

- 多服务内嵌LogStash+ElasticSearch+Kibana `内嵌LogStashCPU开销高`

- 多服务代码耦合LogStashTCPSocketAppender依赖+统一发给LogStash+ElasticSearch+Kibana`LogStashTCPSocketAppender代码耦合大`
- 多服务Beats监听+统一发给LogStash+ElasticSearch+Kibana`Beats监听的导出数据单一化,不可导出到多个中间件,于是先导到Kafka,大家从Kafka读数据`
- 多服务Beats监听+统一发给Kafka+分发给LogStash（以及Redis等其他数据中心）+Kibana`部署成本昂贵`

D.预处理零宽空格问题：切记rollbackFor="Exception.class",否则回滚的条件过窄。@Transaction的回滚默认是基于RuntimeException的,倘若是ParseException则无法回滚

E.Paxos算法：两阶段/提议者-接受者-学习者/Raft是Paxos的简化版本

F.索引覆盖：海量数据大页码MySQL查询优化思路

G.预防XSS注入：SpringBoot策略：HtmlUtils和CSP规范响应头

H.RocketMQ：角色/生产消费流程/Broker主挂了怎么办?/Broker主从都挂了怎么办/NameServer挂了怎么办/同步复制与异步复制之间的区别和应用场景

I.MySQL执行计划分析(索引结构/回表/关键看Extra的MRR和index-condition,type一定一定要避免All的全表扫描)

J.驱动表-多表关联执行计划-MYSQL-小表驱动大表-只有在外键建索引有用-在查询字段建索引不会走查询优化器-大厂禁用三表联查也是避免表链接太多查询寻优化器工作失败导致慢SQL

# 12.8日报

1.LC一道medium：求集合的幂集。策略是双重遍历，充分利用已有的幂集合。

2.六级新增20，复习119

3.复习大物

4.项目学习:

A.本地消息表+定时任务+MQ：保证分布式最终一致性

B.基于Prometheus+grafana+TSBD的指标监控架构

C.日志分析：从springboot内置到logback/slf4j是通用接口

D.CDN+ELK+多ECS+Redis解决高并发贺卡应用流量冲击

E.事务消息：RocketMQ优化先发消息再写库导致的(库回滚+消息影响下游)问题：发送半消息/根据事务执行情况将半消息转化为最终消息or丢弃半消息

F.APM(应用性能管理)链路追踪-分布式系统调用链跟踪(A.Sleuth+ZipKin基于日志 B.SkyWalking基于Agent代理)(单次调用同TraceID,不同SpanID)

# 12.9日报

1.LC一道easy：按数组的每个元素的二进制含1个数排序。策略是先bitCount而后*100000+nums[i]之后排序然后取模10

2.六级新增20,复习117

3.复习离散，大学物理

4.工程

A.并非用了索引查询就快：扫描行数+回表次数决定速度。聚簇索引树与非聚簇索引树/覆盖索引/回表/索引下推

B.快手红包雨：场景题回答思路

C.银行ETL架构(提取,转化,加载)：核心业务域通过sqlLoader导出csv,隔日传输FTP,定时任务XXL-JOB提取FTP中的csv,通过ETL传输到ODS（数据集市）三方业务通过数据集市获取银行上一日的交易信息，进行储存分析

D.微信手机端(Token登录,封装设备信息)PC端扫码阶段(封装PC端设备信息请求二维码+手机扫码根据手机的token+二维码id=临时token+手机确认登录,临时token转化为正式token+token转发给PC端->PC端带着正式Token请求API)

E.阿里为何禁用Java内置线程池（Fixed和Single的阻塞队列无界限，多请求OOM/Cached和Scheduled无队列直接创建线程，线程太多也OOM）

F.RocketMQ保证消息有序消费的策略：(序号取模分队列/消费者每次只能与一个队列建立一个链接)

G.MySQL主从复制：BinLog和RelayLog都是存具体操作。异步复制：从不一定能收到BinLog/半同步复制(至少一个从和主全量同步)/全同步复制(基于MGR)

# 12.10日报

1.LC一道easy：给定两个十进制数，要求判定两个数的汉明距离。汉明距离是指两个二进制数，位一一对应，其中位不同的个数。基于概念：我们利用Integer.bitCount()，对x^y这个整体进行bitCount()。这里借助异或的定义：不同则1。也就是说：统计两个数的二进制位异或后值为1的数量就是汉明距离。

2.六级单词新增20，复习402

3.备考学习：

3.1静电场：

A.在复习静电场之前，需要明确三角函数的罕见部分。我们已知正弦，余弦，正切。实际上构成五边形，存在正割(sec)，余割(csc)，余切(cot)。对于切而言：仅有一组对应，且tanx=1/cotx。对于割和弦，我们采取交错的策略。有sinx=1/cscx，有cosx=1/secx。

B.其次要复习微分的定义：当存在函数y，实际上存在dy/dx=y`。这一点在大物下被广泛运用。

C.在接触正式知识前，还需要明确lamba(线密度),sigma(面密度),rho(体密度)。以静电场为例：λ=q/长度；σ=q/面积 ρ=q/体积。这三个符号不提前了解，直接进入静电场学习，会一头雾水。这是我的亲身体会。

D.静电场的学习逻辑是：场强->(积分)->电势。在该过程中涉及到高斯定理。最后需要提及一个环流定理：在保守场中E*dL=0

E.静电场需要掌握线段模型/平板模型/球壳模型/球体模型

F.明确高斯定理：∮S E⋅dA=Q/ϵ。这一块的Q花样很多，实际题目中要和上述提到的三种密度做多样的变化。E往往是被表达的量。A是与密度对应的单位。如果此时考虑的是圆柱壳面(半径为r)的壳外一点R。那A应该是2πrl，而σ=q/πR^2。我们做线和体的变化也是这个逻辑。

G.需要记忆：高中的k在大学被展开为了1/4πϵ。

H.实际上：电势的表达中，分母的r比电场强度少一个次方。感性认识是：U=E*d

I.在涉及到球壳类型的电势题目时，需要先算出内，中，外的E，接着根据不同的距离做积分，将积分结果累加。

J.双层嵌套的壳模型，里层壳的高斯面围绕的部分没有电荷，因为电荷在壳上，因此里层壳的E应该为0。而外层壳的高斯面要小于外层壳，因此外层壳的E实际是里层壳提供的。

K.静电平衡：内无电场强度，电场强度垂直于表面。U=C。

3.2磁场：

A.电场有电通量，磁场有磁通量，自然磁场存在高斯定理。但是正如静电场的安培环路定理结果为0，磁场的高斯定理值为0。这是由于磁场是无源场（有旋场），而电场是保守场(无旋场)。

B.高斯定理可以粗糙的理解成：某种通量*通量覆盖的微元面积=内部的总元素/元素常数。比方说电场的高斯定理，这个元素就是电荷量，因此常数是相对介电常数。

C.恒稳磁场的安培环路定理有效，B*dl的线积分=μ点乘（I的累加）

D.毕萨定律：dB=μIdlsin角度/4πr^2。这一块会涉及到几何关系的三角运算。我们的目的是将dl转化为d角度。我们会用到上述提到的比较罕见的三角函数来完成这个过程。这里一般涉及到两个角度。我们会令一个为0，一个为无限大。从而得到一个2倍效果。

E.磁场力的计算：f=ILB。Pm=ISn

4.项目学习：

A.选型虚拟机：为何优先用G1?（A.传统SerialOld在32G内存下性能堪忧,会产生几个小时的STW,一旦老年代GC了应用基本就挂了,而G1不做物理分代,物理上分区逻辑上分代,并发回收速度快/B.PS+SerialOld是全量回收,不回收完毕就一直STW,G1不一样,G1是小步快跑,先把小的回收了，大的暂时不动。STW最多不能超过200ms）

B.Redis大key导致的线上事故：单个key>512k。(set存储读过某本书的全部用户id,这一块如果有100个用户读,那没有问题,但是如果有100w个用户读过这本书,那么redis就会出现集群的倾斜)解决思路：首先这个set集合要缩减,我们只规定查看前100名读过该书的用户,也就是set容积缩减到100。其次是redis6.0的本地缓存（高频且不易变动的数据）,在服务里面缓存一部分数据,不必每次请求都访问redis。最后是用go写的rdb_BigKey去定期扫描大key，定期定理。

# 12.11日报

1.LC一道Easy：给定两个数，要求出二者不同位的数量。使用Integer.bitCount()+异或即可。

2.六级单词新增20，复习500

3.记忆计组的零散化知识，看王道书，刷题。

4.计组大题知识：

1.IEEE转化：切记二进制部分包含：S+E+M。对于三十二位情况，S一位，E是8位，M是23位。S直接用数字的符号判别，0正1负号。中间的E满足E=e+127。其中的e是2的阶，E是移的码。比方说给定二进制数： 1.10101x2^3。这里因为是正数，所以s=0，因为有1.M存在，尾数取10101。因为2的阶码是3，因此满足E=3+127。将E转化为二进制的130，然后用0和1组成SEM即可。注意M需要在末尾补0到第23个位置。这一块一般给的都是十进制或者十六进制，十六进制就直接划四个位置，十进制你得用8421码进行一个转化。

2.双符号位判溢出：两个符号位就0和1有四种可能性，两个符号位做异或运算。异或为1的情况就是溢出。比如00,01,10,11。这里的01和10就是溢出的。这里01就是正溢出，10是负溢出。你可以理解成：首位是1就是负，首位是0就是正。这里我们经常遇到[x-y]补码。也就是[x]补+[-y]补。比如x=0.11 y=-0.111.我们先转化为双符号位的原码：x=00.110 y=11.111。然后求x的补码：因为是正数，不必取反，直接+1有x=0.111。然后处理y补：11.000->11.001。**接着要注意：-y补并不是对y做除去符号位的取反，而是要全部取反后+1**，也就是00.110+1=00.111。最后[x]+[-y]有01.101。这就是典型的正溢出。

3.存储容量扩展：位扩展类似串联，字扩展类似并联，字位扩展就是又串联又并联。

- 原有1k x 4的存储器 （k一般是横向扩展, 后面的4是纵向扩展 从图片来看的话）
  - 位扩展后为： 1k x 16
  - 字扩展后为： 4k  x 4
  - 字位扩展后为： 2k x 8
- 容量为1K x 8位的存储器，地址线和数据线有多少根
  - 因为是8位，所以数据线有8根
  - 1K，K是2^10，所以有10根地址线。
- 用32K x 8位 的芯片组成一个 128K x 16位的只读存储器
- 数据寄存器和地址寄存器都看组合后的结果，但是需要多少个芯片就得做除法
  - 数据寄存器：16。因为组合后的地址是读16位
  - 地址寄存器：128K,首先128是2^7  k是2^10 那总共地址寄存器就是17位
  - 需要芯片的数量：128k x 16 / 32k x8 =8
  - 画出存储器的框图：
    - 因为是将32k组合成了128k，所以横向是4个
    - 因为是将8位组合为了16位：所以纵向是2个。
    - 然后左侧接CPU的上数据，下地址，因为32K实际上只需要15x2根。你这多的1x2根做片选信号，用来确定何时选择某个纵列

# 12.12日报

1.LC一道easy：判定某个数组中是否出现过至少一次的相邻元素或运算的尾随零。策略是快慢指针遍历相邻元素，然后对二者进行或运算。判定或运算结果是否为偶数，如果为偶数则直接返回true。最后在外层循环return false即可。最近备考压力太大，LC刷easy保持手感即可。

2.六级新增20，复习1000个

3.项目学习：

A.OAuth2.0的理解：OAuth2.0是一个标准，这个标准规范的对象是AccessToken的请求与响应体部分。OAuth2.0是工作在三方应用调用主服务资源的场景下的。最简单的例子就是用QQ登录网易云音乐。

B.ElasticSearch聚合分析-Bucket与Metric：通过Restful请求让ElasticSearch处理数据。坏处：由于是Json嵌套格式的请求，阅读和维护非常不易。

C.不用MQ如何保障消息传递的可靠性？（核心思路：重试+补偿）-(Spring-retry做重试/本地消息表/确保接收的幂等性)

D..一致性哈希解决MySQL的扩容问题(A.传统Hash在扩容后,需要对多个数据库中的数据重新Hash,重新分配,更改的数据量太大了！B.引入一致性哈希->构建一个长度为2^32 -1的环,将服务器分布在环上,按照数据的hash将数据放到环上,并且按照顺时针顺序将数据归属到若干服务器 C.但是现实是:服务器分布并不均匀,可能有某台服务器需要接收90%的数据,但是余下两台只用抗10%,这是真实节点的弊端 D.引入虚拟节点，虚拟节点一定是均匀的,同时建立虚拟节点和真实节点的映射表 E.业界目前没有成熟的开源方案,通常Hash算法需要自己写,或者用Google的CityHash/没有现成的迁移组件,也得自己写/集群的节点不固定,需要添加前置代理，这里可以用京东的ShardingProxy)

4.计组学习：

A.存储器相关：划分/指标/主存分类/刷新方式/主存容量三种扩容技术/并行技术/并行启动的两种方案/模块化存储器的两种方案/外部存储器：SSD和机械的差异分析/Cache结构/三种映射关系/替换算法/一致性分析

B.指令系统相关：指令格式与分类/指令寻址/程序机器代码表示/X86汇编助记符/Mips相关汇编助记符/CISC/RISC

5.明日计划：

A.上午清完日常+把计组进度推到CPU功能

B.下午和晚上冲刺六级单词+翻译+听力

# 12.13日报

1.LC一道medium：给定a,b,c。要求：对于每一位：a|b==c。如果存在不满足条件情况，要对对应的位进行翻转，统计总共需要翻转的位数量。策略是循环i,每次右移i位，同时右移结果和1做&运算。记录单词的bitA和bitB = 0。接着分三种条件判定：若bitC为0，则sum=1时候需要翻转一位，sum=2时需要翻转两位。bitC若为1且sum不为1时必翻转一位。统计最终的翻转位数返回即可。

2.六级单词新增20，复习300

3.复习模版+翻译+听力。

4.计组复习到CPU结构

# 12.14日报

1.LC一道medium，给定十进制数组，譬如[3,2,3,3,3,3]。找出连续相与为1的最大子数组长度。由于是子数组，因此必须连续。又由于要求最大子数组，因此这个子数组中的成员应该为整个数组中值最高的元素。我们先迭代找出最大的元素。接着重新迭代数组，在出现三方变量与最大元素相同时让size自增，其他时候size置为0。每次for时候都计算maxSize。最后return maxSize即可。

2.单次复习100，新增20

3.看计组，刷题。知识面已经覆盖了考试要求了，但是细节掌握的很差，需要不断细化。

4.明日开概率论+计组刷题。

5.感觉六级要寄了，已老实，这就是平时不练听力+只背单词的后果。

# 12.15日报

1.LC一道Medium，给定一维数组，求其中所有元素彼此的汉明距离总和。我们分两步做，首先用双层for嵌套，第一层为i起点，第二层j=i+1起点，两层的终点都是末尾索引。然后我们用Integer.bitCount来解决汉明距离的问题。最后将汉明距离做累计即可。我们回顾一下汉明距离：给定两个十进制数，将其转化为二进制数，从右往左依次比较bit，如果相同则不记录，如果不同则记录。也就是求两个数异或的结果。

2.单词复习100，新增20。

3.计组：刷题，看ppt，继续复盘。计组的知识体量过于庞大了，还有半个月的时间查漏补缺，这一块还是以今年的题库为主，以刷题驱动知识点复习。

4.概率论：二维离散/随机变量数字特征/中心极限定理与三种分布(卡方/t/f)/参数估计/假设校验。今天把概率论的知识点和例题都速通完了，明天开始刷往年真题，感觉概率论的学习痛感还是不大，可以说是对高数下知识+高中概率统计部分的延伸，比较难受的一点的是需要记忆一些枢轴量+各种概率的相关E,D。

5.项目开发：

A.单体->服务化架构(SOA)->微服务：ESB粗粒度划分服务，且总线昂贵负载高。微服务细粒度划分服务，不依赖总线成本低。

B.ProxySQL实现MySQL读写分离-基于MGR集群(基于集群+monitor用户+节点视图表查询节点状态+避免在应用中硬编码主单节点,导致主节点挂掉后无法访问集群.)

C.高可用架构的脑裂问题，Redis的解决策略（主服务器挂了,应用还在朝着主服务器写入数据,Sentinel选举了新主,此时旧主变从,得接受新主的RDB和后续的AOF,同时清空旧数据，导致主挂后的持续写入数据丢失。）(解决策略：设置min-slaves-to-write为1,以及lag为3s)（前者参数的意思是：只有1一个从库才能写入,当主挂掉后,其无从库,无法写入,避免了数据丢失,后者参数的意思是：从节点延迟时间超过3s后主节点将失去从,失去从的旧主节点无法被写入数据。）

# 12.16日报

1.LC一道medium：双指针法-找出数组的乱序部分，返回一个区间索引集合。思路是左指针找出第一个出现单调递减的索引，右指针从末端找到第一个单调递增的索引。然后如果left>right说明数组有序，直接返回有序的{-1,-1}。接着就[left,right]遍历每一个元素，如果该元素比left左侧的元素小,那左边界就要左扩张。如果该元素比right右侧元素大,则右边界还得扩张。

2.单词新增20，复习119

3.备考复习：

A.刷计组选择题，把题库选择题刷完了。

B.高数把七种极限归纳总结了，特别记忆抬高法和万能公式这两个，用的太少容易记混。看了点数列放缩。

C.复习C语言：这一块归纳平时写码忽略的知识。比方说转义字符（\a是发出铃声,\b是退格符,\f是换页符,\'和\
"都是符号.\t是水平制表,\v是垂直制表 \r是回车符）以及字符串相关函数(strcmp,strcpy,strchr,...)，这一块的共性是，如果涉及到dest和src，那dest一定是第一个参数，src是第二个参数。以及用字符串给char数组赋值的情况只能出现在声明时，你是不能对一个已经存在的字符数组用字符串赋值的。还有就是切记把字符串放到字符数组里面，不要忘记字符串的\0，比方说"abcd"要用5个位置存。文件操作的：fopen,fclose,fseek,ftell(a是append后面继续增加,w是写,r是读,这一块和Linux管理文件的权限逻辑一样的)。还有就是scanf的返回值是成功读取的元素个数，而printf的返回值是写出的字符数量，切记是字符数量。scanf放在if语句里面通常用EOF判断是否结束...C语言这块真的很零碎啊，对于这种不常用的语言，很多语言特性记忆起来很不自然。

4.项目经验：

A.MySQL主从复制：数据一致性的延时问题。MySQL读写分离后，先写后查如何保障一致性?（问题：数据写到主库后,马上需要读数据,此时默认从从库读,但是主库写入的数据尚未同步到从库,导致查询的数据不包含刚刚插入到主库的数据,出现了数据不一致）(A.延迟查询,等待主从同步后再查,缺点是高并发性能差 B.利用ShardingJDBC的特性,要求插入后的下一条select必须走主库,绕开了同步问题实现了一致性查询,C.MGR全同步复制,强一致性数据同步,在没有完成主从同步之前,jdbc.insert方法无法得到结果,从而阻塞后续的查询操作.从而查询操作有一定查询的是已同步的数据.缺点是MGR架构需要在服务设计之初考虑,成熟项目迁移成本大。)

B.Redis6客户端缓存-解决多级缓存一致性问题（A.多级缓存指的是实例内存有缓存,也用Redis做缓存 B.Redis集群通过Sentinel解决了缓存一致性,但是多个业务实例中的本地缓存是不同步的 C.传统做法是用RocketMQ接收某个业务实例发起的变更，将其推送到其他业务实例和Redis集群中,这样做增加了系统的复杂性 D.Redis6提供了客户端缓存,让Redis替换RocketMQ的缓存同步功能,Redis监听实例的缓存,在Redis发生更改时通知实例某条缓存已经失效 E.我们通过lettuce来接入Redis）

C.如何优雅保存MySQL数据变更历史(网dai的订单明细表)(A.首先否决订单双写,太重量级 B.既然存MySQL不行,那就存到异构数据库中去.通过把MySQL的binlog传给Canal,Canal通过订单id%n将Json写入MongoDB分片(这一块需要Canal拿到订单id去MySQL回查全量数据并储存) C.反范式设计灵活,与业务不相关,只需要在主键和时间戳上建立索引,天然支持Json,分布式NoSQL支持百亿数据)

D.瞬时流量洪峰保障系统不被击垮：Alibaba-Sentinel：系统预热流控(某接口的平时单机阈值QPS处于低水位，默认为1000/3(冷加载因子)=333，如果此时有瞬时大流量，Sentinel会通过预热逐渐将QPS阈值拉到1000，这样就为系统留出了缓存时间)

E.几万行Excel批量导入库如何优化：(A.数据表给身份证号加索引,选择性强 B.多线程分段写入Excel,将数据按身份证号分段,每一段数据用一个线程去写excel C.采取SXSSF这种流式API逐行写入,避免Jvm的OOM问题 D.JDBC批量操作)

F.Redisson的分布式锁实现-分布式锁不止setnx：(Reddison的底层是封装了加锁的lua脚本用于生成key/Redisson采用hash类型,客户端持有的形如lck_acc_000是一串哈希码,value是持有次数,且这个key-value有生命周期30s/持有锁的实例会启用watchDog,每隔10s给锁续期,避免锁内代码还在跑，但是锁没了导致并发冲突/由于value是一个数,所以Redisson是可重入的,在这一块和juc的reentrantLock设计很近似/我们以value的计数器实现可重入锁)

G.使用MySQLBench中集成的Visual Explain做SQL性能分析

# 12.17日报

1.LC一道medium：状态机DP-买卖股票的最大利润+手续费

2.单词新增20，复习120

3.复习计组

4.看了点python的库,写了两个简单的脚本,把小时候爱玩的游戏的活动速通了。大致思路是用pyautogui来模拟鼠标操作，类似脚本精灵，实现拖拽/自动挖矿。游戏的话，边界case比较繁多，本身写脚本是出于学习的目的，没有做太多的优化，仅仅实现了基本功能，代码传github上。

5.效率实在有点低下了今日。还是太放纵了。明日必须进入战斗状态。

6.基因法和倒排索引优化MySQL分库分表(A.基因法就是哈希法,将每个元素的最后若干二进制位选取为基因,比如有4个库,就选择最后两位为基因,00就进第一个库,01就进第二个库,10进第三个库,11进第四个库.缺点是数据库一旦扩容,迁移成本非常之高.好处是：可以快速确定非uid字段到底在哪个库中 B.倒排索引法就是引入Redis,比方说就邮箱而言,把邮箱做Key,将邮箱所处的db服务器名和邮箱的ui做value,这样也能确定某个字段所在哪一台服务器上.坏处是对Redis内存需求量高，如果SSD性能好也可以用innodb替代,其次就是注意数据库和缓存的一致性,要当心软状态时的数据不一致。)

# 12.18日报

1.LC一道medium：逆波兰表达式，使用Stack，遇见数字就入栈，遇见运算符号就从栈中取出两个元素，先取出的是num2,后取出的是num1,执行num1 OP num2，将运算结果压入栈中。循环往复，最终从栈中取出最后一个元素即可。

2.单词新增20,复习119

3.复习高数上：隐函数求导/参数方程求导/对数求导/微分/连续性/第一类间断点：可去和跳跃，第二类间断点：震荡和无穷/可导和连续关系->刷题

4.计组：二轮复习数制部分+指令编码部分/刷题库

5.工程学习：

A.Redis在项目中的设计规范（A.业务名：实体名:id/B.key不要超过39个字节/C.很好记忆：把回收策略分为两个段：前者是要回收的范围，后者是回收的规则/回收的范围有volatile:即将过期的 allkeys:所有数据/回收规则是：random/ttl/lru））

B.MySQL文库表优化-利用Json特性(A.对于在一个表中存储非标准内容：譬如书籍有出版社,作者/期刊有刊号,发行日期,采用宽表是不可取的,因此我们使用Json来存储这些非标准信息/B.在MySQL5.7以后新版本使用SQL语句查询Json中字段默认性能开销很高/C.我们采取虚拟列来一次性提取Json字段中的某一项,避免每次查询重复解析Json字段)。

# 12.19日报

1.LC一道medium：先求GCD，再用GCD求LCM。接着双层for循环，第一层for表示要跳过的元素，从-1开始，意思是存在一个元素都不跳过的情况。第二层是遍历整个数组，两个数彼此求GCD和LCM。并且做乘积。通过贪心选出乘积最大的存储在成员变量中，最后返回最大值

2.单词新增20，复习97

3.高数上：不定积分：第一类和第二类换元法/设t法|渐近线：水平渐近线/铅锤渐近线/求y=ax+b形的渐近线。

术内幕》：梳理ORM框架发展历程，复习Mybatis原理，新增对XML的多种读取策略以及多种ORM框架的异同

5.计组：指令流水线的：加速比/吞吐率/流水线效率计算|CPI相关计算,这一块切记程序执行时间是n个时钟周期|波特率计算和比特率计算/虚存相关

6.马哲刷题1.5个单元

7.总体效率不高，娱乐时间较长，明日还是强化计组+高数上+大物+马哲。明日计组最后一节课务必要去。

# 12.20日报

1.LC一道medium，埃氏筛求范围内素数个数。策略是先设置一个长度为n数组，然后默认数组所有元素都是素数。接着i*i<n遍历做为桩子的数，每一个桩子内，用桩子的平方为起点，用桩子的大小为步长，所有包含在步长内的数字都设置为非素数。最后遍历整个数组，在元素为素数的值遇见时做count自增即可。这里要切记：最小的素数是2，因此你得先判断n<2情况，一旦n<2就得直接return 0。

2.复习计组：波特率原理/复习数值计算/总线相关机制计算

3.复习离散：把一二三单元的书和例题看了一遍，增加对数学归纳法和2^n是n长度集合的子集数的认识(用01特征集合)

4.大物：干涉相关/光程计算/相位差计算/干涉图样判别(偶数亮,奇数暗)/算平均光强,式子就是两个光强按照矢量方式叠加(这里相位就决定了是`(A1-A2)^2还是(A1+A2)^2`，也就是所谓的亮和暗)/可见度计算(V=Imax-Imin/IImax+Imin)所以Imin为0效果最好，Imin=Imax基本就不可见了。I的计算也是矢量加法

5.马哲刷题

6.C语言细节知识点：A.switch可以用int long char 以及enum/B.switch中case1：case2:printf("");情况下，出现case1情况因为没有遇见;所以会执行case2的命令/C.C语言中char是一个字节，务必不要和Java弄混/D.p[-1]是绝对的未定义行为，哪怕p是&arr[5]。也不能用负数的形式去查询前一个元素/D.C语言文件有：二进制和文本类型

7.今天娱乐时间过长了，这是堕落的开端。要复习的课程太多了，工程知识完全被落下了，只能寒假拼命冲进度了。务必清醒，保持战斗状态。

# 12.21日报

1.LC一道easy。判断一个数是否是2的幂次方。两个策略：A.比较对n和n-1进行与运算，如果结果为0则是2的幂次方，这是通过观察规律得出的。第二个策略是遍历n的所有因数，判断除了1之外的因数是否都是偶数，如果有一个数不是，则判定n不是2的幂次方。

2.单词新增20，复习119

3.复习大物 ，刷了四节习题：二次复习静电场相关

4.复习离散：刷了两章习题：齐次递推公式/入度出度图求法/布尔矩阵求法/关系相关

5.复习高数：把不定积分的第二类换元法看了看，复习了六个三角函数的转换关系。关于第二类换元法涉及到的代换：很好记忆，a^2开头的就是弦和切，遇到-，-小就匹配x=asint 遇到+，就匹配大的x=atant。剩下一个x^a开头情况，弦和切都有了，还有个割，因为之前都是正，所以现在用余割 x = asect。然后还有正切方+1=余割方 余弦方+1=正割方。以及相关的一些做题技巧。

6.复习概率论：这一块就是纯把之前过了一遍的概念和例题又过了一部分，没什么好陈述的。

7.工程训练：RocketMQ消息存储与检索原理(A.RocketMQ使用CommitLog存消息,一个log文件存1GB,顺序写盘。B.检索消息有两种策略，一种是用发送者决定的MessageId查,这种是精准索引查询,另一种是用消息的Tag查,这种是分类筛选过滤 C.了解RocketMQ文件结构,以及消息通讯时,相关中间文件的转化)

# 12.22日报

1.LC一道easy：给定一个数组，将数组中所有的0都移动到数组的尾部。通过这题复习了冒泡排序。这段时间太忙了，只能写easy保持下手感了。

2.单词新增20，复习97

3.复习大物：写了两节习题，一节静电场介质，一节恒稳磁场。

4.复习高数：继续不定积分，刷题。

5.复习离散数学：二轮复习了函数的定义部分，以及相关的矩阵运算，以及对称/传递/闭包/自反等概念。

6.复习计组：二轮复习存储器部分的记忆性知识，复盘了几种经典计算题。

7.复习概率论：二轮复习到一维连续部分，主要是看题+刷题。保持记忆活性，避免学过的知识在考试前遗忘。

8.工程训练：

A.细化了RocketMQ关于消息有序消费与死信队列的实现策略。（技术内幕这本书的质量真的很低，如若不是没有更好的原理书，我是绝对不会去读一本错字颇多的教程的。）

B.顺序消费相关：1.全局顺序[同一个消息队列中的消息按发送顺序消费,无论来自哪个生产者]2.局部顺序[基于类似消息键值的条件,相同的消息被发送在同一个队列,确保在该队列内的顺序消费]。具体来看，RocketMQ是确保相同消息键的消息路由到同一个消息队列。底层是根据Key来计算哈希值。齐次需要考量消费者，顺序消费情况下，一个队列只能有一个消费者处理。避免多消费者同时读取并消费该队列信息。最后就是消息的Offset，消费者得定期给Broker回报当前消费偏移量，确保每个消费者消息偏移量是顺序的，避免错过消息和重复消费

C.死信队列相关：首先明确消息是存在CommitLog和ConsumeQueue中的。其次消费者对死信队列的消费有两种策略，前者是Pull，后者是Push。终点都是消费者，区别在于这个消息是主动拉取的，还是被动推送的。这一块和RabbitMQ差异性不是很高。

# 12.23日报

1.LC一道midium，三色排序问题。用快排做。A.先写swap函数 B.快排的精髓就在于写partition函数实现基于某个pivot的分区，比pivot小的元素放左边，比pivot大的元素放右边，这一块用双指针写，最后返回pivot的索引。C.写一个递归，从0到nums.length-1进行类似前序遍历的操作，先用partition求出pivot节点，接着对pivot左右两侧的段继续切割细分。

2.单词新增20，复习97

3.离散复习：格与图相关，WireShall算法，刷题

4.计组：看存储器相关记忆性知识点，手搓连线ROM字位同时扩展框图。A.用8位片合成16位，在数据线部分，得分高位片和低位片，连线这一块得注意。B.在地址线部分，每一个小片都得连单片的所有地址线 C.合成新片的多余地址线是接译码器的，比如2-4,3-8。然后这个用来做片选信号，每一个片选信号对应一个合成芯片。D.不要忘记每一合成芯片都要链接上A/W线。

5.大物：恒稳磁场部分，刷了两章节题。主要是温习安培环路定理和求磁力矩。

6.概率论：二轮复习一维和二维的连续和离散。

7.工程：RedLock相较于setnx的优越性(A.setnx设置的锁在主从复制时无法被查询,此时有额外请求设置新锁,会导致锁被重复获取,产生并发冲突 B.RedLock本质是让多个Redis实例同步保存一把锁,每一次的查询和新增锁都需要半数以上的实例同意,借鉴了选举算法.其中锁的key为uuid+数字,支持可重入.C.锁的释放分两种情况,一种是hincrby-1这是针对可重入锁做的释放设计,另一种是直接del,这是针对仅单次持有做的设计D.Redisson提供了RedissonRedLock的API,只需要在创建的时候传入若干个RLock即可,至于tryLock和unlock和setnx锁没有太大差异

